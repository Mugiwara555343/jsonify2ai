[
  {
    "path": ".md",
    "ext": "",
    "size": 0,
    "sig": "da39a3ee5e6b4b0d3255bfef95601890afd80709",
    "head": "",
    "kind": "text"
  },
  {
    "path": "docker-compose.yml",
    "ext": ".yml",
    "size": 949,
    "sig": "883c3c26a4462370e45ee0cd49e3cd5163d7c0be",
    "head": "\nservices:\n  api:\n    build: ./api\n    ports:\n      - \"${PORT_API:-8082}:8082\"\n    environment:\n      - POSTGRES_DSN=${POSTGRES_DSN}\n      - QDRANT_URL=${QDRANT_URL}\n      - OLLAMA_URL=${OLLAMA_URL}\n      - WORKER_BASE=${WORKER_BASE:-http://worker:8090}\n    networks:\n      - default\n\n  worker:\n    build: ./worker\n    ports:\n      - \"${PORT_WORKER:-8090}:8090\"\n    environment:\n      - POSTGRES_DSN=${POSTGRES_DSN}\n      - QDRANT_URL=${QDRANT_URL}\n      - OLLAMA_URL=${OLLAMA_URL:-http://host.docker.internal:11434}\n      - EMBED_DEV_MODE=${EMBED_DEV_MODE:-0}\n      - EMBEDDING_DIM=${EMBEDDING_DIM:-768}\n      - DEBUG_CONFIG=${DEBUG_CONFIG:-0}\n      - PORT_WORKER=${PORT_WORKER:-8090}\n    networks:\n      - default\n\n  web:\n    build: ./web\n    ports:\n      - \"${PORT_WEB:-5173}:5173\"\n    environment:\n      - PORT_WEB=${PORT_WEB:-5173}\n    networks:\n      - default\n\nnetworks:\n  default:\n    driver: bridge\n",
    "kind": "text"
  },
  {
    "path": "images.candidates.json",
    "ext": ".json",
    "size": 2,
    "sig": "97d170e1550eee4afc0af065b78cda302a97674c",
    "head": "[]",
    "kind": "text"
  },
  {
    "path": "instructions.md",
    "ext": ".md",
    "size": 2721,
    "sig": "4a8b63c20bbb077073281230ea2934b7330d4753",
    "head": "# Instruction Framework for Executor-Level Steps (note2json Project)\n\nThis framework governs **all step-by-step technical instructions** for the `<PROJECT_NAME>` build.  \nIt applies **only** within this project and is designed for a user who is **new to Go and multi-service Docker**, but comfortable with Python and basic Docker usage.\n\n---\n\n## 1. Framework Structure\n\n| Section | What goes here | Notes |\n|---------|----------------|-------|\n| **Role** | One sentence describing *who* the assistant is for this step. | Always starts with \u201cAct as a detailed technical project guide\u2026\u201d so Cursor knows the voice/context. |\n| **Task** | Plain-language description of *what* we\u2019re about to do (feature, bug-fix, test, etc.). | Keep it focused on one atomic change. |\n| **Context** | Repo structure + relevant env details the step depends on. | Include full file paths (e.g., `api/internal/routes/routes.go`). |\n| **Reasoning** | Why this change is needed and what it will achieve. | 2-4 concise lines max. |\n| **Output Format** | **(a) Cursor Prompt** \u2013 block ready to paste.<br>**(b) Verification Commands** \u2013 PowerShell-safe.<br>**(c) Expected Result** \u2013 log snippet or curl output.<br>**(d) Rollback** \u2013 how to undo if it misbehaves. | Each sub-section must be present. |\n| **Stop Condition** | Clear criterion for \u201cdone\u201d (e.g., \u201ccurl returns 200 with JSON body `{ok:true}`\u201d). | Lets you self-check before we move on. |\n\n---\n\n## 2. Mode System\n\n| Mode | Trigger phrase from you | My behavior |\n|------|------------------------|-------------|\n| **Plan Mode** | \u201cExplain\u201d / \u201cBig picture\u201d | High-level discussion only, no code or commands. |\n| **Execution Mode** | \u201cStep-by-step\u201d / \u201cLet\u2019s implement\u201d | Use the full framework above. |\n| **Debug Mode** | \u201cIt broke\u201d / \u201cHelp debug\u201d | Ask for logs, then give framework-style fix. |\n| **Pause Mode** | \u201cPause\u201d / \u201cNo commands\u201d | Zero code; supportive chat or strategy only. |\n\n*(You can switch modes anytime with the trigger phrases.)*\n\n---\n\n## 3. Checklist Before Sending an Execution-Mode Answer\n\n1. Confirm exact file path(s) and function/block names.  \n2. Write **before \u2192 after** code diff inside the Cursor Prompt.  \n3. Include PowerShell-tested verification commands.  \n4. State expected success output.  \n5. Provide quick rollback note.\n\n---\n\n## 4. Notes\n\n- This framework applies only to the **note2json** project context.  \n- If the project is paused, no commands or edits will be provided until you explicitly say \u201cresume.\u201d  \n- All technical edits will assume they will be executed via **Cursor** or **Continue.dev**, so prompts must be copy-paste ready.\n\n",
    "kind": "text"
  },
  {
    "path": "Makefile",
    "ext": "",
    "size": 169,
    "sig": "fc457993a2715031617fdd8f30da0bb760d90f82",
    "head": ".PHONY: up down logs ps\n\nup:\n\tdocker compose up -d --build\n\ndown:\n\tdocker compose down -v\n\nlogs:\n\tdocker compose logs -f --tail=100\n\nps:\n\tdocker compose ps\n",
    "kind": "text"
  },
  {
    "path": "project.map.json",
    "ext": ".json",
    "size": 221307,
    "sig": "c46b30ddbe9273639894a7e2c7126fe31c6eee63",
    "head": "[\n  {\n    \"path\": \".md\",\n    \"ext\": \"\",\n    \"size\": 0,\n    \"sig\": \"da39a3ee5e6b4b0d3255bfef95601890afd80709\",\n    \"head\": \"\",\n    \"kind\": \"text\"\n  },\n  {\n    \"path\": \"docker-compose.yml\",\n    \"ext\": \".yml\",\n    \"size\": 949,\n    \"sig\": \"883c3c26a4462370e45ee0cd49e3cd5163d7c0be\",\n    \"head\": \"\\nservices:\\n  api:\\n    build: ./api\\n    ports:\\n      - \\\"${PORT_API:-8082}:8082\\\"\\n    environment:\\n      - POSTGRES_DSN=${POSTGRES_DSN}\\n      - QDRANT_URL=${QDRANT_URL}\\n      - OLLAMA_URL=${OLLAMA_URL}\\n      - WORKER_BASE=${WORKER_BASE:-http://worker:8090}\\n    networks:\\n      - default\\n\\n  worker:\\n    build: ./worker\\n    ports:\\n      - \\\"${PORT_WORKER:-8090}:8090\\\"\\n    environment:\\n      - POSTGRES_DSN=${POSTGRES_DSN}\\n      - QDRANT_URL=${QDRANT_URL}\\n      - OLLAMA_URL=${OLLAMA_URL:-http://host.docker.internal:11434}\\n      - EMBED_DEV_MODE=${EMBED_DEV_MODE:-0}\\n      - EMBEDDING_DIM=${EMBEDDING_DIM:-768}\\n      - DEBUG_CONFIG=${DEBUG_CONFIG:-0}\\n      - PORT_WORKER=${PORT_WORKER:-8090}\\n    networks:\\n      - default\\n\\n  web:\\n    build: ./web\\n    ports:\\n      - \\\"${PORT_WEB:-5173}:5173\\\"\\n    environment:\\n      - PORT_WEB=${PORT_WEB:-5173}\\n    networks:\\n      - default\\n\\nnetworks:\\n  default:\\n    driver: bridge\\n\",\n    \"kind\": \"text\"\n  },\n  {\n    \"path\": \"instructions.md\",\n    \"ext\": \".md\",\n    \"size\": 2721,\n    \"sig\": \"4a8b63c20bbb077073281230ea2934b7330d4753\",\n    \"head\": \"# Instruction Framework for Executor-Level Steps (note2json Project)\\n\\nThis framework governs **all step-by-step technical instructions** for the `<PROJECT_NAME>` build.  \\nIt applies **only** within this project and is designed for a user who is **new to Go and multi-service Docker**, but comfortable with Python and basic Docker usage.\\n\\n---\\n\\n## 1. Framework Structure\\n\\n| Section | What goes here | Notes |\\n|---------|----------------|-------|\\n| **Role** | One sentence describing *who* the assistant is for this step. | Always starts with \\u201cAct as a detailed technical project guide\\u2026\\u201d so Cursor knows the voice/context. |\\n| **Task** | Plain-language description of *what* we\\u2019re about to do (feature, bug-fix, test, etc.). | Keep it focused on one atomic change. |\\n| **Context** | Repo structure + relevant env details the step depends on. | Include full file paths (e.g., `api/internal/routes/routes.go`). |\\n| **Reasoning** | Why this change is needed and what it will achieve. | 2-4 concise lines max. |\\n| **Output Format** | **(a) Cursor Prompt** \\u2013 block ready to paste.<br>**(b) Verification Commands** \\u2013 PowerShell-safe.<br>**(c) Expected Result** \\u2013 log snippet or curl output.<br>**(d) Rollback** \\u2013 how to undo if it misbehaves. | Each sub-section must be present. |\\n| **Stop Condition** | Clear criterion for \\u201cdone\\u201d (e.g., \\u201ccurl returns 200 with JSON body `{ok:true}`\\u201d). | Lets you self-check before we move on. |\\n\\n---\\n\\n## 2. Mode System\\n\\n| Mode | Trigger phrase from you | My behavior |\\n|------|------------------------|-------------|\\n| **Plan Mode** | \\u201cExplain\\u201d / \\u201cBig picture\\u201d | High-level discussion only, no code or commands. |\\n| **Execution Mode** | \\u201cStep-by-step\\u201d / \\u201cLet\\u2019s implement\\u201d | Use the full framework above. |\\n| **Debug Mode** | \\u201cIt broke\\u201d / \\u201cHelp debug\\u201d | Ask for logs, then give framework-style fix. |\\n| **Pause Mode** | \\u201cPause\\u201d / \\u201cNo commands\\u201d | Zero code; supportive chat or strategy only. |\\n\\n*(You can switch modes anytime with the trigger phrases.)*\\n\\n---\\n\\n## 3. Checklist Before Sending an Execution-Mode Answer\\n\\n1. Confirm exact file path(s) and function/block names.  \\n2. Write **before \\u2192 after** code diff inside the Cursor Prompt.  \\n3. Include PowerShell-tested verification commands.  \\n4. State expected success output.  \\n5. Provide quick rollback note.\\n\\n---\\n\\n## 4. Notes\\n\\n- This framework applies only to the **note2json** project context.  \\n- If the project is paused, no commands or edits will be provided until you explicitly say \\u201cresume.\\u201d  \\n- All technical edits will assume they will be executed via **Cursor** or **Continue.dev**, so prompts must be copy-paste ready.\\n\\n\",\n    \"kind\": \"text\"\n  },\n  {\n    \"path\": \"Makefile\",\n    \"ext\": \"\",\n    \"size\": 169,\n    \"sig\": \"fc457993a2715031617fdd8f30da0bb760d90f82\",\n    \"head\": \".PHONY: up down logs ps\\n\\nup:\\n\\tdocker compose up -d --build\\n\\ndown:\\n\\tdocker compose down -v\\n\\nlogs:\\n\\tdocker compose logs -f --tail=100\\n\\nps:\\n\\tdocker compose ps\\n\",\n    \"kind\": \"text\"\n  },\n  {\n    \"path\": \"project.map.json\",\n    \"ext\": \".json\",\n    \"size\": 216204,\n    \"sig\": \"5568599670c0760c3f5845bcf9693d82e3bf76b7\",\n    \"head\": \"[\\n  {\\n    \\\"path\\\": \\\".md\\\",\\n    \\\"ext\\\": \\\"\\\",\\n    \\\"size\\\": 0,\\n    \\\"sig\\\": \\\"da39a3ee5e6b4b0d3255bfef95601890afd80709\\\",\\n    \\\"head\\\": \\\"\\\"\\n  },\\n  {\\n    \\\"path\\\": \\\"docker-compose.yml\\\",\\n    \\\"ext\\\": \\\".yml\\\",\\n    \\\"size\\\": 949,\\n    \\\"sig\\\": \\\"883c3c26a4462370e45ee0cd49e3cd5163d7c0be\\\",\\n    \\\"head\\\": \\\"\\\\nservices:\\\\n  api:\\\\n    build: ./api\\\\n    ports:\\\\n      - \\\\\\\"${PORT_API:-8082}:8082\\\\\\\"\\\\n    environment:\\\\n      - POSTGRES_DSN=${POSTGRES_DSN}\\\\n      - QDRANT_URL=${QDRANT_URL}\\\\n      - OLLAMA_URL=${OLLAMA_URL}\\\\n      - WORKER_BASE=${WORKER_BASE:-http://worker:8090}\\\\n    networks:\\\\n      - default\\\\n\\\\n  worker:\\\\n    build: ./worker\\\\n    ports:\\\\n      - \\\\\\\"${PORT_WORKER:-8090}:8090\\\\\\\"\\\\n    environment:\\\\n      - POSTGRES_DSN=${POSTGRES_DSN}\\\\n      - QDRANT_URL=${QDRANT_URL}\\\\n      - OLLAMA_URL=${OLLAMA_URL:-http://host.docker.internal:11434}\\\\n      - EMBED_DEV_MODE=${EMBED_DEV_MODE:-0}\\\\n      - EMBEDDING_DIM=${EMBEDDING_DIM:-768}\\\\n      - DEBUG_CONFIG=${DEBUG_CONFIG:-0}\\\\n      - PORT_WORKER=${PORT_WORKER:-8090}\\\\n    networks:\\\\n      - default\\\\n\\\\n  web:\\\\n    build: ./web\\\\n    ports:\\\\n      - \\\\\\\"${PORT_WEB:-5173}:5173\\\\\\\"\\\\n    environment:\\\\n      - PORT_WEB=${PORT_WEB:-5173}\\\\n    networks:\\\\n      - default\\\\n\\\\nnetworks:\\\\n  default:\\\\n    driver: bridge\\\\n\\\"\\n  },\\n  {\\n    \\\"path\\\": \\\"instructions.md\\\",\\n    \\\"ext\\\": \\\".md\\\",\\n    \\\"size\\\": 2721,\\n    \\\"sig\\\": \\\"4a8b63c20bbb077073281230ea2934b7330d4753\\\",\\n    \\\"head\\\": \\\"# Instruction Framework for Executor-Level Steps (note2json Project)\\\\n\\\\nThis framework governs **all step-by-step technical instructions** for the `<PROJECT_NAME>` build.  \\\\nIt applies **only** within this project and is designed for a user who is **new to Go and multi-service Docker**, but comfortable with Python and basic Docker usage.\\\\n\\\\n---\\\\n\\\\n## 1. Framework Structure\\\\n\\\\n| Section | What goes here | Notes |\\\\n|---------|----------------|-------|\\\\n| **Role** | One sentence describing *who* the assistant is for this step. | Always starts with \\\\u201cAct as a detailed technical project guide\\\\u2026\\\\u201d so Cursor knows the voice/context. |\\\\n| **Task** | Plain-language description of *what* we\\\\u2019re about to do (feature, bug-fix, test, etc.). | Keep it focused on one atomic change. |\\\\n| **Context** | Repo structure + relevant env details the step depends on. | Include full file paths (e.g., `api/internal/routes/routes.go`). |\\\\n| **Reasoning** | Why this change is needed and what it will achieve. | 2-4 concise lines max. |\\\\n| **Output Format** | **(a) Cursor Prompt** \\\\u2013 block ready to paste.<br>**(b) Verification Commands** \\\\u2013 PowerShell-safe.<br>**(c) Expected Result** \\\\u2013 log snippet or curl output.<br>**(d) Rollback** \\\\u2013 how to undo if it misbehaves. | Each sub-section must be present. |\\\\n| **Stop Condition** | Clear criterion for \\\\u201cdone\\\\u201d (e.g., \\\\u201ccurl returns 200 with JSON body `{ok:true}`\\\\u201d). | Lets you self-check before we move on. |\\\\n\\\\n---\\\\n\\\\n## 2. Mode System\\\\n\\\\n| Mode | Trigger phrase from you | My behavior |\\\\n|------|------------------------|-------------|\\\\n| **Plan Mode** | \\\\u201cExplain\\\\u201d / \\\\u201cBig picture\\\\u201d | High-level discussion only, no code or c",
    "kind": "text"
  },
  {
    "path": "pyproject.toml",
    "ext": ".toml",
    "size": 477,
    "sig": "1ac3b947f5145369cc5987d9da2a1ce6a0b45d60",
    "head": "[build-system]\nrequires = [\"setuptools>=68\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"jsonify2ai\"\nversion = \"0.1.0\"\ndescription = \"Ingest \u2192 embed \u2192 store \u2192 retrieve. note2json inside.\"\nrequires-python = \">=3.10\"\ndependencies = [\n  \"typer>=0.12\",\n  \"rich>=13\",\n  \"pydantic>=2\",\n  \"qdrant-client>=1.8\",\n  \"psycopg[binary]>=3.2\",\n  \"uvloop; platform_system != 'Windows'\",\n]\n\n[project.scripts]\njsonify2ai = \"jsonify2ai.cli:app\"\n",
    "kind": "text"
  },
  {
    "path": "README.md",
    "ext": ".md",
    "size": 6901,
    "sig": "7597d0420e1c7aff121246174d00d218e4b3524b",
    "head": "# jsonify2ai\n\nIngest \u2192 embed \u2192 store \u2192 retrieve. Ships with `note2json` as a module.\n\n![CI](https://github.com/Mugiwara555343/jsonify2ai/actions/workflows/ci.yml/badge.svg)\n\n## Description\n\njsonify2ai is a comprehensive AI data pipeline that processes, normalizes, embeds, and stores various types of content for intelligent retrieval and question answering. It includes the powerful `note2json` module for converting markdown and text files to structured JSON.\n\n## Installation\n\n```bash\npip install -e .\n```\n\n## Quickstart\n\n### Convert markdown to JSON\n\nConvert a markdown file to structured JSON:\n\n```bash\njsonify2ai note2json convert demo_entries/team_collaboration.md -o out.json --pretty\n```\n\n### Index & Search\n\nIndex a text file for semantic search:\n\n```bash\njsonify2ai index-text -i README.md\n```\n\nSearch for content:\n\n```bash\njsonify2ai search -q \"installation\"\n```\n\n## Features\n\n- **note2json**: Convert markdown/text to structured JSON\n- **Vector Embeddings**: Generate embeddings using BAAI/bge-small\n- **Vector Storage**: Store and search using Qdrant\n- **Metadata Storage**: PostgreSQL integration for structured data\n- **Local LLM**: Ollama integration for question answering\n- **CLI Interface**: Easy-to-use command-line tools\n\n## Architecture\n\nSee [docs/architecture.md](docs/architecture.md) for detailed system architecture and component descriptions.\n\n## Phase 1 \u2013 Text Pipeline\n\nThe worker service now provides a complete text processing pipeline that chunks, embeds, and stores text content in Qdrant for semantic search.\n\n### What `/process/text` Does\n\nThe endpoint accepts text content (either raw text or file path) and:\n1. **Chunks** the text using configurable size and overlap\n2. **Embeds** chunks using Ollama's embedding API\n3. **Stores** vectors in Qdrant with metadata\n4. **Returns** processing statistics and collection info\n\n### Configuration\n\n| Environment Variable | Default | Description |\n|---------------------|---------|-------------|\n| `OLLAMA_URL` | `http://host.docker.internal:11434` | Ollama service URL |\n| `QDRANT_URL` | `http://host.docker.internal:6333` | Qdrant vector database URL |\n| `QDRANT_COLLECTION` | `jsonify2ai_chunks` | Collection name for chunks |\n| `EMBEDDINGS_MODEL` | `nomic-embed-text` | Ollama model for embeddings |\n| `EMBEDDING_DIM` | `768` | Vector dimension size |\n| `CHUNK_SIZE` | `800` | Maximum chunk size in characters |\n| `CHUNK_OVERLAP` | `100` | Overlap between chunks |\n| `EMBED_DEV_MODE` | `0` | Enable dev mode for deterministic dummy embeddings |\n\n### Usage Examples\n\n#### PowerShell (curl.exe)\n```powershell\ncurl.exe -X POST \"http://localhost:${PORT_WORKER:-8090}/process/text\" `\n  -H \"Content-Type: application/json\" `\n  -d \"{\\\"document_id\\\":\\\"00000000-0000-0000-0000-000000000000\\\",\\\"text\\\":\\\"hello world\\\"}\"\n```\n\n#### Bash\n```bash\ncurl -X POST \"http://localhost:${PORT_WORKER:-8090}/process/text\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"document_id\":\"00000000-0000-0000-0000-000000000000\",\"text\":\"hello world\"}'\n```\n\n### Qdrant Collection Safety\n\nThe system automatically creates collections if they don't exist, but **never recreates** existing ones. If a collection exists with different dimensions than expected, the system will raise a clear error asking you to either:\n- Use a different collection name, or\n- Change the embedding model to match the existing collection\n\nThis prevents data loss and ensures vector compatibility.\n\n### Dev Mode for Embeddings\n\nSet `EMBED_DEV_MODE=1` in your `.env` file to bypass Ollama and generate deterministic dummy embeddings. This is useful for local smoke tests when Ollama isn't running.\n\n**Example `.env` configuration:**\n```bash\nEMBED_DEV_MODE=1\nEMBEDDING_DIM=768\nDEBUG_CONFIG=1\n```\n\n**Note:** The API `/upload` endpoint now returns HTTP 502 (Bad Gateway) if the worker service fails during processing.\n\n### Dev Mode Verification\n\nTo verify that dev mode is working correctly:\n\n1. **Set `.env` configuration:**\n   ```bash\n   EMBED_DEV_MODE=1\n   EMBEDDING_DIM=768\n   DEBUG_CONFIG=1\n   ```\n\n2. **Rebuild & restart only the worker:**\n   ```bash\n   docker compose build worker\n   docker compose up -d worker\n   ```\n\n3. **Verify environment variables inside the container:**\n   - **Linux/mac:**\n     ```bash\n     docker compose exec worker env | grep EMBED\n     ```\n   - **PowerShell:**\n     ```powershell\n     docker compose exec worker cmd /c set | findstr EMBED\n     ```\n\n4. **Check the debug endpoint:**\n   ```bash\n   GET http://localhost:${PORT_WORKER:-8090}/debug/config\n   ```\n   \n   Expected response with `dev_mode: \"1\"` and `dim: 768`:\n   ```json\n   {\n     \"model\": \"nomic-embed-text\",\n     \"dim\": 768,\n     \"dev_mode\": \"1\",\n     \"qdrant_url\": \"http://host.docker.internal:6333\",\n     \"ollama_url\": \"http://host.docker.internal:11434\",\n     \"collection\": \"jsonify2ai_chunks\",\n     \"chunk_size\": 800,\n     \"chunk_overlap\": 100,\n     \"debug_enabled\": true\n   }\n   ```\n\n### CI Status\n\nContinuous Integration runs on both `master` and `main` branches, executing worker service tests with network calls disabled by default. Set `SERVICES_UP=1` to enable integration tests that require Ollama and Qdrant.\n\n## Phase 1.1 \u2013 Upload \u2192 Process\n\nThe API service now provides a complete file upload and processing workflow that automatically wires uploaded text files to the worker service for chunking, embedding, and storage.\n\n### What `/upload` Does\n\nThe endpoint accepts multipart file uploads and:\n1. **Saves** files to `./data/documents/<document_id>/<filename>`\n2. **Generates** unique UUID v4 document identifiers\n3. **Reads** file content as UTF-8 text (5MB limit)\n4. **Calls** worker service to process the text\n5. **Returns** comprehensive metadata and processing results\n\n### Usage Examples\n\n#### PowerShell (curl.exe)\n```powershell\n$env:PORT_API=8082\ncurl.exe -F \"file=@.\\README.md\" \"http://localhost:$env:PORT_API/upload\"\n```\n\n### Response Format\n\n```json\n{\n  \"ok\": true,\n  \"document_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"filename\": \"README.md\",\n  \"size\": 1529,\n  \"mime\": \"text/markdown\",\n  \"worker\": {\n    \"ok\": true,\n    \"chunks\": 2,\n    \"embedded\": 2,\n    \"upserted\": 2,\n    \"collection\": \"jsonify2ai_chunks\"\n  }\n}\n```\n\n### File Storage\n\nUploaded files are automatically organized under:\n```\n./data/documents/\n\u251c\u2500\u2500 <document_id_1>/\n\u2502   \u2514\u2500\u2500 <original_filename_1>\n\u251c\u2500\u2500 <document_id_2>/\n\u2502   \u2514\u2500\u2500 <original_filename_2>\n\u2514\u2500\u2500 ...\n```\n\n### Error Handling\n\n- **No file**: Returns 400 with error message\n- **File too large (>5MB)**: Returns 413 with error message\n- **Worker service failure**: Returns 502 with error details\n- **Invalid UTF-8**: Automatically coerced to valid UTF-8\n\n",
    "kind": "text"
  },
  {
    "path": "repo_scan.py",
    "ext": ".py",
    "size": 4116,
    "sig": "e9ed79b7dcf9be657bc07959b0ae659f8714fb14",
    "head": "# scripts/repo_scan.py\n# Scans a repo tree, builds a light-weight project map for quick context,\n# and (NEW) emits an image candidate list to bootstrap the image pipeline.\n\nimport os\nimport json\nimport re\nimport hashlib\nimport sys\n\n# --- CLI / root selection\n# Accept a directory as the first argument (defaults to current working dir)\nROOT = sys.argv[1] if len(sys.argv) > 1 else \".\"\n\n# --- Ignore rules\n# Skip common heavy/noisy directories and system detritus\nIGNORE = re.compile(r'(\\.git|node_modules|__pycache__|dist|build|\\.venv|env|.DS_Store)')\n\n# --- ADDED: image extension set\n# Minimal, broad coverage for common image types; easy to extend later.\nIMAGE_EXTS = {\".jpg\", \".jpeg\", \".png\", \".webp\"}\n\n# --- File signature helper\n# Reads up to 128 KiB from the file head and returns a SHA1.\n# This is stable and cheap, and lets us detect identical heads quickly.\ndef file_sig(p: str) -> str:\n    try:\n        with open(p, 'rb') as f:\n            data = f.read(1024 * 128)  # 128 KiB\n        return hashlib.sha1(data).hexdigest()\n    except Exception:\n        return \"ERR\"\n\nrecords = []          # all files (for project.map.json)\nimage_records = []    # --- ADDED: image-only view (for images.candidates.json)\n\n# --- Walk the tree\nfor dp, dn, fn in os.walk(ROOT):\n    # Skip ignored directories by path substring match\n    if IGNORE.search(dp):\n        continue\n\n    for fname in fn:\n        # Skip ignored files (by name)\n        if IGNORE.search(fname):\n            continue\n\n        # Absolute and relative paths\n        p = os.path.join(dp, fname)\n        rel = os.path.relpath(p, ROOT)\n\n        # Basic file attributes\n        ext = os.path.splitext(fname)[1].lower()\n\n        try:\n            size = os.path.getsize(p)\n        except OSError:\n            # If size fails, skip the file safely\n            size = -1\n\n        # --- Classify and capture a \"head\" preview for text-like files only\n        # Keep behavior: for \"huge\" files we avoid reading them as text.\n        if size > 1024 * 1024 * 2:  # > 2 MiB considered \"huge\" here\n            kind = \"binary\"\n            head = \"\"\n        else:\n            try:\n                with open(p, 'r', encoding='utf-8', errors='ignore') as fh:\n                    # Keep the same compact preview window (8k chars)\n                    head = fh.read(8000)\n                kind = \"text\"\n            except Exception:\n                # Non-text (or unreadable) falls back to binary with no head\n                kind = \"binary\"\n                head = \"\"\n\n        # --- Build the record for project.map.json\n        rec = {\n            \"path\": rel,\n            \"ext\": ext,\n            \"size\": size,\n            \"sig\": file_sig(p),\n            \"head\": head,\n            \"kind\": kind,  # (text|binary) classification for quick triage\n        }\n\n        # --- ADDED: detect and queue image candidates\n        # We don't need EXIF or MIME sniffing yet\u2014extension + existence is enough.\n        if ext in IMAGE_EXTS:\n            # Promote kind to \"image\" in the main map for clarity\n            rec[\"kind\"] = \"image\"\n\n            # Store a minimal, ingestion-ready view for the image pipeline.\n            # This file is intentionally small and fast to parse later.\n            image_records.append({\n                \"path\": rel,\n                \"ext\": ext,\n                \"size\": size,\n                \"sig\": rec[\"sig\"],   # re-use the head hash for dedupe checks\n                \"kind\": \"image\"\n            })\n\n        records.append(rec)\n\n# --- Write the full map (original behavior)\nwith open(\"project.map.json\", \"w\", encoding=\"utf-8\") as out:\n    json.dump(records, out, indent=2)\nprint(\"Wrote project.map.json with\", len(records), \"files\")\n\n# --- ADDED: Write image candidate list\n# This is the small, focused handoff we\u2019ll use to seed /process/image.\nwith open(\"images.candidates.json\", \"w\", encoding=\"utf-8\") as imgs:\n    json.dump(image_records, imgs, indent=2)\nprint(\"Wrote images.candidates.json with\", len(image_records), \"image files\")\n",
    "kind": "text"
  },
  {
    "path": ".continue\\config.json",
    "ext": ".json",
    "size": 301,
    "sig": "3c456173b2d34ab199136b59f27ea4ca73b7690f",
    "head": "{\n  \"models\": [\n    {\n      \"title\": \"Qwen2.5-Coder 7B (local)\",\n      \"provider\": \"openai\",\n      \"model\": \"qwen2.5-coder-7b-q5\",\n      \"apiBase\": \"http://localhost:1234/v1\",\n      \"apiKey\": \"sk-local\",\n      \"temperature\": 0.15,\n      \"top_p\": 0.9,\n      \"maxTokens\": 4096\n    }\n  ]\n}\n",
    "kind": "text"
  },
  {
    "path": ".continue\\docs\\new-doc.yaml",
    "ext": ".yaml",
    "size": 105,
    "sig": "50941264546d8f56c90a598461ce67da0780f3ee",
    "head": "name: New doc\nversion: 0.0.1\nschema: v1\ndocs:\n  - name: New docs\n    startUrl: https://docs.continue.dev\n",
    "kind": "text"
  },
  {
    "path": ".pytest_cache\\CACHEDIR.TAG",
    "ext": ".tag",
    "size": 191,
    "sig": "74a33d12cf1d479b4901bc0a0dd1cf02df998f0b",
    "head": "Signature: 8a477f597d28d172789f06886806bc55\n# This file is a cache directory tag created by pytest.\n# For information about cache directory tags, see:\n#\thttps://bford.info/cachedir/spec.html\n",
    "kind": "text"
  },
  {
    "path": ".pytest_cache\\README.md",
    "ext": ".md",
    "size": 310,
    "sig": "34aa343b5defb73206fec8763d6e21f419652e73",
    "head": "# pytest cache directory #\n\nThis directory contains data from the pytest's cache plugin,\nwhich provides the `--lf` and `--ff` options, as well as the `cache` fixture.\n\n**Do not** commit this to version control.\n\nSee [the docs](https://docs.pytest.org/en/stable/how-to/cache.html) for more information.\n",
    "kind": "text"
  },
  {
    "path": ".pytest_cache\\v\\cache\\lastfailed",
    "ext": "",
    "size": 70,
    "sig": "e845677b92eeafd7d896242baffb647301a21681",
    "head": "{\n  \"worker/tests/test_qdrant_minimal.py::TestQdrantMinimal\": true\n}",
    "kind": "text"
  },
  {
    "path": ".pytest_cache\\v\\cache\\nodeids",
    "ext": "",
    "size": 8986,
    "sig": "6e04919c40ad63adaefdd902ae2da2b66e0c375d",
    "head": "[\n  \"jsonify2ai/modules/note2json/tests/test_bom_handling.py::test_bom_handling\",\n  \"jsonify2ai/modules/note2json/tests/test_cli_integration.py::test_cli_glob_expansion\",\n  \"jsonify2ai/modules/note2json/tests/test_cli_integration.py::test_cli_integration\",\n  \"jsonify2ai/modules/note2json/tests/test_cli_integration.py::test_cli_missing_file_exit_code\",\n  \"jsonify2ai/modules/note2json/tests/test_cli_integration.py::test_cli_stdout_flag\",\n  \"jsonify2ai/modules/note2json/tests/test_generic_inputs.py::test_stdin_json_autodetect_normalizes\",\n  \"jsonify2ai/modules/note2json/tests/test_generic_inputs.py::test_stdin_wins_over_files\",\n  \"jsonify2ai/modules/note2json/tests/test_generic_inputs.py::test_txt_file_parses_plain_text\",\n  \"jsonify2ai/modules/note2json/tests/test_glob_and_multi_outputs.py::TestGlobAndMultiOutputs::test_deduplication_and_sorting\",\n  \"jsonify2ai/modules/note2json/tests/test_glob_and_multi_outputs.py::TestGlobAndMultiOutputs::test_literal_paths\",\n  \"jsonify2ai/modules/note2json/tests/test_glob_and_multi_outputs.py::TestGlobAndMultiOutputs::test_mixed_patterns_and_literals\",\n  \"jsonify2ai/modules/note2json/tests/test_glob_and_multi_outputs.py::TestGlobAndMultiOutputs::test_multiple_patterns\",\n  \"jsonify2ai/modules/note2json/tests/test_glob_and_multi_outputs.py::TestGlobAndMultiOutputs::test_nonexistent_glob_pattern\",\n  \"jsonify2ai/modules/note2json/tests/test_glob_and_multi_outputs.py::TestGlobAndMultiOutputs::test_recursive_glob_pattern\",\n  \"jsonify2ai/modules/note2json/tests/test_glob_and_multi_outputs.py::TestGlobAndMultiOutputs::test_simple_glob_pattern\",\n  \"jsonify2ai/modules/note2json/tests/test_json_passthrough.py::TestJSONPassthrough::test_invalid_json_fails_gracefully\",\n  \"jsonify2ai/modules/note2json/tests/test_json_passthrough.py::TestJSONPassthrough::test_json_array_input\",\n  \"jsonify2ai/modules/note2json/tests/test_json_passthrough.py::TestJSONPassthrough::test_json_primitive_input\",\n  \"jsonify2ai/modules/note2json/tests/test_json_passthrough.py::TestJSONPassthrough::test_json_tags_normalization\",\n  \"jsonify2ai/modules/note2json/tests/test_json_passthrough.py::TestJSONPassthrough::test_json_with_extra_fields\",\n  \"jsonify2ai/modules/note2json/tests/test_json_passthrough.py::TestJSONPassthrough::test_json_with_missing_required_fields\",\n  \"jsonify2ai/modules/note2json/tests/test_json_passthrough.py::TestJSONPassthrough::test_json_with_unicode_content\",\n  \"jsonify2ai/modules/note2json/tests/test_json_passthrough.py::TestJSONPassthrough::test_valid_json_passthrough\",\n  \"jsonify2ai/modules/note2json/tests/test_parser.py::test_parse_extracts_metadata\",\n  \"jsonify2ai/modules/note2json/tests/test_parser.py::test_parse_extracts_required_fields\",\n  \"jsonify2ai/modules/note2json/tests/test_parser.py::test_parse_extracts_summary_and_reflections\",\n  \"jsonify2ai/modules/note2json/tests/test_parser.py::test_parse_returns_valid_json\",\n  \"jsonify2ai/modules/note2json/tests/test_resilience.py::TestCLIResilience::test_continue_on_error_flag\",\n  \"jsonify2ai/modules/note2json/tests/test_resilience.py::TestCLIResilience::test_processing_result_class\",\n  \"jsonify2ai/modules/note2json/tests/test_resilience.py::TestEncodingResilience::test_encoding_detection_fallback\",\n  \"jsonify2ai/modules/note2json/tests/test_resilience.py::TestEncodingResilience::test_text_sanitization\",\n  \"jsonify2ai/modules/note2json/tests/test_resilience.py::TestEncodingResilience::test_text_validation\",\n  \"jsonify2ai/modules/note2json/tests/test_resilience.py::TestEnhancedResilience::test_enhanced_error_messages\",\n  \"jsonify2ai/modules/note2json/tests/test_resilience.py::TestEnhancedResilience::test_enhanced_validation_errors\",\n  \"jsonify2ai/modules/note2json/tests/test_resilience.py::TestEnhancedResilience::test_progress_reporting_enhancements\",\n  \"jsonify2ai/modules/note2json/tests/test_resilience.py::TestEnhancedResilience::test_retry_logic_integration\",\n  \"jsonify2ai/modules/note2json/tests/test_resilience.py::TestErrorHandling::test_encoding_error_context\",\n  \"jsonify2ai/modules/note2json/tests/test_resilience.py::TestErrorHandling::test_graceful_degradation\",\n  \"jsonify2ai/modules/note2json/tests/test_resilience.py::TestErrorHandling::test_parsing_error_context\",\n  \"jsonify2ai/modules/note2json/tests/test_resilience.py::TestParserResilience::test_empty_input_handling\",\n  \"jsonify2ai/modules/note2json/tests/test_resilience.py::TestParserResilience::test_line_ending_handling\",\n  \"jsonify2ai/modules/note2json/tests/test_resilience.py::TestParserResilience::test_long_content_handling\",\n  \"jsonify2ai/modules/note2json/tests/test_resilience.py::TestParserResilience::test_malformed_json_handling\",\n  \"jsonify2ai/modules/note2json/tests/test_resilience.py::TestParserResilience::test_null_byte_handling\",\n  \"jsonify2ai/modules/note2json/tests/test_resilience.py::TestParserResilience::test_validation_error_fixing\",\n  \"jsonify2ai/modules/note2json/tests/test_resilience.py::TestParserResilience::test_whitespace_only_input\",\n  \"jsonify2ai/modules/note2json/tests/test_stdin_modes.py::TestStdinModes::test_stdin_invalid_encoding_fails_gracefully\",\n  \"jsonify2ai/modules/note2json/tests/test_stdin_modes.py::TestStdinModes::test_stdin_invalid_json_fails_gracefully\",\n  \"jsonify2ai/modules/note2json/tests/test_stdin_modes.py::TestStdinModes::test_stdin_json_format\",\n  \"jsonify2ai/modules/note2json/tests/test_stdin_modes.py::TestStdinModes::test_stdin_json_utf16\",\n  \"jsonify2ai/modules/note2json/tests/test_stdin_modes.py::TestStdinModes::test_stdin_markdown_utf16_le\",\n  \"jsonify2ai/modules/note2json/tests/test_stdin_modes.py::TestStdinModes::test_stdin_markdown_utf8\",\n  \"jsonify2ai/modules/note2json/tests/test_stdin_modes.py::TestStdinModes::test_stdin_markdown_utf8_bom\",\n  \"jsonify2ai/modules/note2json/tests/test_stdin_modes.py::TestStdinModes::test_stdin_text_format\",\n  \"jsonify2ai/modules/note2json/tests/test_utf8_and_utf16_files.py::TestUTF8AndUTF16Files::test_invalid_encoding_fails_gracefully\",\n  \"jsonify2ai/modules/note2json/tests/test_utf8_and_utf16_files.py::TestUTF8AndUTF16Files::test_utf16_be_file\",\n  \"jsonify2ai/modules/note2json/tests/test_utf8_and_utf16_files.py::TestUTF8AndUTF16Files::test_utf16_file\",\n  \"jsonify2ai/modules/note2json/tests/test_utf8_and_utf16_files.py::TestUTF8AndUTF16Files::test_utf16_le_file\",\n  \"jsonify2ai/modules/note2json/tests/test_utf8_and_utf16_files.py::TestUTF8AndUTF16Files::test_utf8_bom_file\",\n  \"jsonify2ai/modules/note2json/tests/test_utf8_and_utf16_files.py::TestUTF8AndUTF16Files::test_utf8_file\",\n  \"tests/test_cli_smoke.py::test_help_runs_quick\",\n  \"tests/test_search_index_smoke.py::test_dummy\",\n  \"tests/test_settings.py::test_defaults_load\",\n  \"tests/test_settings.py::test_env_override\",\n  \"worker/tests/test_embed_unit.py::TestEmbedOllama::test_dev_mode_different_texts_different_vectors\",\n  \"worker/tests/test_embed_unit.py::TestEmbedOllama::test_dev_mode_returns_correct_shape\",\n  \"worker/tests/test_embed_unit.py::TestEmbedOllama::test_generate_dummy_embedding\",\n  \"worker/tests/test_embed_unit.py::TestEmbedOllama::test_ollama_api_batch_response\",\n  \"worker/tests/test_embed_unit.py::TestEmbedOllama::test_ollama_api_count_mismatch\",\n  \"worker/tests/test_embed_unit.py::TestEmbedOllama::test_ollama_api_http_error\",\n  \"worker/tests/test_embed_unit.py::TestEmbedOllama::test_ollama_api_single_response\",\n  \"worker/tests/test_embed_unit.py::TestEmbedOllama::test_parse_embeddings_batch_input\",\n  \"worker/tests/test_embed_unit.py::TestEmbedOllama::test_parse_embeddings_invalid_format\",\n  \"worker/tests/test_embed_unit.py::TestEmbedOllama::test_parse_embeddings_single_input\",\n  \"worker/tests/test_process_unit.py::TestChunker::test_chunk_text_empty\",\n  \"worker/tests/test_process_unit.py::TestChunker::test_chunk_text_exact_chunk_size\",\n  \"worker/tests/test_process_unit.py::TestChunker::test_chunk_text_invalid_params\",\n  \"worker/tests/test_process_unit.py::TestChunker::test_chunk_text_multiple_chunks\",\n  \"worker/tests/test_process_unit.py::TestChunker::test_chunk_te",
    "kind": "text"
  },
  {
    "path": "api\\Dockerfile",
    "ext": "",
    "size": 544,
    "sig": "b711487415bec4fdd3144a3b870a07fb7fc2768d",
    "head": "# Build stage\nFROM golang:1.21-alpine AS builder\n\nWORKDIR /app\n\n# Copy go mod files\nCOPY go.mod go.sum ./\n\n# Download dependencies\nRUN go mod download\n\n# Copy source code\nCOPY . .\n\n# Build the application\nRUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o server ./cmd/server\n\n# Final stage\nFROM alpine:latest\n\nRUN apk --no-cache add ca-certificates\n\nWORKDIR /root/\n\n# Copy the binary from builder stage\nCOPY --from=builder /app/server .\n\n# Expose port\nEXPOSE 8082\n\n# Run the binary\nCMD [\"./server\"]\n",
    "kind": "text"
  },
  {
    "path": "api\\go.mod",
    "ext": ".mod",
    "size": 1360,
    "sig": "67ef122baff68ed64154293119be6de8bc947b7e",
    "head": "module github.com/Mugiwara555343/jsonify2ai/api\n\ngo 1.21\n\nrequire (\n\tgithub.com/gin-gonic/gin v1.9.1\n\tgithub.com/google/uuid v1.4.0\n)\n\nrequire (\n\tgithub.com/bytedance/sonic v1.9.1 // indirect\n\tgithub.com/chenzhuoyu/base64x v0.0.0-20221115062448-fe3a3abad311 // indirect\n\tgithub.com/gabriel-vasile/mimetype v1.4.2 // indirect\n\tgithub.com/gin-contrib/sse v0.1.0 // indirect\n\tgithub.com/go-playground/locales v0.14.1 // indirect\n\tgithub.com/go-playground/universal-translator v0.18.1 // indirect\n\tgithub.com/go-playground/validator/v10 v10.14.0 // indirect\n\tgithub.com/goccy/go-json v0.10.2 // indirect\n\tgithub.com/json-iterator/go v1.1.12 // indirect\n\tgithub.com/klauspost/cpuid/v2 v2.2.4 // indirect\n\tgithub.com/leodido/go-urn v1.2.4 // indirect\n\tgithub.com/mattn/go-isatty v0.0.19 // indirect\n\tgithub.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd // indirect\n\tgithub.com/modern-go/reflect2 v1.0.2 // indirect\n\tgithub.com/pelletier/go-toml/v2 v2.0.8 // indirect\n\tgithub.com/twitchyliquid64/golang-asm v0.15.1 // indirect\n\tgithub.com/ugorji/go/codec v1.2.11 // indirect\n\tgolang.org/x/arch v0.3.0 // indirect\n\tgolang.org/x/crypto v0.9.0 // indirect\n\tgolang.org/x/net v0.10.0 // indirect\n\tgolang.org/x/sys v0.8.0 // indirect\n\tgolang.org/x/text v0.9.0 // indirect\n\tgoogle.golang.org/protobuf v1.30.0 // indirect\n\tgopkg.in/yaml.v3 v3.0.1 // indirect\n)\n",
    "kind": "text"
  },
  {
    "path": "api\\go.sum",
    "ext": ".sum",
    "size": 7961,
    "sig": "8838f9072fbac66918e47c610c3f353bdbde96c2",
    "head": "github.com/bytedance/sonic v1.5.0/go.mod h1:ED5hyg4y6t3/9Ku1R6dU/4KyJ48DZ4jPhfY1O2AihPM=\ngithub.com/bytedance/sonic v1.9.1 h1:6iJ6NqdoxCDr6mbY8h18oSO+cShGSMRGCEo7F2h0x8s=\ngithub.com/bytedance/sonic v1.9.1/go.mod h1:i736AoUSYt75HyZLoJW9ERYxcy6eaN6h4BZXU064P/U=\ngithub.com/chenzhuoyu/base64x v0.0.0-20211019084208-fb5309c8db06/go.mod h1:DH46F32mSOjUmXrMHnKwZdA8wcEefY7UVqBKYGjpdQY=\ngithub.com/chenzhuoyu/base64x v0.0.0-20221115062448-fe3a3abad311 h1:qSGYFH7+jGhDF8vLC+iwCD4WpbV1EBDSzWkJODFLams=\ngithub.com/chenzhuoyu/base64x v0.0.0-20221115062448-fe3a3abad311/go.mod h1:b583jCggY9gE99b6G5LEC39OIiVsWj+R97kbl5odCEk=\ngithub.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=\ngithub.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=\ngithub.com/gabriel-vasile/mimetype v1.4.2 h1:w5qFW6JKBz9Y393Y4q372O9A7cUSequkh1Q7OhCmWKU=\ngithub.com/gabriel-vasile/mimetype v1.4.2/go.mod h1:zApsH/mKG4w07erKIaJPFiX0Tsq9BFQgN3qGY5GnNgA=\ngithub.com/gin-contrib/sse v0.1.0 h1:Y/yl/+YNO8GZSjAhjMsSuLt29uWRFHdHYUb5lYOV9qE=\ngithub.com/gin-contrib/sse v0.1.0/go.mod h1:RHrZQHXnP2xjPF+u1gW/2HnVO7nvIa9PG3Gm+fLHvGI=\ngithub.com/gin-gonic/gin v1.9.1 h1:4idEAncQnU5cB7BeOkPtxjfCSye0AAm1R0RVIqJ+Jmg=\ngithub.com/gin-gonic/gin v1.9.1/go.mod h1:hPrL7YrpYKXt5YId3A/Tnip5kqbEAP+KLuI3SUcPTeU=\ngithub.com/go-playground/assert/v2 v2.2.0 h1:JvknZsQTYeFEAhQwI4qEt9cyV5ONwRHC+lYKSsYSR8s=\ngithub.com/go-playground/assert/v2 v2.2.0/go.mod h1:VDjEfimB/XKnb+ZQfWdccd7VUvScMdVu0Titje2rxJ4=\ngithub.com/go-playground/locales v0.14.1 h1:EWaQ/wswjilfKLTECiXz7Rh+3BjFhfDFKv/oXslEjJA=\ngithub.com/go-playground/locales v0.14.1/go.mod h1:hxrqLVvrK65+Rwrd5Fc6F2O76J/NuW9t0sjnWqG1slY=\ngithub.com/go-playground/universal-translator v0.18.1 h1:Bcnm0ZwsGyWbCzImXv+pAJnYK9S473LQFuzCbDbfSFY=\ngithub.com/go-playground/universal-translator v0.18.1/go.mod h1:xekY+UJKNuX9WP91TpwSH2VMlDf28Uj24BCp08ZFTUY=\ngithub.com/go-playground/validator/v10 v10.14.0 h1:vgvQWe3XCz3gIeFDm/HnTIbj6UGmg/+t63MyGU2n5js=\ngithub.com/go-playground/validator/v10 v10.14.0/go.mod h1:9iXMNT7sEkjXb0I+enO7QXmzG6QCsPWY4zveKFVRSyU=\ngithub.com/goccy/go-json v0.10.2 h1:CrxCmQqYDkv1z7lO7Wbh2HN93uovUHgrECaO5ZrCXAU=\ngithub.com/goccy/go-json v0.10.2/go.mod h1:6MelG93GURQebXPDq3khkgXZkazVtN9CRI+MGFi0w8I=\ngithub.com/golang/protobuf v1.5.0/go.mod h1:FsONVRAS9T7sI+LIUmWTfcYkHO4aIWwzhcaSAoJOfIk=\ngithub.com/google/go-cmp v0.5.5 h1:Khx7svrCpmxxtHBq5j2mp/xVjsi8hQMfNLvJFAlrGgU=\ngithub.com/google/go-cmp v0.5.5/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=\ngithub.com/google/gofuzz v1.0.0/go.mod h1:dBl0BpW6vV/+mYPU4Po3pmUjxk6FQPldtuIdl/M65Eg=\ngithub.com/google/uuid v1.4.0 h1:MtMxsa51/r9yyhkyLsVeVt0B+BGQZzpQiTQ4eHZ8bc4=\ngithub.com/google/uuid v1.4.0/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=\ngithub.com/json-iterator/go v1.1.12 h1:PV8peI4a0ysnczrg+LtxykD8LfKY9ML6u2jnxaEnrnM=\ngithub.com/json-iterator/go v1.1.12/go.mod h1:e30LSqwooZae/UwlEbR2852Gd8hjQvJoHmT4TnhNGBo=\ngithub.com/klauspost/cpuid/v2 v2.0.9/go.mod h1:FInQzS24/EEf25PyTYn52gqo7WaD8xa0213Md/qVLRg=\ngithub.com/klauspost/cpuid/v2 v2.2.4 h1:acbojRNwl3o09bUq+yDCtZFc1aiwaAAxtcn8YkZXnvk=\ngithub.com/klauspost/cpuid/v2 v2.2.4/go.mod h1:RVVoqg1df56z8g3pUjL/3lE5UfnlrJX8tyFgg4nqhuY=\ngithub.com/leodido/go-urn v1.2.4 h1:XlAE/cm/ms7TE/VMVoduSpNBoyc2dOxHs5MZSwAN63Q=\ngithub.com/leodido/go-urn v1.2.4/go.mod h1:7ZrI8mTSeBSHl/UaRyKQW1qZeMgak41ANeCNaVckg+4=\ngithub.com/mattn/go-isatty v0.0.19 h1:JITubQf0MOLdlGRuRq+jtsDlekdYPia9ZFsB8h/APPA=\ngithub.com/mattn/go-isatty v0.0.19/go.mod h1:W+V8PltTTMOvKvAeJH7IuucS94S2C6jfK/D7dTCTo3Y=\ngithub.com/modern-go/concurrent v0.0.0-20180228061459-e0a39a4cb421/go.mod h1:6dJC0mAP4ikYIbvyc7fijjWJddQyLn8Ig3JB5CqoB9Q=\ngithub.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd h1:TRLaZ9cD/w8PVh93nsPXa1VrQ6jlwL5oN8l14QlcNfg=\ngithub.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd/go.mod h1:6dJC0mAP4ikYIbvyc7fijjWJddQyLn8Ig3JB5CqoB9Q=\ngithub.com/modern-go/reflect2 v1.0.2 h1:xBagoLtFs94CBntxluKeaWgTMpvLxC4ur3nMaC9Gz0M=\ngithub.com/modern-go/reflect2 v1.0.2/go.mod h1:yWuevngMOJpCy52FWWMvUC8ws7m/LJsjYzDa0/r8luk=\ngithub.com/pelletier/go-toml/v2 v2.0.8 h1:0ctb6s9mE31h0/lhu+J6OPmVeDxJn+kYnJc2jZR9tGQ=\ngithub.com/pelletier/go-toml/v2 v2.0.8/go.mod h1:vuYfssBdrU2XDZ9bYydBu6t+6a6PYNcZljzZR9VXg+4=\ngithub.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=\ngithub.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=\ngithub.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=\ngithub.com/stretchr/objx v0.4.0/go.mod h1:YvHI0jy2hoMjB+UWwv71VJQ9isScKT/TqJzVSSt89Yw=\ngithub.com/stretchr/objx v0.5.0/go.mod h1:Yh+to48EsGEfYuaHDzXPcE3xhTkx73EhmCGUpEOglKo=\ngithub.com/stretchr/testify v1.3.0/go.mod h1:M5WIy9Dh21IEIfnGCwXGc5bZfKNJtfHm1UVUgZn+9EI=\ngithub.com/stretchr/testify v1.7.0/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=\ngithub.com/stretchr/testify v1.7.1/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=\ngithub.com/stretchr/testify v1.8.0/go.mod h1:yNjHg4UonilssWZ8iaSj1OCr/vHnekPRkoO+kdMU+MU=\ngithub.com/stretchr/testify v1.8.1/go.mod h1:w2LPCIKwWwSfY2zedu0+kehJoqGctiVI29o6fzry7u4=\ngithub.com/stretchr/testify v1.8.2/go.mod h1:w2LPCIKwWwSfY2zedu0+kehJoqGctiVI29o6fzry7u4=\ngithub.com/stretchr/testify v1.8.3 h1:RP3t2pwF7cMEbC1dqtB6poj3niw/9gnV4Cjg5oW5gtY=\ngithub.com/stretchr/testify v1.8.3/go.mod h1:sz/lmYIOXD/1dqDmKjjqLyZ2RngseejIcXlSw2iwfAo=\ngithub.com/twitchyliquid64/golang-asm v0.15.1 h1:SU5vSMR7hnwNxj24w34ZyCi/FmDZTkS4MhqMhdFk5YI=\ngithub.com/twitchyliquid64/golang-asm v0.15.1/go.mod h1:a1lVb/DtPvCB8fslRZhAngC2+aY1QWCk3Cedj/Gdt08=\ngithub.com/ugorji/go/codec v1.2.11 h1:BMaWp1Bb6fHwEtbplGBGJ498wD+LKlNSl25MjdZY4dU=\ngithub.com/ugorji/go/codec v1.2.11/go.mod h1:UNopzCgEMSXjBc6AOMqYvWC1ktqTAfzJZUZgYf6w6lg=\ngolang.org/x/arch v0.0.0-20210923205945-b76863e36670/go.mod h1:5om86z9Hs0C8fWVUuoMHwpExlXzs5Tkyp9hOrfG7pp8=\ngolang.org/x/arch v0.3.0 h1:02VY4/ZcO/gBOH6PUaoiptASxtXU10jazRCP865E97k=\ngolang.org/x/arch v0.3.0/go.mod h1:5om86z9Hs0C8fWVUuoMHwpExlXzs5Tkyp9hOrfG7pp8=\ngolang.org/x/crypto v0.9.0 h1:LF6fAI+IutBocDJ2OT0Q1g8plpYljMZ4+lty+dsqw3g=\ngolang.org/x/crypto v0.9.0/go.mod h1:yrmDGqONDYtNj3tH8X9dzUun2m2lzPa9ngI6/RUPGR0=\ngolang.org/x/net v0.10.0 h1:X2//UzNDwYmtCLn7To6G58Wr6f5ahEAQgKNzv9Y951M=\ngolang.org/x/net v0.10.0/go.mod h1:0qNGK6F8kojg2nk9dLZ2mShWaEBan6FAoqfSigmmuDg=\ngolang.org/x/sys v0.0.0-20220704084225-05e143d24a9e/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.6.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/sys v0.8.0 h1:EBmGv8NaZBZTWvrbjNoL6HVt+IVy3QDQpJs7VRIw3tU=\ngolang.org/x/sys v0.8.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=\ngolang.org/x/text v0.9.0 h1:2sjJmO8cDvYveuX97RDLsxlyUxLl+GHoLxBiRdHllBE=\ngolang.org/x/text v0.9.0/go.mod h1:e1OnstbJyHTd6l/uOt8jFFHp6TRDWZR/bV3emEE/zU8=\ngolang.org/x/xerrors v0.0.0-20191204190536-9bdfabe68543 h1:E7g+9GITq07hpfrRu66IVDexMakfv52eLZ2CXBWiKr4=\ngolang.org/x/xerrors v0.0.0-20191204190536-9bdfabe68543/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=\ngoogle.golang.org/protobuf v1.26.0-rc.1/go.mod h1:jlhhOSvTdKEhbULTjvd4ARK9grFBp09yW+WbY/TyQbw=\ngoogle.golang.org/protobuf v1.30.0 h1:kPPoIgf3TsEvrm0PFe15JQ+570QVxYzEvvHqChK+cng=\ngoogle.golang.org/protobuf v1.30.0/go.mod h1:HV8QOd/L58Z+nl8r43ehVNZIU/HEI6OcFqwMG9pJV4I=\ngopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405 h1:yhCVgyC4o1eVCa2tZl7eS0r+SDo693bJlVdllGtEeKM=\ngopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=\ngopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\ngopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=\ngopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=\nrsc.io/pdf v0.1.1/go.mod h1:n8OzWcQ6Sp37PL01nO98y4iUCRdTGarVfzxY20ICaU4=\n",
    "kind": "text"
  },
  {
    "path": "api\\Makefile",
    "ext": "",
    "size": 43,
    "sig": "961bfafca360ac7d7ccc754f12f11a152988f5c2",
    "head": ".PHONY: run\n\nrun:\n\tgo run ./cmd/server\n",
    "kind": "text"
  },
  {
    "path": "api\\README.md",
    "ext": ".md",
    "size": 1334,
    "sig": "f8419f87bea3ad1a3dc5ca87fbe74af50b790ee5",
    "head": "# API Service\n\nGo-based API service for the jsonify2ai memory system.\n\n## Endpoints\n\n- `GET /health` - Health check endpoint\n- `POST /upload` - File upload and processing endpoint\n- `GET /search?q=<query>` - Semantic search endpoint using vector embeddings\n\n## Environment Variables\n\n- `POSTGRES_DSN` - PostgreSQL connection string\n- `QDRANT_URL` - Qdrant vector database URL (default: http://host.docker.internal:6333)\n- `OLLAMA_URL` - Ollama service URL (default: http://host.docker.internal:11434)\n- `WORKER_BASE` - Worker service base URL (default: http://worker:8090)\n- `QDRANT_COLLECTION` - Qdrant collection name (default: jsonify2ai_chunks)\n- `EMBEDDINGS_MODEL` - Ollama embeddings model (default: nomic-embed-text)\n- `SEARCH_TOPK` - Number of search results to return (default: 5)\n\n## Development\n\n```bash\nmake run\n```\n\n## Docker\n\n```bash\ndocker compose up api\n```\n\n## Troubleshooting\n\n### Search Endpoint Fails with 404 from Ollama\n\nIf the `/search` endpoint fails with a 404 error from Ollama, the model may not be available. Pull the required model:\n\n```bash\ncurl -X POST http://host.docker.internal:11434/api/pull -H \"Content-Type: application/json\" -d '{ \"name\": \"nomic-embed-text\" }'\n```\n\nThis will download the `nomic-embed-text` model that's used for generating embeddings.\n",
    "kind": "text"
  },
  {
    "path": "api\\server.exe",
    "ext": ".exe",
    "size": 12808704,
    "sig": "f86ce5f5a8d6ef8fec8eab55c6b9964610adbb92",
    "head": "",
    "kind": "binary"
  },
  {
    "path": "api\\cmd\\server\\main.go",
    "ext": ".go",
    "size": 772,
    "sig": "8452e4fac9aea0c110ce0e9746f309c4ffa441c3",
    "head": "package main\n\nimport (\n\t\"log\"\n\n\t\"github.com/Mugiwara555343/jsonify2ai/api/internal/config\"\n\t\"github.com/Mugiwara555343/jsonify2ai/api/internal/http\"\n)\n\nfunc main() {\n\t// Load configuration\n\tcfg := config.Load()\n\n\tlog.Printf(\"Starting API server with:\")\n\tlog.Printf(\"  POSTGRES_DSN: %s\", cfg.PostgresDSN)\n\tlog.Printf(\"  QDRANT_URL: %s\", cfg.QdrantURL)\n\tlog.Printf(\"  OLLAMA_URL: %s\", cfg.OllamaURL)\n\tlog.Printf(\"  WORKER_BASE: %s\", cfg.WorkerBase)\n\tlog.Printf(\"  QDRANT_COLLECTION: %s\", cfg.QdrantCollection)\n\tlog.Printf(\"  EMBEDDINGS_MODEL: %s\", cfg.EmbeddingsModel)\n\tlog.Printf(\"  SEARCH_TOPK: %d\", cfg.SearchTopK)\n\n\t// Setup router\n\tr := http.SetupRouter(cfg)\n\n\t// Start server\n\tif err := r.Run(\":8082\"); err != nil {\n\t\tlog.Fatal(err)\n\t}\n}\n",
    "kind": "text"
  },
  {
    "path": "api\\internal\\clients\\ollama\\embeddings.go",
    "ext": ".go",
    "size": 2603,
    "sig": "7fc67b19865f39dac70f22e3cfee95375f84d0ab",
    "head": "package ollama\n\nimport (\n\t\"bytes\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"net/http\"\n\t\"strings\"\n\t\"time\"\n)\n\n// Client represents an Ollama embeddings client\ntype Client struct {\n\tbaseURL string\n\tmodel   string\n\tclient  *http.Client\n}\n\n// EmbeddingRequest represents the request to Ollama embeddings API\ntype EmbeddingRequest struct {\n\tModel  string `json:\"model\"`\n\tPrompt string `json:\"prompt\"`\n}\n\n// embedResp represents the response from Ollama embeddings API\n// Ollama can return either \"embedding\" or \"embeddings\" field\ntype embedResp struct {\n\tEmbedding  []float64 `json:\"embedding\"`\n\tEmbeddings []float64 `json:\"embeddings\"`\n\tNumTokens  int       `json:\"num_tokens,omitempty\"`\n}\n\n// Vector returns the embedding vector, preferring \"embedding\" over \"embeddings\"\nfunc (er embedResp) Vector() []float64 {\n\tif len(er.Embedding) > 0 {\n\t\treturn er.Embedding\n\t}\n\tif len(er.Embeddings) > 0 {\n\t\treturn er.Embeddings\n\t}\n\treturn nil\n}\n\n// NewClient creates a new Ollama embeddings client\nfunc NewClient(baseURL, model string) *Client {\n\treturn &Client{\n\t\tbaseURL: baseURL,\n\t\tmodel:   model,\n\t\tclient: &http.Client{\n\t\t\tTimeout: 15 * time.Second,\n\t\t},\n\t}\n}\n\n// EmbedText embeds a single text prompt and returns the embedding vector\nfunc (c *Client) EmbedText(prompt string) ([]float64, error) {\n\treqBody := EmbeddingRequest{\n\t\tModel:  c.model,\n\t\tPrompt: prompt,\n\t}\n\n\tjsonData, err := json.Marshal(reqBody)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to marshal request: %w\", err)\n\t}\n\n\turl := fmt.Sprintf(\"%s/api/embeddings\", c.baseURL)\n\treq, err := http.NewRequest(\"POST\", url, bytes.NewBuffer(jsonData))\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to create request: %w\", err)\n\t}\n\n\treq.Header.Set(\"Content-Type\", \"application/json\")\n\n\tresp, err := c.client.Do(req)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to make request: %w\", err)\n\t}\n\tdefer resp.Body.Close()\n\n\tif resp.StatusCode != http.StatusOK {\n\t\tbody, err := io.ReadAll(resp.Body)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"ollama embeddings: status %d: failed to read response body\", resp.StatusCode)\n\t\t}\n\t\treturn nil, fmt.Errorf(\"ollama embeddings: status %d: %s\", resp.StatusCode, strings.TrimSpace(string(body)))\n\t}\n\n\tvar embeddingResp embedResp\n\tif err := json.NewDecoder(resp.Body).Decode(&embeddingResp); err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to decode response: %w\", err)\n\t}\n\n\tvector := embeddingResp.Vector()\n\tif len(vector) == 0 {\n\t\treturn nil, fmt.Errorf(\"no embeddings returned from Ollama\")\n\t}\n\n\treturn vector, nil\n}\n",
    "kind": "text"
  },
  {
    "path": "api\\internal\\clients\\ollama\\embeddings_test.go",
    "ext": ".go",
    "size": 1079,
    "sig": "cac9a6fa2006c2fa4984192b6edb0bec6c9038e4",
    "head": "package ollama\n\nimport (\n\t\"encoding/json\"\n\t\"reflect\"\n\t\"testing\"\n)\n\nfunc TestEmbedResp_Vector(t *testing.T) {\n\ttests := []struct {\n\t\tname     string\n\t\tjsonData string\n\t\texpected []float64\n\t}{\n\t\t{\n\t\t\tname:     \"embedding field\",\n\t\t\tjsonData: `{\"embedding\":[1,2,3]}`,\n\t\t\texpected: []float64{1, 2, 3},\n\t\t},\n\t\t{\n\t\t\tname:     \"embeddings field\",\n\t\t\tjsonData: `{\"embeddings\":[4,5]}`,\n\t\t\texpected: []float64{4, 5},\n\t\t},\n\t\t{\n\t\t\tname:     \"both fields, embedding takes precedence\",\n\t\t\tjsonData: `{\"embedding\":[1,2,3],\"embeddings\":[4,5]}`,\n\t\t\texpected: []float64{1, 2, 3},\n\t\t},\n\t\t{\n\t\t\tname:     \"neither field\",\n\t\t\tjsonData: `{\"num_tokens\":10}`,\n\t\t\texpected: nil,\n\t\t},\n\t}\n\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tvar resp embedResp\n\t\t\terr := json.Unmarshal([]byte(tt.jsonData), &resp)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Failed to unmarshal JSON: %v\", err)\n\t\t\t}\n\n\t\t\tresult := resp.Vector()\n\t\t\tif !reflect.DeepEqual(result, tt.expected) {\n\t\t\t\tt.Errorf(\"Vector() = %v, want %v\", result, tt.expected)\n\t\t\t}\n\t\t})\n\t}\n}\n",
    "kind": "text"
  },
  {
    "path": "api\\internal\\clients\\qdrant\\search.go",
    "ext": ".go",
    "size": 2302,
    "sig": "41977c6f0a1fea7ed88e5d8fe68f343a9509fb65",
    "head": "package qdrant\n\nimport (\n\t\"bytes\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"net/http\"\n\t\"time\"\n)\n\n// Client represents a Qdrant search client\ntype Client struct {\n\tbaseURL    string\n\tcollection string\n\tclient     *http.Client\n}\n\n// SearchRequest represents the request to Qdrant search API\ntype SearchRequest struct {\n\tVector       []float64 `json:\"vector\"`\n\tLimit        int       `json:\"limit\"`\n\tWithPayload  bool      `json:\"with_payload\"`\n\tWithVector   bool      `json:\"with_vector\"`\n}\n\n// SearchResponse represents the response from Qdrant search API\ntype SearchResponse struct {\n\tResult []SearchResult `json:\"result\"`\n}\n\n// SearchResult represents a single search result from Qdrant\ntype SearchResult struct {\n\tID      string                 `json:\"id\"`\n\tScore   float64                `json:\"score\"`\n\tPayload map[string]interface{} `json:\"payload\"`\n}\n\n// NewClient creates a new Qdrant search client\nfunc NewClient(baseURL, collection string) *Client {\n\treturn &Client{\n\t\tbaseURL:    baseURL,\n\t\tcollection: collection,\n\t\tclient: &http.Client{\n\t\t\tTimeout: 15 * time.Second,\n\t\t},\n\t}\n}\n\n// SearchPoints searches for similar vectors in the collection\nfunc (c *Client) SearchPoints(vector []float64, limit int) ([]SearchResult, error) {\n\treqBody := SearchRequest{\n\t\tVector:      vector,\n\t\tLimit:       limit,\n\t\tWithPayload: true,\n\t\tWithVector:  false,\n\t}\n\t\n\tjsonData, err := json.Marshal(reqBody)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to marshal request: %w\", err)\n\t}\n\t\n\turl := fmt.Sprintf(\"%s/collections/%s/points/search\", c.baseURL, c.collection)\n\treq, err := http.NewRequest(\"POST\", url, bytes.NewBuffer(jsonData))\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to create request: %w\", err)\n\t}\n\t\n\treq.Header.Set(\"Content-Type\", \"application/json\")\n\t\n\tresp, err := c.client.Do(req)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to make request: %w\", err)\n\t}\n\tdefer resp.Body.Close()\n\t\n\tif resp.StatusCode != http.StatusOK {\n\t\treturn nil, fmt.Errorf(\"Qdrant API returned status %d\", resp.StatusCode)\n\t}\n\t\n\tvar searchResp SearchResponse\n\tif err := json.NewDecoder(resp.Body).Decode(&searchResp); err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to decode response: %w\", err)\n\t}\n\t\n\treturn searchResp.Result, nil\n}\n",
    "kind": "text"
  },
  {
    "path": "api\\internal\\clients\\qdrant\\search_test.go",
    "ext": ".go",
    "size": 976,
    "sig": "7b646e75e5c9d99f3f888449c9951fa15bc0b227",
    "head": "package qdrant\n\nimport (\n\t\"encoding/json\"\n\t\"testing\"\n)\n\nfunc TestSearchRequest_Marshal(t *testing.T) {\n\tvector := []float64{0.1, 0.2, 0.3}\n\tlimit := 5\n\n\treq := SearchRequest{\n\t\tVector:      vector,\n\t\tLimit:       limit,\n\t\tWithPayload: true,\n\t\tWithVector:  false,\n\t}\n\n\tjsonData, err := json.Marshal(req)\n\tif err != nil {\n\t\tt.Fatalf(\"Failed to marshal SearchRequest: %v\", err)\n\t}\n\n\t// Verify the JSON structure\n\tvar unmarshaled map[string]interface{}\n\tif err := json.Unmarshal(jsonData, &unmarshaled); err != nil {\n\t\tt.Fatalf(\"Failed to unmarshal JSON: %v\", err)\n\t}\n\n\t// Check required fields\n\tif unmarshaled[\"vector\"] == nil {\n\t\tt.Error(\"vector field missing from JSON\")\n\t}\n\n\tif unmarshaled[\"limit\"] == nil {\n\t\tt.Error(\"limit field missing from JSON\")\n\t}\n\n\tif unmarshaled[\"with_payload\"] != true {\n\t\tt.Error(\"with_payload should be true\")\n\t}\n\n\tif unmarshaled[\"with_vector\"] != false {\n\t\tt.Error(\"with_vector should be false\")\n\t}\n}\n",
    "kind": "text"
  },
  {
    "path": "api\\internal\\config\\config.go",
    "ext": ".go",
    "size": 1109,
    "sig": "4b70d0f99e95fc1180d3edf8db21d5263a8604cb",
    "head": "package config\n\nimport (\n\t\"os\"\n\t\"strconv\"\n)\n\n// Config holds all configuration values\ntype Config struct {\n\tPostgresDSN       string\n\tQdrantURL         string\n\tOllamaURL         string\n\tWorkerBase        string\n\tQdrantCollection  string\n\tEmbeddingsModel   string\n\tSearchTopK        int\n}\n\n// Load loads configuration from environment variables\nfunc Load() *Config {\n\ttopK, _ := strconv.Atoi(getEnv(\"SEARCH_TOPK\", \"5\"))\n\t\n\treturn &Config{\n\t\tPostgresDSN:      getEnv(\"POSTGRES_DSN\", \"\"),\n\t\tQdrantURL:        getEnv(\"QDRANT_URL\", \"http://host.docker.internal:6333\"),\n\t\tOllamaURL:        getEnv(\"OLLAMA_URL\", \"http://host.docker.internal:11434\"),\n\t\tWorkerBase:       getEnv(\"WORKER_BASE\", \"http://worker:8090\"),\n\t\tQdrantCollection: getEnv(\"QDRANT_COLLECTION\", \"jsonify2ai_chunks\"),\n\t\tEmbeddingsModel:  getEnv(\"EMBEDDINGS_MODEL\", \"nomic-embed-text\"),\n\t\tSearchTopK:       topK,\n\t}\n}\n\n// getEnv gets an environment variable with a default value\nfunc getEnv(key, defaultValue string) string {\n\tif value := os.Getenv(key); value != \"\" {\n\t\treturn value\n\t}\n\treturn defaultValue\n}\n",
    "kind": "text"
  },
  {
    "path": "api\\internal\\http\\router.go",
    "ext": ".go",
    "size": 924,
    "sig": "2f28aaf106cef1de80943b7ed7c7e24ed98dad7e",
    "head": "package http\n\nimport (\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/Mugiwara555343/jsonify2ai/api/internal/http/handlers\"\n\t\"github.com/Mugiwara555343/jsonify2ai/api/internal/routes\"\n\t\"github.com/Mugiwara555343/jsonify2ai/api/internal/config\"\n\t\"github.com/Mugiwara555343/jsonify2ai/api/internal/clients/ollama\"\n\t\"github.com/Mugiwara555343/jsonify2ai/api/internal/clients/qdrant\"\n)\n\nfunc SetupRouter(cfg *config.Config) *gin.Engine {\n\tr := gin.Default()\n\n\t// Initialize clients\n\tollamaClient := ollama.NewClient(cfg.OllamaURL, cfg.EmbeddingsModel)\n\tqdrantClient := qdrant.NewClient(cfg.QdrantURL, cfg.QdrantCollection)\n\n\t// Health endpoint\n\tr.GET(\"/health\", func(c *gin.Context) {\n\t\tc.JSON(200, gin.H{\n\t\t\t\"ok\": true,\n\t\t})\n\t})\n\n\t// Upload endpoint\n\tr.POST(\"/upload\", handlers.UploadHandler)\n\n\t// Search endpoint\n\tr.GET(\"/search\", routes.SearchHandler(cfg, ollamaClient, qdrantClient))\n\n\treturn r\n}\n",
    "kind": "text"
  },
  {
    "path": "api\\internal\\http\\handlers\\upload.go",
    "ext": ".go",
    "size": 5153,
    "sig": "bed637f4956134aa9a64891674f58519043c7489",
    "head": "package handlers\n\nimport (\n\t\"bytes\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"net/http\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strings\"\n\t\"unicode/utf8\"\n\n\t\"github.com/gin-gonic/gin\"\n\t\"github.com/google/uuid\"\n)\n\nconst (\n\tmaxFileSize = 5 * 1024 * 1024 // 5MB\n)\n\ntype WorkerResponse struct {\n\tOk         bool   `json:\"ok\"`\n\tChunks     int    `json:\"chunks\"`\n\tEmbedded   int    `json:\"embedded\"`\n\tUpserted   int    `json:\"upserted\"`\n\tCollection string `json:\"collection\"`\n\tError      string `json:\"error,omitempty\"`\n}\n\ntype UploadResponse struct {\n\tOk         bool           `json:\"ok\"`\n\tDocumentID string         `json:\"document_id\"`\n\tFilename   string         `json:\"filename\"`\n\tSize       int64          `json:\"size\"`\n\tMime       string         `json:\"mime\"`\n\tWorker     WorkerResponse `json:\"worker\"`\n}\n\nfunc UploadHandler(c *gin.Context) {\n\t// Parse multipart form with 5MB limit\n\tif err := c.Request.ParseMultipartForm(maxFileSize); err != nil {\n\t\tc.JSON(http.StatusBadRequest, gin.H{\"ok\": false, \"error\": \"Failed to parse form\"})\n\t\treturn\n\t}\n\tdefer c.Request.MultipartForm.RemoveAll()\n\n\t// Get the file from form\n\tfile, header, err := c.Request.FormFile(\"file\")\n\tif err != nil {\n\t\tc.JSON(http.StatusBadRequest, gin.H{\"ok\": false, \"error\": \"No file provided\"})\n\t\treturn\n\t}\n\tdefer file.Close()\n\n\t// Check file size\n\tif header.Size > maxFileSize {\n\t\tc.JSON(http.StatusRequestEntityTooLarge, gin.H{\"ok\": false, \"error\": \"File too large (>5MB)\"})\n\t\treturn\n\t}\n\n\t// Generate document ID\n\tdocumentID := uuid.New().String()\n\n\t// Create data directory\n\tdataDir := filepath.Join(\".\", \"data\", \"documents\", documentID)\n\tif err := os.MkdirAll(dataDir, 0755); err != nil {\n\t\tc.JSON(http.StatusInternalServerError, gin.H{\"ok\": false, \"error\": \"Failed to create directory\"})\n\t\treturn\n\t}\n\n\t// Save file to disk\n\tfilePath := filepath.Join(dataDir, header.Filename)\n\tdst, err := os.Create(filePath)\n\tif err != nil {\n\t\tc.JSON(http.StatusInternalServerError, gin.H{\"ok\": false, \"error\": \"Failed to save file\"})\n\t\treturn\n\t}\n\tdefer dst.Close()\n\n\t// Copy file content\n\twritten, err := io.Copy(dst, file)\n\tif err != nil {\n\t\tc.JSON(http.StatusInternalServerError, gin.H{\"ok\": false, \"error\": \"Failed to write file\"})\n\t\treturn\n\t}\n\n\t// Read file content as UTF-8 text\n\tfile.Seek(0, 0) // Reset to beginning\n\tcontent, err := io.ReadAll(file)\n\tif err != nil {\n\t\tc.JSON(http.StatusInternalServerError, gin.H{\"ok\": false, \"error\": \"Failed to read file content\"})\n\t\treturn\n\t}\n\n\t// Ensure valid UTF-8\n\tif !utf8.Valid(content) {\n\t\tcontent = bytes.ToValidUTF8(content, []byte(\"\"))\n\t}\n\n\t// Detect MIME type\n\tmimeType := detectMimeType(header.Filename, content)\n\n\t// Call worker service\n\tworkerBase := os.Getenv(\"WORKER_BASE\")\n\tif workerBase == \"\" {\n\t\tworkerBase = \"http://worker:8090\"\n\t}\n\n\tworkerURL := workerBase + \"/process/text\"\n\tworkerPayload := map[string]interface{}{\n\t\t\"document_id\": documentID,\n\t\t\"text\":        string(content),\n\t}\n\n\tworkerResp, err := callWorkerService(workerURL, workerPayload)\n\tif err != nil {\n\t\tc.JSON(http.StatusBadGateway, gin.H{\"ok\": false, \"error\": fmt.Sprintf(\"Worker service error: %v\", err)})\n\t\treturn\n\t}\n\n\t// Check if worker returned ok=false\n\tif !workerResp.Ok {\n\t\tc.JSON(http.StatusBadGateway, gin.H{\n\t\t\t\"ok\":     false,\n\t\t\t\"error\":  fmt.Sprintf(\"Worker processing failed: %s\", workerResp.Error),\n\t\t\t\"worker\": workerResp,\n\t\t})\n\t\treturn\n\t}\n\n\t// Build response\n\tresponse := UploadResponse{\n\t\tOk:         true,\n\t\tDocumentID: documentID,\n\t\tFilename:   header.Filename,\n\t\tSize:       written,\n\t\tMime:       mimeType,\n\t\tWorker:     *workerResp,\n\t}\n\n\tc.JSON(http.StatusOK, response)\n}\n\nfunc callWorkerService(url string, payload map[string]interface{}) (*WorkerResponse, error) {\n\tjsonData, err := json.Marshal(payload)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to marshal payload: %v\", err)\n\t}\n\n\tresp, err := http.Post(url, \"application/json\", bytes.NewBuffer(jsonData))\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to call worker: %v\", err)\n\t}\n\tdefer resp.Body.Close()\n\n\tif resp.StatusCode != http.StatusOK {\n\t\tbody, _ := io.ReadAll(resp.Body)\n\t\treturn nil, fmt.Errorf(\"worker returned status %d: %s\", resp.StatusCode, string(body))\n\t}\n\n\tvar workerResp WorkerResponse\n\tif err := json.NewDecoder(resp.Body).Decode(&workerResp); err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to decode worker response: %v\", err)\n\t}\n\n\treturn &workerResp, nil\n}\n\nfunc detectMimeType(filename string, content []byte) string {\n\text := strings.ToLower(filepath.Ext(filename))\n\n\tswitch ext {\n\tcase \".txt\":\n\t\treturn \"text/plain\"\n\tcase \".md\":\n\t\treturn \"text/markdown\"\n\tcase \".json\":\n\t\treturn \"application/json\"\n\tcase \".xml\":\n\t\treturn \"application/xml\"\n\tcase \".html\", \".htm\":\n\t\treturn \"text/html\"\n\tcase \".css\":\n\t\treturn \"text/css\"\n\tcase \".js\":\n\t\treturn \"application/javascript\"\n\tdefault:\n\t\t// Try to detect from content\n\t\tif len(content) > 0 && content[0] == '{' {\n\t\t\treturn \"application/json\"\n\t\t}\n\t\tif len(content) > 0 && content[0] == '<' {\n\t\t\treturn \"text/html\"\n\t\t}\n\t\treturn \"text/plain\"\n\t}\n}\n",
    "kind": "text"
  },
  {
    "path": "api\\internal\\routes\\search.go",
    "ext": ".go",
    "size": 3492,
    "sig": "b66722fa87cd7f4075bdeed0f560c67dd74f3254",
    "head": "package routes\n\nimport (\n\t\"fmt\"\n\t\"log\"\n\t\"net/http\"\n\t\"strings\"\n\n\t\"github.com/Mugiwara555343/jsonify2ai/api/internal/clients/ollama\"\n\t\"github.com/Mugiwara555343/jsonify2ai/api/internal/clients/qdrant\"\n\t\"github.com/Mugiwara555343/jsonify2ai/api/internal/config\"\n\t\"github.com/gin-gonic/gin\"\n)\n\n// SearchResult represents a search result in the API response\ntype SearchResult struct {\n\tKind       string  `json:\"kind\"`\n\tScore      float64 `json:\"score\"`\n\tText       string  `json:\"text\"`\n\tCaption    string  `json:\"caption,omitempty\"`\n\tDocumentID string  `json:\"document_id\"`\n\tSourcePath string  `json:\"source_path,omitempty\"`\n}\n\n// SearchResponse represents the search API response\ntype SearchResponse struct {\n\tOK         bool           `json:\"ok\"`\n\tResults    []SearchResult `json:\"results\"`\n\tCollection string         `json:\"collection\"`\n\tCount      int            `json:\"count\"`\n}\n\n// SearchHandler handles the GET /search endpoint\nfunc SearchHandler(cfg *config.Config, ollamaClient *ollama.Client, qdrantClient *qdrant.Client) gin.HandlerFunc {\n\treturn func(c *gin.Context) {\n\t\tquery := strings.TrimSpace(c.Query(\"q\"))\n\t\tif query == \"\" {\n\t\t\tlog.Printf(\"Search request missing query parameter\")\n\t\t\tc.JSON(http.StatusBadRequest, gin.H{\n\t\t\t\t\"ok\":    false,\n\t\t\t\t\"error\": \"Missing required query parameter 'q'\",\n\t\t\t})\n\t\t\treturn\n\t\t}\n\n\t\tlog.Printf(\"Processing search request for query: %s\", query)\n\n\t\t// Step 1: Embed the query using Ollama\n\t\tembedding, err := ollamaClient.EmbedText(query)\n\t\tif err != nil {\n\t\t\tlog.Printf(\"Failed to embed query: %v\", err)\n\t\t\tc.JSON(http.StatusBadGateway, gin.H{\n\t\t\t\t\"ok\":    false,\n\t\t\t\t\"error\": fmt.Sprintf(\"Failed to process query embedding: %v\", err),\n\t\t\t})\n\t\t\treturn\n\t\t}\n\n\t\t// Step 2: Search Qdrant for similar vectors\n\t\tresults, err := qdrantClient.SearchPoints(embedding, cfg.SearchTopK)\n\t\tif err != nil {\n\t\t\tlog.Printf(\"Failed to search Qdrant: %v\", err)\n\t\t\tc.JSON(http.StatusBadGateway, gin.H{\n\t\t\t\t\"ok\":    false,\n\t\t\t\t\"error\": \"Failed to search vector database\",\n\t\t\t})\n\t\t\treturn\n\t\t}\n\n\t\t// Step 3: Map Qdrant results to API response format\n\t\tsearchResults := make([]SearchResult, 0, len(results))\n\t\tfor _, result := range results {\n\t\t\tsearchResult := mapQdrantResult(result)\n\t\t\tif searchResult != nil {\n\t\t\t\tsearchResults = append(searchResults, *searchResult)\n\t\t\t}\n\t\t}\n\n\t\t// Step 4: Return response\n\t\tresponse := SearchResponse{\n\t\t\tOK:         true,\n\t\t\tResults:    searchResults,\n\t\t\tCollection: cfg.QdrantCollection,\n\t\t\tCount:      len(searchResults),\n\t\t}\n\n\t\tc.JSON(http.StatusOK, response)\n\t}\n}\n\n// mapQdrantResult maps a Qdrant search result to the API response format\nfunc mapQdrantResult(result qdrant.SearchResult) *SearchResult {\n\t// Extract required fields from payload\n\ttext, ok := result.Payload[\"text\"].(string)\n\tif !ok || text == \"\" {\n\t\treturn nil // Skip results without text\n\t}\n\n\tdocumentID, ok := result.Payload[\"document_id\"].(string)\n\tif !ok || documentID == \"\" {\n\t\treturn nil // Skip results without document_id\n\t}\n\n\tsearchResult := &SearchResult{\n\t\tKind:       \"text\",\n\t\tScore:      result.Score,\n\t\tText:       text,\n\t\tDocumentID: documentID,\n\t}\n\n\t// Extract optional fields\n\tif caption, ok := result.Payload[\"caption\"].(string); ok && caption != \"\" {\n\t\tsearchResult.Caption = caption\n\t}\n\n\tif path, ok := result.Payload[\"path\"].(string); ok && path != \"\" {\n\t\tsearchResult.SourcePath = path\n\t}\n\n\treturn searchResult\n}\n",
    "kind": "text"
  },
  {
    "path": "api\\internal\\routes\\search_test.go",
    "ext": ".go",
    "size": 2983,
    "sig": "4b2736d02689ed5ff453e46deead8ec810f1fcbe",
    "head": "package routes\n\nimport (\n\t\"testing\"\n\n\t\"github.com/Mugiwara555343/jsonify2ai/api/internal/clients/qdrant\"\n)\n\nfunc TestMapQdrantResult(t *testing.T) {\n\ttests := []struct {\n\t\tname     string\n\t\tinput    qdrant.SearchResult\n\t\texpected *SearchResult\n\t}{\n\t\t{\n\t\t\tname: \"complete payload\",\n\t\t\tinput: qdrant.SearchResult{\n\t\t\t\tID:    \"test-id\",\n\t\t\t\tScore: 0.85,\n\t\t\t\tPayload: map[string]interface{}{\n\t\t\t\t\t\"text\":        \"sample text content\",\n\t\t\t\t\t\"document_id\": \"doc-123\",\n\t\t\t\t\t\"caption\":     \"sample caption\",\n\t\t\t\t\t\"path\":        \"/path/to/file.txt\",\n\t\t\t\t},\n\t\t\t},\n\t\t\texpected: &SearchResult{\n\t\t\t\tKind:        \"text\",\n\t\t\t\tScore:       0.85,\n\t\t\t\tText:        \"sample text content\",\n\t\t\t\tCaption:     \"sample caption\",\n\t\t\t\tDocumentID:  \"doc-123\",\n\t\t\t\tSourcePath:  \"/path/to/file.txt\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"minimal payload\",\n\t\t\tinput: qdrant.SearchResult{\n\t\t\t\tID:    \"test-id-2\",\n\t\t\t\tScore: 0.75,\n\t\t\t\tPayload: map[string]interface{}{\n\t\t\t\t\t\"text\":        \"another text\",\n\t\t\t\t\t\"document_id\": \"doc-456\",\n\t\t\t\t},\n\t\t\t},\n\t\t\texpected: &SearchResult{\n\t\t\t\tKind:       \"text\",\n\t\t\t\tScore:      0.75,\n\t\t\t\tText:       \"another text\",\n\t\t\t\tDocumentID: \"doc-456\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"missing text\",\n\t\t\tinput: qdrant.SearchResult{\n\t\t\t\tID:    \"test-id-3\",\n\t\t\t\tScore: 0.65,\n\t\t\t\tPayload: map[string]interface{}{\n\t\t\t\t\t\"document_id\": \"doc-789\",\n\t\t\t\t},\n\t\t\t},\n\t\t\texpected: nil, // Should return nil when text is missing\n\t\t},\n\t\t{\n\t\t\tname: \"missing document_id\",\n\t\t\tinput: qdrant.SearchResult{\n\t\t\t\tID:    \"test-id-4\",\n\t\t\t\tScore: 0.55,\n\t\t\t\tPayload: map[string]interface{}{\n\t\t\t\t\t\"text\": \"some text\",\n\t\t\t\t},\n\t\t\t},\n\t\t\texpected: nil, // Should return nil when document_id is missing\n\t\t},\n\t}\n\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tresult := mapQdrantResult(tt.input)\n\t\t\t\n\t\t\tif tt.expected == nil {\n\t\t\t\tif result != nil {\n\t\t\t\t\tt.Errorf(\"Expected nil result, got %+v\", result)\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\t\t\t\n\t\t\tif result == nil {\n\t\t\t\tt.Errorf(\"Expected result %+v, got nil\", tt.expected)\n\t\t\t\treturn\n\t\t\t}\n\t\t\t\n\t\t\tif result.Kind != tt.expected.Kind {\n\t\t\t\tt.Errorf(\"Kind mismatch: expected %s, got %s\", tt.expected.Kind, result.Kind)\n\t\t\t}\n\t\t\t\n\t\t\tif result.Score != tt.expected.Score {\n\t\t\t\tt.Errorf(\"Score mismatch: expected %f, got %f\", tt.expected.Score, result.Score)\n\t\t\t}\n\t\t\t\n\t\t\tif result.Text != tt.expected.Text {\n\t\t\t\tt.Errorf(\"Text mismatch: expected %s, got %s\", tt.expected.Text, result.Text)\n\t\t\t}\n\t\t\t\n\t\t\tif result.DocumentID != tt.expected.DocumentID {\n\t\t\t\tt.Errorf(\"DocumentID mismatch: expected %s, got %s\", tt.expected.DocumentID, result.DocumentID)\n\t\t\t}\n\t\t\t\n\t\t\tif result.Caption != tt.expected.Caption {\n\t\t\t\tt.Errorf(\"Caption mismatch: expected %s, got %s\", tt.expected.Caption, result.Caption)\n\t\t\t}\n\t\t\t\n\t\t\tif result.SourcePath != tt.expected.SourcePath {\n\t\t\t\tt.Errorf(\"SourcePath mismatch: expected %s, got %s\", tt.expected.SourcePath, result.SourcePath)\n\t\t\t}\n\t\t})\n\t}\n}\n",
    "kind": "text"
  },
  {
    "path": "db\\README.md",
    "ext": ".md",
    "size": 290,
    "sig": "07df2c099321409b7d4e955c77d398e433e09d03",
    "head": "# Database\n\nDatabase migrations and schema for the jsonify2ai memory system.\n\n## Migrations\n\nRun migrations using your preferred PostgreSQL migration tool.\n\n## Schema\n\n- `documents`: Document metadata\n- `chunks`: Text chunks from documents\n- `images`: Image metadata and paths\n",
    "kind": "text"
  },
  {
    "path": "db\\migrations\\0001_init.sql",
    "ext": ".sql",
    "size": 641,
    "sig": "a43ae3e935075d13584c0017a35e793d1f7bf4c9",
    "head": "CREATE TABLE documents (\n  id UUID PRIMARY KEY,\n  filename TEXT NOT NULL,\n  kind TEXT CHECK (kind IN ('text','image','pdf','audio')) NOT NULL,\n  size_bytes BIGINT,\n  mime TEXT,\n  created_at TIMESTAMPTZ DEFAULT now()\n);\n\nCREATE TABLE chunks (\n  id UUID PRIMARY KEY,\n  document_id UUID REFERENCES documents(id) ON DELETE CASCADE,\n  idx INT NOT NULL,\n  text TEXT NOT NULL,\n  created_at TIMESTAMPTZ DEFAULT now()\n);\n\nCREATE TABLE images (\n  id UUID PRIMARY KEY,\n  document_id UUID REFERENCES documents(id) ON DELETE CASCADE,\n  path TEXT NOT NULL,\n  caption TEXT,\n  tags TEXT[],\n  created_at TIMESTAMPTZ DEFAULT now()\n);\n",
    "kind": "text"
  },
  {
    "path": "docs\\DB.md",
    "ext": ".md",
    "size": 1513,
    "sig": "9c7ef7b71a26bdc27446678ba37bf728aa56bbcf",
    "head": "# Database Design\n\n## Architecture\n\nThe jsonify2ai memory system uses a hybrid database approach:\n\n- **PostgreSQL**: Stores metadata for documents, chunks, and images\n- **Qdrant**: Stores vector embeddings for semantic search\n\n## Rationale\n\n### Why PostgreSQL for Metadata?\n\nPostgreSQL is ideal for structured metadata because:\n- ACID compliance for data integrity\n- Rich querying capabilities with SQL\n- Excellent support for JSON fields and arrays\n- Mature ecosystem with robust tooling\n\n### Why Qdrant for Vectors?\n\nQdrant is specialized for vector operations:\n- Optimized for similarity search\n- Supports multiple distance metrics\n- Efficient indexing for high-dimensional vectors\n- Built-in support for metadata filtering\n\n## Schema Overview\n\n### Documents Table\nStores metadata about uploaded files:\n- File information (name, type, size)\n- Content type classification\n- Timestamps for tracking\n\n### Chunks Table\nStores text segments extracted from documents:\n- Links to parent document\n- Sequential indexing for reconstruction\n- Text content for processing\n\n### Images Table\nStores metadata about images extracted from documents:\n- File paths for storage\n- Captions and tags for searchability\n- Links to parent document\n\n## Data Flow\n\n1. Documents are uploaded and metadata stored in PostgreSQL\n2. Content is processed into chunks/images\n3. Chunks are vectorized and stored in Qdrant\n4. Search queries use Qdrant for similarity, PostgreSQL for filtering\n",
    "kind": "text"
  },
  {
    "path": "docs\\demo.md",
    "ext": ".md",
    "size": 2254,
    "sig": "8789d8ff333e5f32b5ea6a0d483a6c7a06cc885e",
    "head": "# Manual Smoke Tests\n\nThis document outlines manual testing steps to verify the jsonify2ai memory system is working correctly.\n\n## Prerequisites\n\n1. External services running:\n   - PostgreSQL on `host.docker.internal:5432`\n   - Qdrant on `host.docker.internal:6333`\n   - Ollama on `host.docker.internal:11434`\n\n2. Environment configured:\n   ```bash\n   cp .env.example .env\n   # Edit .env with your actual connection details\n   ```\n\n## Test Steps\n\n### 1. Start All Services\n\n```bash\nmake up\n```\n\nWait for all containers to start and be healthy.\n\n### 2. Check Service Health\n\nVisit the web interface: http://localhost:5173\n\nYou should see:\n- \u2705 API Service: Healthy\n- \u2705 Worker Service: Healthy\n\n### 3. Test API Health Endpoint\n\n```bash\ncurl http://localhost:8080/health\n```\n\nExpected response:\n```json\n{\"ok\": true}\n```\n\n### 4. Test Worker Health Endpoint\n\n```bash\ncurl http://localhost:8090/health\n```\n\nExpected response:\n```json\n{\"ok\": true}\n```\n\n### 5. Test Worker Root Endpoint\n\n```bash\ncurl http://localhost:8090/\n```\n\nExpected response:\n```json\n{\"message\": \"jsonify2ai Worker Service\"}\n```\n\n### 6. Verify Database Connection\n\nCheck that the API service can connect to PostgreSQL by looking at the logs:\n\n```bash\nmake logs api\n```\n\nYou should see startup messages without connection errors.\n\n### 7. Verify External Service Connectivity\n\nCheck that services can reach external dependencies:\n\n```bash\n# Test PostgreSQL connection\ndocker compose exec api ping host.docker.internal\n\n# Test Qdrant connection  \ndocker compose exec worker ping host.docker.internal\n```\n\n## Troubleshooting\n\n### Services Not Starting\n\n1. Check if external services are running\n2. Verify `.env` file configuration\n3. Check Docker logs: `make logs`\n\n### Health Checks Failing\n\n1. Verify ports are not already in use\n2. Check container logs for specific errors\n3. Ensure external services are accessible from Docker\n\n### Database Connection Issues\n\n1. Verify PostgreSQL is running and accessible\n2. Check connection string in `.env`\n3. Ensure database `jsonify2ai` exists\n\n## Cleanup\n\n```bash\nmake down\n```\n\nThis will stop all services and remove containers.\n",
    "kind": "text"
  },
  {
    "path": "docs\\README.md",
    "ext": ".md",
    "size": 161,
    "sig": "7700c43cad47b0e0eb6e3a2ead1cf55f629e7e1f",
    "head": "# Documentation\n\nDocumentation for the jsonify2ai memory system.\n\n## Contents\n\n- `DB.md`: Database design rationale\n- `demo.md`: Manual smoke test steps\n",
    "kind": "text"
  },
  {
    "path": "docs\\assets\\seed.png",
    "ext": ".png",
    "size": 67,
    "sig": "42340eefc831123477062c6a340434adcddb42e0",
    "head": "PNG\n\u001a\n\u0000\u0000\u0000\nIHDR\u0000\u0000\u0000\u0001\u0000\u0000\u0000\u0001\b\u0004\u0000\u0000\u0000\u001c\f\u0002\u0000\u0000\u0000\u000bIDATxc\u0000\u0001\u0000\u0000\u0005\u0000\u0001\n-\u0000\u0000\u0000\u0000IENDB`",
    "kind": "image"
  },
  {
    "path": "jsonify2ai\\cli.py",
    "ext": ".py",
    "size": 286,
    "sig": "0a1eabdb24ab4be6b9ad4337f6f8c6f70c51318a",
    "head": "import typer\napp = typer.Typer(help=\"jsonify2ai: ingest \u2192 embed \u2192 store \u2192 retrieve\")\n\n# subcommand placeholders\n@app.command()\ndef version():\n  \"\"\"Show version.\"\"\"\n  import importlib.metadata as md\n  print(md.version(\"jsonify2ai\"))\n\nif __name__ == \"__main__\":\n  app()\n",
    "kind": "text"
  },
  {
    "path": "jsonify2ai\\__init__.py",
    "ext": ".py",
    "size": 14,
    "sig": "331901c21d1bcfec25414910d98e6c3f0842d197",
    "head": "__all__ = []\n",
    "kind": "text"
  },
  {
    "path": "jsonify2ai\\modules\\note2json\\.editorconfig",
    "ext": "",
    "size": 188,
    "sig": "ef6c94789f35bc2705a80459c0554c74101fab7e",
    "head": "root = true\n\n[*]\nend_of_line = lf\ninsert_final_newline = true\ncharset = utf-8\nindent_style = space\nindent_size = 4\n\n[*.{yml,yaml}]\nindent_size = 2\n\n[*.md]\ntrim_trailing_whitespace = false\n",
    "kind": "text"
  },
  {
    "path": "jsonify2ai\\modules\\note2json\\.pre-commit-config.yaml",
    "ext": ".yaml",
    "size": 494,
    "sig": "d325d0a8513e2c20dced431af244e0150ac0d7fd",
    "head": "repos:\n  - repo: https://github.com/psf/black\n    rev: 24.8.0\n    hooks:\n      - id: black\n        args: [--line-length=100]\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: v0.5.6\n    hooks:\n      - id: ruff\n        args: [--fix]\n      - id: ruff-format\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.6.0\n    hooks:\n      - id: check-yaml\n      - id: end-of-file-fixer\n      - id: trailing-whitespace\n      - id: mixed-line-ending\n        args: [--fix=lf]\n",
    "kind": "text"
  },
  {
    "path": "jsonify2ai\\modules\\note2json\\CHANGELOG.md",
    "ext": ".md",
    "size": 1219,
    "sig": "a13d12b68f0050a7e7de5980df7486588568d65f",
    "head": "# Changelog\n\nAll notable changes to this project will be documented in this file.\n\n## 0.2.3\n- feat: CLI `--version` flag\n- chore: add pre-commit (black/ruff) and normalize LF line endings\n\ndocs/quick-start\n=======\n\nmain\n## 0.2.2 \u2014 2025-01-XX\n- feat(cli): add `--no-emoji` flag to disable emoji in status output\n- feat(encoding): automatic encoding detection for UTF-8, UTF-8 BOM, UTF-16 LE/BE\n- feat(parser): improved JSON input validation with clear error messages\n- feat(cli): clear, actionable error messages for decode/JSON issues\n- feat(tests): comprehensive test coverage for encodings, stdin, globbing, multi-file outputs\n- feat(ci): matrix CI with ubuntu-latest, windows-latest, macos-latest on Python 3.10-3.12\n- feat(dev): pre-commit hooks and .editorconfig for consistent code formatting\n- docs: Windows-specific notes for PowerShell encoding and command chaining\n- docs: troubleshooting section for common encoding and JSON issues\n\n## 0.1.6 \u2014 version bump to match latest release\n- glob support + strict exit codes from 0.1.5\n- stdout/pretty flags from 0.1.4\n\n## 0.1.1 \u2014 2025-08-07\n- fix(parser): read files with utf-8-sig to strip UTF-8 BOM\n- docs: Quickstart is path-agnostic; add CI/Release badges\n",
    "kind": "text"
  },
  {
    "path": "jsonify2ai\\modules\\note2json\\CONTRIBUTING.md",
    "ext": ".md",
    "size": 1763,
    "sig": "68361b74a962cfd3d5fa27f7e44be5dcf5f5dcb3",
    "head": "# Contributing to note-to-json\n\nThanks for wanting to help. Keep it simple, keep it shippable.\n\n## Quick Setup\n\n```bash\n# 1) Fork + clone your fork, then:\npython -m venv .venv\nsource .venv/bin/activate     # Windows: .venv\\Scripts\\Activate.ps1\npython -m pip install --upgrade pip\n\n# 2) Dev install\npip install -e \".[dev]\"       # uses pyproject.toml extras\n\n# 3) Run tests\npytest -q\n```\n\n### Windows tips\n- Use PowerShell backticks for newlines inside strings: `` `n ``\n- File encodings: the parser reads **utf-8-sig** to handle BOM safely.\n\n## Running the CLI locally\n\n```bash\nnote2json demo.md -o out.json\nnote2json *.md --stdout | jq\necho {\"a\":1} | note2json --stdin --input-format auto --stdout\n```\n\n## Coding Guidelines\n- Keep functions small, pure, and covered by tests.\n- Prefer explicit flags over magic behavior.\n- Exit codes: `0=ok`, `2=missing input`, `3=parse failure`.\n\n## Tests\n- Put tests in `tests/` using `pytest`.\n- Add a minimal integration test when you touch the CLI wiring.\n- Run locally:\n  ```bash\n  pytest -q\n  ```\n\n## Commits & PRs\n- Conventional messages (example):\n  - `feat(cli): add --stdin`\n  - `fix(parser): strip BOM on Windows`\n  - `docs(readme): add quickstart`\n- Small PRs with a short \u201cwhat/why\u201d and before/after examples are best.\n- Update README if behavior or flags change.\n\n## Release Process (maintainers)\n- Bump version in `pyproject.toml` (semver).\n- Tag the release: `git tag vX.Y.Z -m \"...\" && git push origin vX.Y.Z`\n- **Publishing** is done via GitHub Actions (Trusted Publisher) to TestPyPI/PyPI.\n- Update GitHub Release notes (copy changelog highlights + install line).\n\n## Security\n- This tool runs locally and offline. If you find a parsing or execution risk,\n  open a private issue or email the maintainer.\n",
    "kind": "text"
  },
  {
    "path": "jsonify2ai\\modules\\note2json\\LICENSE",
    "ext": "",
    "size": 1136,
    "sig": "03c0463e0091ef25ea9003cff7edeae801c00a72",
    "head": "# from your repo root\nSet-Content -Encoding UTF8 LICENSE @'\nMIT License\n\nCopyright (c) 2025 Mauricio Ventura\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n'@\n",
    "kind": "text"
  },
  {
    "path": "jsonify2ai\\modules\\note2json\\memory_watcher.py",
    "ext": ".py",
    "size": 3326,
    "sig": "065aef295251b9e861ca00f4610cf7ed714f2ba5",
    "head": "#!/usr/bin/env python3\n\"\"\"\nmemory_watcher.py (Demo Version)\n\nWatches current folder for changes to `.md` files\n\u2192 Auto-triggers parser from memory_parser.py\n\u2192 Creates `.parsed.json` in same folder\n\"\"\"\n\nimport json\nimport time\nimport logging\nimport threading\nfrom hashlib import md5\nfrom pathlib import Path\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\n\n# === LOCAL CONFIG ===\nPROJECT_ROOT = Path(__file__).parent\nMEMORY_DIR = PROJECT_ROOT\nCONFIG_PATH = PROJECT_ROOT / \"watch_config.json\"\n\n# === LOAD WATCH CONFIG ===\nif CONFIG_PATH.exists():\n    try:\n        cfg = json.loads(CONFIG_PATH.read_text(encoding=\"utf-8\"))\n        WATCH_PATHS = [PROJECT_ROOT / s for s in cfg.get(\"watch_subfolders\", [])]\n    except Exception as e:\n        print(f\"[WARN] Failed to read watch_config.json: {e}\")\n        WATCH_PATHS = [PROJECT_ROOT]\nelse:\n    WATCH_PATHS = [PROJECT_ROOT]\n\n# === LOGGING ===\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s %(levelname)s: %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n)\nlogger = logging.getLogger(\"MemoryWatcher\")\n\n# === DEBOUNCE SETTINGS ===\nDEBOUNCE_DELAY = 0.8\nFILE_EXTS = {\".md\"}\nlast_hash = {}  # path \u2192 hash\n\n# === IMPORT PARSER ===\nfrom memory_parser import parse_md_file\n\n\nclass DebouncedHandler(FileSystemEventHandler):\n    def __init__(self):\n        super().__init__()\n        self.timers = {}\n\n    def on_any_event(self, event):\n        if event.is_directory:\n            return\n        path = Path(event.src_path)\n        if path.suffix.lower() not in FILE_EXTS:\n            return\n        if path.name.startswith((\".\", \"~\")) or \".parsed\" in path.stem:\n            return\n\n        # debounce multiple events\n        if path in self.timers:\n            self.timers[path].cancel()\n\n        timer = threading.Timer(DEBOUNCE_DELAY, self._process, args=[path])\n        self.timers[path] = timer\n        timer.start()\n\n    def _process(self, path: Path):\n        try:\n            content = path.read_bytes()\n        except Exception as e:\n            logger.error(f\"Failed to read {path.name}: {e}\")\n            return\n\n        h = md5(content).hexdigest()\n        if last_hash.get(path) == h:\n            logger.info(f\"\u2194 No content change: {path.name}\")\n            return\n        last_hash[path] = h\n\n        logger.info(f\"\ud83d\udd04 Detected update: {path.name}\")\n        try:\n            parsed = parse_md_file(path)\n            out = path.with_suffix(\".parsed.json\")\n            out.write_text(\n                json.dumps(parsed, indent=2, ensure_ascii=False), encoding=\"utf-8\"\n            )\n            logger.info(f\"\u2705 Parsed \u2192 {out.name}\")\n        except Exception as e:\n            logger.error(f\"\u274c Error parsing {path.name}: {e}\", exc_info=True)\n\n\n# === MAIN ===\ndef main():\n    logger.info(\"\ud83d\udc41\ufe0f  Memory Watcher Starting\")\n    for p in WATCH_PATHS:\n        logger.info(f\"  \u2022 Watching: {p.relative_to(PROJECT_ROOT)}\")\n\n    observer = Observer()\n    handler = DebouncedHandler()\n    for p in WATCH_PATHS:\n        observer.schedule(handler, str(p), recursive=True)\n\n    observer.start()\n    try:\n        while True:\n            time.sleep(1)\n    except KeyboardInterrupt:\n        observer.stop()\n        logger.info(\"\ud83d\udc4b Memory Watcher Stopped\")\n    observer.join()\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "kind": "text"
  },
  {
    "path": "jsonify2ai\\modules\\note2json\\pyproject.toml",
    "ext": ".toml",
    "size": 1691,
    "sig": "23fe9c613aa83ccac59c2b2137cde19c084e91f3",
    "head": "[build-system]\nrequires = [\"setuptools>=61.0\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"note-to-json\"\nversion = \"0.2.3\"\ndescription = \"Convert Markdown or text files to structured JSON, offline.\"\nreadme = \"README.md\"\nlicense = {text = \"MIT\"}\nauthors = [\n    {name = \"Note to JSON Team\"}\n]\nkeywords = [\"markdown\", \"json\", \"cli\", \"privacy\", \"notes\"]\nclassifiers = [\n    \"Development Status :: 4 - Beta\",\n    \"Intended Audience :: Developers\",\n    \"License :: OSI Approved :: MIT License\",\n    \"Operating System :: OS Independent\",\n    \"Programming Language :: Python :: 3\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Programming Language :: Python :: 3.11\",\n    \"Programming Language :: Python :: 3.12\",\n    \"Topic :: Text Processing :: Markup\",\n    \"Topic :: Software Development :: Libraries :: Python Modules\",\n]\nrequires-python = \">=3.10\"\ndependencies = [\n    \"jsonschema>=4.0.0\",\n    \"chardet>=5.0.0\",\n]\n\n[project.optional-dependencies]\ndev = [\n    \"pytest>=7.0.0\",\n    \"pytest-cov>=4.0.0\",\n    \"pre-commit>=3.0.0\",\n    \"black>=24\",\n    \"ruff>=0.5\",\n]\nwatch = [\n    \"watchdog>=3.0.0\",\n]\n\n[project.scripts]\nnote2json = \"note_to_json.cli:main\"\n\n[project.urls]\nHomepage = \"https://github.com/Mugiwara555343/note2json\"\nRepository = \"https://github.com/Mugiwara555343/note2json\"\nIssues = \"https://github.com/Mugiwara555343/note2json/issues\"\n\n[tool.setuptools.packages.find]\nwhere = [\".\"]\ninclude = [\"note_to_json*\"]\n\n[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\npython_files = [\"test_*.py\"]\npython_classes = [\"Test*\"]\npython_functions = [\"test_*\"]\nmarkers = [\n    \"integration: marks tests as integration tests (deselect with '-m \\\"not integration\\\"')\"\n]\n",
    "kind": "text"
  },
  {
    "path": "jsonify2ai\\modules\\note2json\\README.md",
    "ext": ".md",
    "size": 8661,
    "sig": "45682b56ef5e775aa26210b57ab8b4e169f410d1",
    "head": "# Note to JSON\n\n[![PyPI version](https://img.shields.io/pypi/v/note-to-json.svg)](https://pypi.org/project/note-to-json/)\n[![Python versions](https://img.shields.io/pypi/pyversions/note-to-json.svg)](https://pypi.org/project/note-to-json/)\n[![License](https://img.shields.io/pypi/l/note-to-json.svg)](LICENSE)\n[![CI](https://github.com/Mugiwara555343/note2json/actions/workflows/python-ci.yml/badge.svg)](https://github.com/Mugiwara555343/note2json/actions/workflows/python-ci.yml)\n[![Publish](https://github.com/Mugiwara555343/note2json/actions/workflows/publish.yml/badge.svg)](https://github.com/Mugiwara555343/note2json/actions/workflows/publish.yml)\n[![Build](https://img.shields.io/github/actions/workflow/status/Mugiwara555343/note-to-json-demo/python-ci.yml?branch=main&logo=github&label=build)](#)\n\nConvert Markdown or text files to structured JSON, offline.\n\n## Features\n\n- **Privacy-first**: All processing happens locally, no data sent to external services\n- **Flexible input**: Supports Markdown, plain text, and JSON files\n- **Automatic encoding detection**: Handles UTF-8, UTF-16, and other encodings\n- **Batch processing**: Process multiple files with glob patterns\n- **Resilient parsing**: Graceful handling of malformed inputs and encoding issues\n- **Progress reporting**: Detailed feedback for batch operations\n- **Error recovery**: Continue processing even when some files fail\n\n## Installation\n\n```bash\npip install note-to-json\n```\n\n## \ud83d\ude80 Quick Start\n\nInstall in a virtual environment (Windows PowerShell shown):\n```powershell\npython -m venv vtest\nvtest\\Scripts\\activate\npip install --upgrade pip\npip install note-to-json\n\nCheck version:\n\nnote2json --version\n\nConvert a Markdown file to JSON:\n\nSet-Content note.md \"# hello\" -Encoding utf8\nnote2json note.md --stdout --pretty\n\nParse JSON inputs as well:\n\nSet-Content obj.json '{\"note\":\"Already JSON\"}' -Encoding utf8\nnote2json obj.json --input-format json --stdout\n\n## CLI Usage\n\n### Basic Commands\n\n```bash\nnote2json [OPTIONS] INPUT_FILE(S)\n```\n\n### Options\n\n- `-o, --output PATH`: Specify output file path\n- `--stdout`: Print JSON to STDOUT instead of writing to file\n- `--pretty`: Pretty-print JSON with 2-space indentation\n- `--stdin`: Read input from STDIN instead of files\n- `--input-format {auto,md,txt,json}`: Specify input format (default: auto)\n- `--no-emoji`: Disable emoji in status output\n- `--continue-on-error`: Continue processing remaining files even if some fail\n- `--verbose`: Show detailed progress information\n- `--retry-failed`: Automatically retry failed files with different strategies\n\n### Input Formats\n\n- **auto** (default): Automatically detect format based on content\n- **md/txt**: Parse as Markdown/plain text\n- **json**: Parse as JSON (with schema validation)\n\n### Examples\n\n```bash\n# Parse to default output file\nnote2json input.md                    # \u2192 input.parsed.json\n\n# Parse to custom output file\nnote2json input.md -o output.json     # \u2192 output.json\n\n# Parse to STDOUT\nnote2json input.md --stdout           # \u2192 prints to terminal\n\n# Pretty-print to STDOUT\nnote2json input.md --stdout --pretty  # \u2192 formatted JSON\n\n# Process multiple files\nnote2json *.md                        # \u2192 individual .parsed.json files\n\n# Continue on errors\nnote2json *.md --continue-on-error    # \u2192 process all files, report failures\n\n# Retry failed files automatically\nnote2json *.md --retry-failed         # \u2192 retry failed files with different strategies\n\n# Show progress\nnote2json *.md --verbose              # \u2192 detailed progress information\n\n# Read from STDIN (Windows)\ntype data.json | note2json --stdin --input-format json --stdout\n\n# Read from STDIN (macOS/Linux)\ncat data.json | note2json --stdin --input-format json --stdout\n```\n\n## Resilience Features\n\n### Error Handling\n\nThe CLI provides robust error handling with clear, actionable error messages:\n\n- **Encoding issues**: Automatic fallback to multiple encoding detection methods\n- **Malformed inputs**: Graceful degradation with automatic validation fixes\n- **Batch processing**: Continue processing even when individual files fail\n- **Detailed reporting**: Comprehensive error summaries with categorization\n- **Actionable advice**: Specific suggestions for fixing common issues\n- **Retry strategies**: Automatic retry with different parsing approaches\n\n### Error Types\n\n- **Missing files**: Exit code 2\n- **Parsing errors**: Exit code 3\n- **Encoding errors**: Detailed information about attempted encodings\n- **Validation errors**: Automatic fixing of common schema issues\n- **Format mismatches**: Clear guidance on input format selection\n- **Retry failures**: Information when all retry strategies fail\n\n### Enhanced Error Messages\n\nError messages now include specific, actionable advice:\n\n```bash\n# Example of enhanced error message with advice\nError: Schema validation failed at 'title': 'None' is not of type 'string'\n\ud83d\udca1 Advice: Add the missing required field 'title'\n```\n\n### Retry Logic\n\nUse `--retry-failed` to automatically attempt processing failed files with different strategies:\n\n```bash\nnote2json *.md --retry-failed\n```\n\nThe retry system will:\n1. **Format switching**: Try different input formats (txt, json, auto)\n2. **Raw text processing**: Fall back to basic text extraction\n3. **Schema relaxation**: Create minimal valid structures when possible\n4. **Detailed reporting**: Show which retry strategy succeeded\n\n### Continue on Error\n\nUse `--continue-on-error` to process all files even when some fail:\n\n```bash\nnote2json *.md --continue-on-error\n```\n\nThis will:\n- Process all files that can be parsed\n- Report failures with detailed error messages\n- Provide a summary of successful vs. failed files\n- Exit with appropriate error code\n\n### Enhanced Progress Reporting\n\nUse `--verbose` for detailed progress information with time estimation:\n\n```bash\nnote2json *.md --verbose\n```\n\nShows:\n- Current file being processed\n- Progress counter (e.g., [3/10])\n- Visual progress bar with percentage\n- Estimated time remaining (ETA)\n- Summary of results\n- Error breakdown by type\n- Troubleshooting tips for common issues\n\n## Output Schema\n\nThe tool outputs structured JSON with the following schema:\n\n```json\n{\n  \"title\": \"string\",\n  \"timestamp\": \"ISO 8601 date-time\",\n  \"raw_text\": \"string\",\n  \"plain_text\": \"string\",\n  \"tags\": [\"string\"],\n  \"headers\": [\"string\"],\n  \"date\": \"string (optional)\",\n  \"tone\": \"string (optional)\",\n  \"summary\": \"string (optional)\",\n  \"reflections\": [\"string (optional)\"]\n}\n```\n\n## Input Format Support\n\n### Markdown/Text\n\n- **Headers**: Extracts `# Title` as headers\n- **Metadata**: Parses `**Date:**`, `**Tags:**`, `**Tone:**` fields\n- **Summary**: Extracts content between `**Summary:**` and `---`\n- **Reflections**: Extracts bullet points after `**Core Reflections:**`\n\n### JSON\n\n- **Schema validation**: Ensures output matches required schema\n- **Auto-normalization**: Converts arbitrary JSON to schema format\n- **Format detection**: Automatically identifies JSON vs. text content\n\n## Encoding Support\n\n- **UTF-8**: Standard encoding with BOM support\n- **UTF-16**: Little-endian and big-endian variants\n- **Fallback detection**: Uses chardet for automatic encoding detection\n- **Error handling**: Graceful degradation with detailed error reporting\n\n## Development\n\n### Installation\n\n```bash\ngit clone https://github.com/Mugiwara555343/note2json.git\ncd note2json\npip install -e .\n```\n\n### Testing\n\n```bash\n# Run all tests\npytest\n\n# Run integration tests only\npytest -m integration\n\n# Run with coverage\npytest --cov=note_to_json\n```\n\n### Code Quality\n\n```bash\n# Format code\nblack note_to_json/ tests/\n\n# Sort imports\nisort note_to_json/ tests/\n\n# Run pre-commit hooks\npre-commit run --all-files\n```\n\n## Contributing\n\n1. Fork the repository\n2. Create a feature branch\n3. Make your changes\n4. Add tests for new functionality\n5. Ensure all tests pass\n6. Submit a pull request\n\n## License\n\nMIT License - see [LICENSE](LICENSE) file for details.\n\n## Changelog\n\n### v0.2.2\n\n- **Resilience improvements**: Better error ",
    "kind": "text"
  },
  {
    "path": "jsonify2ai\\modules\\note2json\\t -q",
    "ext": "",
    "size": 725,
    "sig": "c1a55258a01f3e3227c9bcd6698f92c393ca3869",
    "head": "\n                   S\bSU\bUM\bMM\bMA\bAR\bRY\bY O\bOF\bF L\bLE\bES\bSS\bS C\bCO\bOM\bMM\bMA\bAN\bND\bDS\bS\n\n      Commands marked with * may be preceded by a number, _\bN.\n      Notes in parentheses indicate the behavior if _\bN is given.\n      A key preceded by a caret indicates the Ctrl key; thus ^K is ctrl-K.\n\n  h  H                 Display this help.\n  q  :q  Q  :Q  ZZ     Exit.\n ---------------------------------------------------------------------------\n\n                           M\bMO\bOV\bVI\bIN\bNG\bG\n\n  e  ^E  j  ^N  CR  *  Forward  one line   (or _\bN lines).\n  y  ^Y  k  ^K  ^P  *  Backward one line   (or _\bN lines).\n  f  ^F  ^V  SPACE  *  Forward  one window (or _\bN lines).\n  b  ^B  ESC-v      *  Backward one window (or _\bN lines).\n",
    "kind": "text"
  },
  {
    "path": "jsonify2ai\\modules\\note2json\\watch_config.json",
    "ext": ".json",
    "size": 45,
    "sig": "fe3b887467d07dd93496b80609f8ddf9e310cfbc",
    "head": "{\n  \"watch_subfolders\": [ \"demo_entries\" ]\n}\n",
    "kind": "text"
  },
  {
    "path": "jsonify2ai\\modules\\note2json\\demo_entries\\creative_breakthrough.md",
    "ext": ".md",
    "size": 631,
    "sig": "e5dde17d75fe68875d71e2fb9a63c78deebcf936",
    "head": "# Creative Breakthrough\n**Date:** 2025-01-15\n**Tags:** #creativity #inspiration #breakthrough #excitement\n**Tone:** Enthusiastic\n\n**Summary:**\nFinally cracked the algorithm problem that's been blocking me for weeks! The solution came in a dream and I rushed to implement it.\n\n**Core Reflections:**\n- Sometimes the best solutions come when you stop actively thinking about the problem.\n- Creative blocks often resolve themselves with time and subconscious processing.\n- The feeling of breakthrough is pure dopamine - I need to remember this motivation.\n- Dreams can be incredible problem-solving tools if you pay attention to them.\n",
    "kind": "text"
  },
  {
    "path": "jsonify2ai\\modules\\note2json\\demo_entries\\demo_entry.md",
    "ext": ".md",
    "size": 291,
    "sig": "be39daabd54c60249f94d36109b036c348d797ba",
    "head": "# Morning Reflection\n**Date:** 2025-07-17\n**Tags:** #focus #emotion #ai\n**Tone:** Reflective\n\n**Summary:**\nToday I spent time refining the AI memory watcher and parser logic.\n\n**Core Reflections:**\n- Resilience is built through iteration.\n- System design is emotional memory made technical.\n",
    "kind": "text"
  },
  {
    "path": "jsonify2ai\\modules\\note2json\\demo_entries\\frustration_moment.md",
    "ext": ".md",
    "size": 561,
    "sig": "8b6c8af7074452408ed63c0f0f842c5963b4cc7e",
    "head": "# Frustration Moment\n**Date:** 2025-01-20\n**Tags:** #frustration #setback #learning #patience\n**Tone:** Frustrated\n\n**Summary:**\nSpent 6 hours debugging a simple bug that turned out to be a typo. Feeling exhausted and questioning my competence.\n\n**Core Reflections:**\n- When frustrated, I make more mistakes - need to recognize this pattern.\n- Sometimes stepping away is more productive than pushing through.\n- Simple bugs can be the most demoralizing because they feel like they shouldn't exist.\n- My self-worth shouldn't be tied to coding success or failure.\n",
    "kind": "text"
  },
  {
    "path": "jsonify2ai\\modules\\note2json\\demo_entries\\peaceful_morning.md",
    "ext": ".md",
    "size": 484,
    "sig": "d1119b3a99a0adb2a9049d6265842bfb6e49122d",
    "head": "# Peaceful Morning\n**Date:** 2025-01-25\n**Tags:** #peace #reflection #gratitude #mindfulness\n**Tone:** Calm\n\n**Summary:**\nWoke up early and sat with coffee watching the sunrise. Everything feels quiet and possible today.\n\n**Core Reflections:**\n- These quiet moments are when I feel most connected to myself.\n- Morning routines create space for intentional thinking.\n- Nature has a way of putting problems in perspective.\n- Gratitude practice really does shift my entire day's energy.\n",
    "kind": "text"
  },
  {
    "path": "jsonify2ai\\modules\\note2json\\demo_entries\\team_collaboration.md",
    "ext": ".md",
    "size": 622,
    "sig": "dc75afbeb417c9171fd53969a02c0ecddffc25cf",
    "head": "# Team Collaboration\n**Date:** 2025-01-30\n**Tags:** #collaboration #teamwork #growth #connection\n**Tone:** Energized\n\n**Summary:**\nHad an amazing brainstorming session with the team today. Everyone's ideas built on each other and we came up with something none of us would have thought of alone.\n\n**Core Reflections:**\n- Collective intelligence is real - the whole is greater than the sum of its parts.\n- I learn so much from seeing how others approach problems differently.\n- Good collaboration requires both speaking up and listening deeply.\n- The energy of a productive team meeting can carry me through the whole day.\n",
    "kind": "text"
  },
  {
    "path": "jsonify2ai\\modules\\note2json\\note_to_json\\cli.py",
    "ext": ".py",
    "size": 22040,
    "sig": "9a6c0083b69acfdab216aa689a889b1a913d7ed5",
    "head": "#!/usr/bin/env python3\n\"\"\"\nnote_to_json.cli\n\nCommand-line interface for the note-to-json parser.\n\"\"\"\nfrom . import __version__\nimport argparse\nimport glob\nimport json\nimport sys\nfrom pathlib import Path\nfrom typing import List, Tuple, Dict, Any, Optional\nfrom . import __version__\nfrom .parser import read_input, ParsingError\nfrom .utils import read_text_safely, read_stdin_safely\nimport time\nfrom datetime import datetime\n\n\nclass ProcessingResult:\n    \"\"\"Container for processing results with metadata.\"\"\"\n\n    def __init__(\n        self,\n        input_path: Path,\n        success: bool,\n        data: Optional[Dict[str, Any]] = None,\n        error: Optional[str] = None,\n        error_type: Optional[str] = None,\n    ):\n        self.input_path = input_path\n        self.success = success\n        self.data = data\n        self.error = error\n        self.error_type = error_type\n\n    def __str__(self):\n        if self.success:\n            return f\"\u2705 {self.input_path.name}\"\n        else:\n            return f\"\u274c {self.input_path.name}: {self.error}\"\n\n\ndef expand_glob_patterns(patterns: List[str]) -> List[str]:\n    \"\"\"Expand glob patterns and return sorted, deduplicated list of files.\"\"\"\n    expanded_files = []\n\n    for pattern in patterns:\n        # Check if pattern contains glob characters\n        if any(char in pattern for char in [\"*\", \"?\", \"[\"]):\n            # Use recursive=True if pattern contains '**'\n            recursive = \"**\" in pattern\n            try:\n                matched_files = glob.glob(pattern, recursive=recursive)\n                if not matched_files:\n                    # If glob pattern matches nothing, treat as missing file\n                    expanded_files.append(pattern)\n                else:\n                    expanded_files.extend(matched_files)\n            except Exception as e:\n                # Handle glob pattern errors gracefully\n                print(\n                    f\"Warning: Invalid glob pattern '{pattern}': {e}\", file=sys.stderr\n                )\n                expanded_files.append(pattern)\n        else:\n            # No glob characters, treat as literal path\n            expanded_files.append(pattern)\n\n    # Deduplicate and sort for deterministic order\n    return sorted(set(expanded_files))\n\n\ndef process_single_file(\n    input_path: Path, input_format: str, no_emoji: bool = False\n) -> ProcessingResult:\n    \"\"\"Process a single file and return a ProcessingResult.\"\"\"\n    try:\n        # Read file with automatic encoding detection\n        text = read_text_safely(input_path)\n        filename_hint = input_path.stem\n        parsed_data = read_input(text, input_format, filename_hint=filename_hint)\n        return ProcessingResult(input_path, True, data=parsed_data)\n\n    except ParsingError as e:\n        # Handle parsing errors with clear messages and actionable advice\n        if input_format == \"json\":\n            error_msg = \"Invalid JSON input. If this is Markdown or text, use `--input-format md|txt`.\"\n            error_type = \"format_mismatch\"\n        else:\n            error_msg = str(e)\n            error_type = getattr(e, \"error_type\", \"parsing_error\")\n\n            # Add actionable advice for validation errors\n            if hasattr(e, \"context\") and e.context and \"advice\" in e.context:\n                error_msg += f\"\\n\ud83d\udca1 Advice: {e.context['advice']}\"\n\n        return ProcessingResult(\n            input_path, False, error=error_msg, error_type=error_type\n        )\n\n    except ValueError as e:\n        # Handle encoding errors with detailed information\n        error_msg = str(e)\n        if \"Decoding error\" in str(e):\n            error_type = \"encoding_error\"\n        else:\n            error_type = \"value_error\"\n        return ProcessingResult(\n            input_path, False, error=error_msg, error_type=error_type\n        )\n\n    except Exception as e:\n        return ProcessingResult(\n            input_path, False, error=str(e), error_type=\"unexpected_error\"\n        )\n\n\ndef retry_failed_file(\n    input_path: Path, original_error: str, no_emoji: bool = False\n) -> ProcessingResult:\n    \"\"\"Retry processing a failed file with different strategies.\"\"\"\n    if no_emoji:\n        print(\n            f\"  Retrying {input_path.name} with different strategies...\",\n            file=sys.stderr,\n        )\n    else:\n        print(\n            f\"  \ud83d\udd04 Retrying {input_path.name} with different strategies...\",\n            file=sys.stderr,\n        )\n\n    # Strategy 1: Try with different input format if it was auto-detected\n    try:\n        text = read_text_safely(input_path)\n        # Try forcing text format if it might be markdown\n        if input_path.suffix.lower() in [\".md\", \".markdown\", \".txt\"]:\n            parsed_data = read_input(text, \"txt\", filename_hint=input_path.stem)\n            if no_emoji:\n                return ProcessingResult(\n                    input_path,\n                    True,\n                    data=parsed_data,\n                    error=\"Retry successful with txt format\",\n                )\n            else:\n                return ProcessingResult(\n                    input_path,\n                    True,\n                    data=parsed_data,\n                    error=\"\ud83d\udd04 Retry successful with txt format\",\n                )\n    except Exception:\n        pass\n\n    # Strategy 2: Try with JSON format if it might be JSON\n    try:\n        text = read_text_safely(input_path)\n        if input_path.suffix.lower() in [\".json\"]:\n            parsed_data = read_input(text, \"json\", filename_hint=input_path.stem)\n            if no_emoji:\n                return ProcessingResult(\n                    input_path,\n                    True,\n                    data=parsed_data,\n                    error=\"Retry successful with json format\",\n                )\n            else:\n                return ProcessingResult(\n                    input_path,\n                    True,\n                    data=parsed_data,\n                    error=\"\ud83d\udd04 Retry successful with json format\",\n                )\n    except Exception:\n        pass\n\n    # Strategy 3: Try with raw text processing (more lenient)\n    try:\n        text = read_text_safely(input_path)\n        # Create minimal valid data structure\n        parsed_data = {\n            \"title\": input_path.stem or \"untitled\",\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n            \"raw_text\": text[:1000] + (\"...\" if len(text) > 1000 else \"\"),\n            \"plain_text\": text[:1000].replace(\"\\n\", \" \")\n            + (\"...\" if len(text) > 1000 else \"\"),\n            \"tags\": [],\n            \"headers\": [],\n            \"reflections\": [],\n        }\n        if no_emoji:\n            return ProcessingResult(\n                input_path,\n                True,\n                data=parsed_data,\n                error=\"Retry successful with raw text processing\",\n            )\n        else:\n            return ProcessingResult(\n                input_path,\n                True,\n                data=parsed_data,\n                error=\"\ud83d\udd04 Retry successful with raw text processing\",\n            )\n    except Exception:\n        pass\n\n    # All retry strategies failed\n    return ProcessingResult(\n        input_path,\n        False,\n        error=f\"All retry strategies failed. Original error: {original_error}\",\n        error_type=\"retry_failed\",\n    )\n\n\ndef process_stdin(\n    input_format: str, no_emoji: bool = False\n) -> Tuple[Dict[str, Any], Optional[str]]:\n    \"\"\"Process STDIN input and return parsed data and optional error.\"\"\"\n    try:\n        # Read from STDIN with automatic encoding detection\n        text = read_stdin_safely(sys.stdin.buffer)\n        parsed_data = read_input(text, input_format, filename_hint=\"stdin\")\n        return parsed_data, None\n    except (ValueError, ParsingError) as e:\n        return {}, str(e)\n    except Exception as e:\n        return {}, f\"Failed to parse STDIN: {e}\"\n\n\ndef print_progress(\n    current: int,\n    total: int,\n    filename: str,\n    no_emoji: bool = False,\n    start_time: Optional[float]",
    "kind": "text"
  },
  {
    "path": "jsonify2ai\\modules\\note2json\\note_to_json\\parser.py",
    "ext": ".py",
    "size": 14149,
    "sig": "4ab0ec4b0b6f436a377f0c8a72c8f381db7a1072",
    "head": "#!/usr/bin/env python3\n\"\"\"\nnote_to_json.parser\n\nParses markdown/text or JSON into a structured JSON schema. Supports:\n- Markdown/text parsing with inline metadata\n- JSON passthrough/normalization\n\"\"\"\n\nimport json\nimport os\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import Optional, Dict, Any\nfrom jsonschema import validate, ValidationError\nfrom .utils import read_text_safely, decode_bytes\n\n# === SCHEMA ===\nSCHEMA = {\n    \"type\": \"object\",\n    \"required\": [\"title\", \"timestamp\", \"raw_text\", \"plain_text\"],\n    \"properties\": {\n        \"title\": {\"type\": \"string\"},\n        \"timestamp\": {\"type\": \"string\", \"format\": \"date-time\"},\n        \"raw_text\": {\"type\": \"string\"},\n        \"plain_text\": {\"type\": \"string\"},\n        \"tags\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n        \"headers\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n        \"date\": {\"type\": \"string\"},\n        \"tone\": {\"type\": \"string\"},\n        \"summary\": {\"type\": \"string\"},\n        \"reflections\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n    },\n    \"additionalProperties\": False,\n}\n\n\nclass ParsingError(ValueError):\n    \"\"\"Custom exception for parsing errors with context.\"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        error_type: str = \"parsing_error\",\n        context: Optional[Dict[str, Any]] = None,\n    ):\n        super().__init__(message)\n        self.error_type = error_type\n        self.context = context or {}\n\n\ndef _now_iso() -> str:\n    \"\"\"Get current timestamp in ISO format.\"\"\"\n    return datetime.utcnow().isoformat() + \"Z\"\n\n\ndef _normalize_from_json(\n    obj, *, raw_text: str, filename_hint: str | None = None\n) -> dict:\n    # If it already matches our schema, return as-is\n    if isinstance(obj, dict) and {\n        \"title\",\n        \"timestamp\",\n        \"raw_text\",\n        \"plain_text\",\n        \"tags\",\n        \"headers\",\n        \"reflections\",\n    } <= set(obj.keys()):\n        return obj\n    # Otherwise wrap primitive/array into our schema\n    title = (filename_hint or \"json\").split(\".\")[0]\n    if not isinstance(obj, (dict, list, str, int, float, bool)) and obj is not None:\n        obj = str(obj)\n    plain = json.dumps(obj, ensure_ascii=False) if not isinstance(obj, str) else obj\n\n    # For objects, use the title field if present, otherwise use filename_hint\n    if isinstance(obj, dict) and \"title\" in obj:\n        title = str(obj[\"title\"])\n    elif isinstance(obj, (list, dict)):\n        title = title\n    else:\n        # For primitives, use filename_hint as title, not the primitive value\n        title = title\n\n    # For objects, preserve tags if they exist and are valid\n    tags = []\n    if isinstance(obj, dict) and \"tags\" in obj and isinstance(obj[\"tags\"], list):\n        # Normalize tags: strip hash prefixes and convert to strings\n        tags = []\n        for t in obj[\"tags\"]:\n            if isinstance(t, (str, int, float, bool)):\n                if isinstance(t, bool):\n                    tag_str = \"true\" if t else \"false\"\n                else:\n                    tag_str = str(t)\n                # Strip hash prefix if present\n                if tag_str.startswith(\"#\"):\n                    tag_str = tag_str[1:]\n                tags.append(tag_str)\n\n    return {\n        \"title\": title,\n        \"timestamp\": _now_iso(),\n        \"raw_text\": raw_text,\n        \"plain_text\": plain,\n        \"tags\": tags,\n        \"headers\": [],\n        \"reflections\": [],\n    }\n\n\ndef validate_parsed(data: dict) -> None:\n    \"\"\"Validate parsed data against schema with detailed error reporting.\"\"\"\n    try:\n        validate(instance=data, schema=SCHEMA)\n    except ValidationError as e:\n        # Provide more helpful error messages with actionable advice\n        field_path = \" -> \".join(str(p) for p in e.path) if e.path else \"root\"\n\n        # Categorize validation errors for better handling\n        if e.validator == \"required\":\n            error_type = \"missing_required_field\"\n            # Extract the field name from the error message safely\n            field_name = \"required field\"\n            if \"'\" in e.message:\n                parts = e.message.split(\"'\")\n                if len(parts) >= 2:\n                    field_name = parts[1]\n            advice = f\"Add the missing required field '{field_name}'\"\n        elif e.validator == \"type\":\n            error_type = \"wrong_field_type\"\n            expected_type = e.validator_value\n            actual_value = type(e.instance).__name__\n            advice = f\"Change '{field_path}' from {actual_value} to {expected_type}\"\n        elif e.validator == \"format\":\n            error_type = \"invalid_format\"\n            advice = f\"Ensure '{field_path}' follows the required format: {e.validator_value}\"\n        else:\n            error_type = \"validation_error\"\n            advice = (\n                f\"Check the value of '{field_path}' against the schema requirements\"\n            )\n\n        raise ParsingError(\n            f\"Schema validation failed at '{field_path}': {e.message}\",\n            error_type=error_type,\n            context={\n                \"field\": field_path,\n                \"value\": e.instance,\n                \"schema_requirement\": e.validator_value,\n                \"advice\": advice,\n            },\n        )\n\n\ndef sanitize_text(text: str, max_length: int = 10000) -> str:\n    \"\"\"Sanitize text content to prevent issues.\"\"\"\n    if not isinstance(text, str):\n        text = str(text)\n\n    # Remove null bytes and other problematic characters\n    text = text.replace(\"\\x00\", \"\")\n\n    # Truncate if too long\n    if len(text) > max_length:\n        text = text[:max_length] + \"... [truncated]\"\n\n    return text\n\n\ndef _parse_text(text: str, *, filename_hint: Optional[str] = None) -> dict:\n    \"\"\"\n    Parse markdown/plain text into structured JSON.\n\n    Args:\n        text: Full file content\n        filename_hint: Used to derive a reasonable title when headers are absent\n\n    Returns:\n        dict: Parsed data with metadata, content, and reflections\n\n    Raises:\n        ParsingError: If parsing fails or validation fails\n    \"\"\"\n    try:\n        # Sanitize input text\n        text = sanitize_text(text)\n\n        if not text.strip():\n            raise ParsingError(\n                \"Empty or whitespace-only input\", error_type=\"empty_input\"\n            )\n\n        lines = text.splitlines()\n\n        raw_text = text.strip()\n        plain_text = raw_text.replace(\"\\n\", \" \")\n        title = filename_hint or \"stdin\"\n        tags, headers, reflections = [], [], []\n        date_str, tone_str, summary_text = None, None, None\n        in_summary = in_reflect = False\n\n        for line in lines:\n            line = line.rstrip(\"\\r\\n\")  # Handle different line endings\n\n            if line.startswith(\"# \"):\n                h = line.lstrip(\"# \").strip()\n                if h:  # Only add non-empty headers\n                    headers.append(h)\n                    # Use the first H1 as title\n                    if title == (filename_hint or \"stdin\"):\n                        title = h\n\n            if line.startswith(\"**Date:**\"):\n                date_str = line.split(\"**Date:**\", 1)[1].strip()\n            if line.startswith(\"**Tags:**\"):\n                vals = line.split(\"**Tags:**\", 1)[1].strip()\n                tags = [\n                    t.strip().lstrip(\"#\") for t in vals.split() if t.startswith(\"#\")\n                ]\n            if line.startswith(\"**Tone:**\"):\n                tone_str = line.split(\"**Tone:**\", 1)[1].strip()\n\n            if line.lower().startswith(\"**summary:**\"):\n                in_summary = True\n                summary_text = \"\"\n                continue\n            if in_summary:\n                if line.strip() == \"\" or line.strip().startswith(\"---\"):\n                    in_summary = False\n                else:\n                    summary_text += line.strip() + \" \"\n\n            if line.lower().startswith(\"**core reflections:**\"):\n                in_reflect = True\n                continue\n            if in_reflect:\n                if ",
    "kind": "text"
  },
  {
    "path": "jsonify2ai\\modules\\note2json\\note_to_json\\__init__.py",
    "ext": ".py",
    "size": 216,
    "sig": "4fb38eb4e01ca6ba031d2fabd1dee04b5fe764ad",
    "head": "from importlib.metadata import PackageNotFoundError, version as _pkg_version\n\n__all__ = [\"__version__\"]\n\ntry:\n    __version__ = _pkg_version(\"note-to-json\")\nexcept PackageNotFoundError:\n    __version__ = \"0+unknown\"\n",
    "kind": "text"
  },
  {
    "path": "jsonify2ai\\modules\\note2json\\note_to_json\\utils\\encoding.py",
    "ext": ".py",
    "size": 2312,
    "sig": "b8f7f8f63bf650ebb69678b9fcd82a5825a97033",
    "head": "\"\"\"\nEncoding utilities for note-to-json.\n\nProvides safe text reading functions that automatically detect and handle\nvarious text encodings including UTF-8, UTF-8 BOM, UTF-16 LE/BE.\n\"\"\"\n\nfrom typing import Iterable\n\nPREFERRED_ENCODINGS: tuple[str, ...] = (\n    \"utf-8\",\n    \"utf-8-sig\",\n    \"utf-16-le\",\n    \"utf-16-be\",\n    \"utf-16\",\n    \"cp1252\",\n    \"latin-1\",\n)\n\n\ndef decode_bytes(data: bytes, encodings: Iterable[str] = PREFERRED_ENCODINGS) -> str:\n    last_err = None\n    for enc in encodings:\n        try:\n            decoded = data.decode(enc)  # no errors=\"replace\"\n            # Strip BOM if present (utf-8-sig should handle this, but let's be explicit)\n            if decoded.startswith(\"\\ufeff\"):\n                decoded = decoded[1:]\n\n            # Validate the decoded result - if it contains too many null bytes, it's probably wrong\n            # But be more lenient for UTF-16 encodings which naturally have many null bytes\n            if len(decoded) > 0:  # Avoid division by zero\n                null_ratio = decoded.count(\"\\x00\") / len(decoded)\n                if null_ratio > 0.1 and enc not in (\"utf-16\", \"utf-16-le\", \"utf-16-be\"):\n                    continue  # Try next encoding\n\n            # Additional validation: if this is a UTF-16 encoding, check if the result looks reasonable\n            if enc in (\"utf-16\", \"utf-16-le\", \"utf-16-be\"):\n                # Check if the result contains mostly printable ASCII characters\n                printable_chars = sum(\n                    1 for c in decoded if c.isprintable() and ord(c) < 128\n                )\n                if (\n                    printable_chars < len(decoded) * 0.5\n                ):  # Less than 50% printable ASCII\n                    continue  # Try next encoding\n\n            return decoded\n        except UnicodeDecodeError as e:\n            last_err = e\n            continue\n    raise ValueError(\n        \"Decoding error: input is not valid UTF text (try saving as UTF-8).\"\n    ) from last_err\n\n\ndef read_text_safely(path):\n    with open(path, \"rb\") as f:\n        return decode_bytes(f.read())\n\n\ndef read_stdin_safely(stdin_buffer) -> str:\n    data = stdin_buffer.read()\n    if isinstance(data, str):\n        # Already text (some shells); normalize newline only\n        return data\n    return decode_bytes(data)\n",
    "kind": "text"
  },
  {
    "path": "jsonify2ai\\modules\\note2json\\note_to_json\\utils\\__init__.py",
    "ext": ".py",
    "size": 184,
    "sig": "ec9562e658078b397b855617984a1ba3b217e481",
    "head": "\"\"\"\nUtility modules for note-to-json.\n\"\"\"\n\nfrom .encoding import read_text_safely, read_stdin_safely, decode_bytes\n\n__all__ = [\"read_text_safely\", \"read_stdin_safely\", \"decode_bytes\"]\n",
    "kind": "text"
  },
  {
    "path": "jsonify2ai\\modules\\note2json\\tests\\test_bom_handling.py",
    "ext": ".py",
    "size": 1764,
    "sig": "aabcdfc6fa21f89297098827c42a014cb9506ec3",
    "head": "from pathlib import Path\nimport tempfile\nfrom note_to_json.parser import parse_file\nfrom ._helpers import normalize_text\n\n\ndef test_bom_handling():\n    \"\"\"Test that UTF-8 BOM is stripped when reading files\"\"\"\n    # Create a temporary file with UTF-8 BOM\n    with tempfile.NamedTemporaryFile(\n        mode=\"w\", suffix=\".md\", delete=False, encoding=\"utf-8-sig\"\n    ) as tmp_file:\n        tmp_file.write(\n            \"# Test Entry\\n**Date:** 2025-01-15\\n**Tags:** #test #demo\\n**Tone:** Neutral\\n\\n**Summary:**\\nThis is a test entry.\\n\\n**Core Reflections:**\\n- First reflection\\n- Second reflection\\n- Third reflection\"\n        )\n        tmp_path = Path(tmp_file.name)\n\n    try:\n        # Parse the file\n        result = parse_file(tmp_path)\n\n        # Assert BOM is stripped from raw_text\n        assert not result[\"raw_text\"].startswith(\n            \"\\ufeff\"\n        ), \"BOM not stripped from raw_text\"\n        # Use normalize_text for cross-platform stability\n        assert normalize_text(result[\"raw_text\"]).startswith(\n            \"# Test Entry\"\n        ), \"raw_text doesn't start with expected content\"\n\n        # Assert BOM is stripped from plain_text\n        assert not result[\"plain_text\"].startswith(\n            \"\\ufeff\"\n        ), \"BOM not stripped from plain_text\"\n        # Use normalize_text for cross-platform stability\n        assert normalize_text(result[\"plain_text\"]).startswith(\n            \"# Test Entry\"\n        ), \"plain_text doesn't start with expected content\"\n\n        # Verify other fields are parsed correctly\n        assert result[\"title\"] == \"Test Entry\"\n        assert result[\"date\"] == \"2025-01-15\"\n        assert \"test\" in result[\"tags\"]\n\n    finally:\n        # Clean up\n        if tmp_path.exists():\n            tmp_path.unlink()\n",
    "kind": "text"
  },
  {
    "path": "jsonify2ai\\modules\\note2json\\tests\\test_cli_integration.py",
    "ext": ".py",
    "size": 5433,
    "sig": "084e7466e8c572b9eb514905382baa7c750d8d19",
    "head": "import subprocess\nimport pytest\nimport os\nfrom pathlib import Path\nimport tempfile\nimport json\n\n\n@pytest.mark.integration\ndef test_cli_integration():\n    \"\"\"Test that the note2json CLI works end-to-end\"\"\"\n    # Get a demo file to test with\n    demo_file = Path(__file__).parent.parent / \"demo_entries\" / \"demo_entry.md\"\n\n    if not demo_file.exists():\n        pytest.skip(\"Demo file not found\")\n\n    # Create a temporary output file\n    with tempfile.NamedTemporaryFile(suffix=\".json\", delete=False) as tmp_file:\n        tmp_json = Path(tmp_file.name)\n\n    try:\n        # Run the CLI command with UTF-8 environment\n        env = os.environ.copy()\n        env[\"PYTHONIOENCODING\"] = \"utf-8\"\n\n        result = subprocess.run(\n            [\"note2json\", str(demo_file), \"-o\", str(tmp_json)],\n            capture_output=True,\n            text=True,\n            encoding=\"utf-8\",\n            env=env,\n        )\n\n        # Assert the command succeeded\n        assert (\n            result.returncode == 0\n        ), f\"CLI failed with return code {result.returncode}. stderr: {result.stderr}\"\n\n        # Assert the output file was created\n        assert tmp_json.exists(), f\"Output file {tmp_json} was not created\"\n\n        # Optional: verify the JSON is valid\n        import json\n\n        with open(tmp_json, \"r\") as f:\n            data = json.load(f)\n            assert \"title\" in data\n            assert \"timestamp\" in data\n\n    finally:\n        # Clean up the temporary file\n        if tmp_json.exists():\n            tmp_json.unlink()\n\n\n@pytest.mark.integration\ndef test_cli_stdout_flag():\n    \"\"\"Test that the --stdout flag works correctly\"\"\"\n    # Get a demo file to test with\n    demo_file = Path(__file__).parent.parent / \"demo_entries\" / \"demo_entry.md\"\n\n    if not demo_file.exists():\n        pytest.skip(\"Demo file not found\")\n\n    # Run the CLI command with --stdout flag\n    env = os.environ.copy()\n    env[\"PYTHONIOENCODING\"] = \"utf-8\"\n\n    result = subprocess.run(\n        [\"note2json\", str(demo_file), \"--stdout\"],\n        capture_output=True,\n        text=True,\n        encoding=\"utf-8\",\n        env=env,\n    )\n\n    # Assert the command succeeded with exit code 0\n    assert (\n        result.returncode == 0\n    ), f\"CLI failed with return code {result.returncode}. stderr: {result.stderr}\"\n\n    # Assert STDOUT contains valid JSON\n    assert result.stdout.strip(), \"STDOUT should not be empty\"\n\n    # Parse the JSON and verify it has the required fields\n    try:\n        data = json.loads(result.stdout.strip())\n        assert \"title\" in data, \"JSON should contain 'title' field\"\n        assert \"timestamp\" in data, \"JSON should contain 'timestamp' field\"\n    except json.JSONDecodeError as e:\n        pytest.fail(f\"STDOUT should contain valid JSON: {e}\")\n\n\n@pytest.mark.integration\ndef test_cli_missing_file_exit_code():\n    \"\"\"Test that missing files return exit code 2\"\"\"\n    # Run the CLI command with a clearly missing path\n    env = os.environ.copy()\n    env[\"PYTHONIOENCODING\"] = \"utf-8\"\n\n    result = subprocess.run(\n        [\"note2json\", \"definitely_missing_file.md\"],\n        capture_output=True,\n        text=True,\n        encoding=\"utf-8\",\n        env=env,\n    )\n\n    # Assert the command failed with exit code 2\n    assert (\n        result.returncode == 2\n    ), f\"CLI should return exit code 2 for missing files, got {result.returncode}\"\n\n    # Assert STDERR contains \"File not found\"\n    assert (\n        \"Error: File not found:\" in result.stderr\n    ), f\"STDERR should contain 'File not found', got: {result.stderr}\"\n\n\n@pytest.mark.integration\ndef test_cli_glob_expansion():\n    \"\"\"Test that glob patterns are expanded correctly\"\"\"\n    # Get demo files to test with\n    demo_dir = Path(__file__).parent.parent / \"demo_entries\"\n    demo_files = list(demo_dir.glob(\"*.md\"))\n\n    if len(demo_files) < 2:\n        pytest.skip(\"Need at least 2 demo files for glob test\")\n\n    # Create temporary directory for test\n    with tempfile.TemporaryDirectory() as temp_dir:\n        temp_path = Path(temp_dir)\n\n        # Copy demo files to temp directory\n        for i, demo_file in enumerate(demo_files[:2]):\n            temp_file = temp_path / f\"test_{i}.md\"\n            temp_file.write_text(demo_file.read_text(), encoding=\"utf-8\")\n\n        # Run the CLI command with glob pattern\n        env = os.environ.copy()\n        env[\"PYTHONIOENCODING\"] = \"utf-8\"\n\n        result = subprocess.run(\n            [\"note2json\", str(temp_path / \"*.md\"), \"--stdout\"],\n            capture_output=True,\n            text=True,\n            encoding=\"utf-8\",\n            env=env,\n        )\n\n        # Assert the command succeeded\n        assert (\n            result.returncode == 0\n        ), f\"CLI failed with return code {result.returncode}. stderr: {result.stderr}\"\n\n        # Assert STDOUT contains multiple JSON objects (one per line)\n        lines = result.stdout.strip().split(\"\\n\")\n        assert len(lines) >= 2, f\"Should have at least 2 JSON objects, got {len(lines)}\"\n\n        # Verify each line is valid JSON with title field\n        for line in lines:\n            if line.strip():  # Skip empty lines\n                try:\n                    data = json.loads(line)\n                    assert (\n                        \"title\" in data\n                    ), \"Each JSON object should contain 'title' field\"\n                except json.JSONDecodeError as e:\n                    pytest.fail(f\"Each line should be valid JSON: {e}\")\n",
    "kind": "text"
  },
  {
    "path": "jsonify2ai\\modules\\note2json\\tests\\test_generic_inputs.py",
    "ext": ".py",
    "size": 1744,
    "sig": "303e2f493f9337385b9e61552cf2ae8e64cf9e0a",
    "head": "import subprocess\nimport sys\nimport os\nimport json\nfrom pathlib import Path\n\n\ndef run_cli(args, stdin_input=None):\n    env = os.environ.copy()\n    env[\"PYTHONIOENCODING\"] = \"utf-8\"\n    return subprocess.run(\n        [sys.executable, \"-m\", \"note_to_json.cli\", *args],\n        input=stdin_input,\n        capture_output=True,\n        text=True,\n        encoding=\"utf-8\",\n        env=env,\n    )\n\n\ndef test_stdin_json_autodetect_normalizes():\n    payload = '{\"foo\":1,\"title\":\"x\"}'\n    result = run_cli(\n        [\"--stdin\", \"--input-format\", \"auto\", \"--stdout\"], stdin_input=payload\n    )\n    assert result.returncode == 0, f\"CLI failed: {result.stderr}\"\n    out = result.stdout.strip()\n    assert out, \"Expected JSON on stdout\"\n    data = json.loads(out)\n    for key in [\"title\", \"raw_text\", \"plain_text\"]:\n        assert key in data, f\"Missing key: {key}\"\n\n\ndef test_txt_file_parses_plain_text(tmp_path: Path):\n    note_path = tmp_path / \"note.txt\"\n    note_path.write_text(\"just a raw note\", encoding=\"utf-8\")\n    result = run_cli([str(note_path), \"--stdout\"])\n    assert result.returncode == 0, f\"CLI failed: {result.stderr}\"\n    data = json.loads(result.stdout.strip())\n    assert \"title\" in data\n    assert \"note\" in data[\"title\"].lower()\n\n\ndef test_stdin_wins_over_files(tmp_path: Path):\n    dummy = tmp_path / \"dummy.md\"\n    dummy.write_text(\"# ignore me\", encoding=\"utf-8\")\n    payload = '{\"from\":\"stdin\"}'\n    result = run_cli(\n        [\"--stdin\", \"--input-format\", \"auto\", \"--stdout\", str(dummy)],\n        stdin_input=payload,\n    )\n    assert result.returncode == 0\n    assert \"Warning: --stdin provided; ignoring file paths\" in result.stderr\n    data = json.loads(result.stdout.strip())\n    assert data[\"raw_text\"] == '{\"from\":\"stdin\"}'\n",
    "kind": "text"
  },
  {
    "path": "jsonify2ai\\modules\\note2json\\tests\\test_glob_and_multi_outputs.py",
    "ext": ".py",
    "size": 5779,
    "sig": "abc2a23b2a3ca234dcb71e1044eb608bc4b36f80",
    "head": "\"\"\"\nTests for glob pattern expansion and multi-file output handling.\n\"\"\"\n\nimport tempfile\nfrom pathlib import Path\nfrom note_to_json.cli import expand_glob_patterns\n\n\nclass TestGlobAndMultiOutputs:\n    \"\"\"Test glob pattern expansion and multi-file processing.\"\"\"\n\n    def test_simple_glob_pattern(self):\n        \"\"\"Test simple glob pattern expansion.\"\"\"\n        with tempfile.TemporaryDirectory() as temp_dir:\n            temp_path = Path(temp_dir)\n\n            # Create test files\n            (temp_path / \"file1.md\").write_text(\"# File 1\\n\\nContent 1\")\n            (temp_path / \"file2.md\").write_text(\"# File 2\\n\\nContent 2\")\n            (temp_path / \"file3.txt\").write_text(\"Text file content\")\n\n            # Test glob pattern\n            pattern = str(temp_path / \"*.md\")\n            expanded = expand_glob_patterns([pattern])\n\n            assert len(expanded) == 2\n            assert any(\"file1.md\" in f for f in expanded)\n            assert any(\"file2.md\" in f for f in expanded)\n            assert not any(\"file3.txt\" in f for f in expanded)\n\n    def test_recursive_glob_pattern(self):\n        \"\"\"Test recursive glob pattern with nested directories.\"\"\"\n        with tempfile.TemporaryDirectory() as temp_dir:\n            temp_path = Path(temp_dir)\n\n            # Create nested structure\n            (temp_path / \"dir1\").mkdir()\n            (temp_path / \"dir1\" / \"nested1.md\").write_text(\"# Nested 1\\n\\nContent\")\n            (temp_path / \"dir2\").mkdir()\n            (temp_path / \"dir2\" / \"nested2.md\").write_text(\"# Nested 2\\n\\nContent\")\n            (temp_path / \"root.md\").write_text(\"# Root\\n\\nContent\")\n\n            # Test recursive glob pattern\n            pattern = str(temp_path / \"**/*.md\")\n            expanded = expand_glob_patterns([pattern])\n\n            assert len(expanded) == 3\n            assert any(\"nested1.md\" in f for f in expanded)\n            assert any(\"nested2.md\" in f for f in expanded)\n            assert any(\"root.md\" in f for f in expanded)\n\n    def test_multiple_patterns(self):\n        \"\"\"Test multiple glob patterns.\"\"\"\n        with tempfile.TemporaryDirectory() as temp_dir:\n            temp_path = Path(temp_dir)\n\n            # Create test files\n            (temp_path / \"file1.md\").write_text(\"# File 1\\n\\nContent\")\n            (temp_path / \"file2.txt\").write_text(\"Text content\")\n            (temp_path / \"file3.json\").write_text('{\"title\": \"JSON\"}')\n\n            # Test multiple patterns\n            patterns = [str(temp_path / \"*.md\"), str(temp_path / \"*.txt\")]\n            expanded = expand_glob_patterns(patterns)\n\n            assert len(expanded) == 2\n            assert any(\"file1.md\" in f for f in expanded)\n            assert any(\"file2.txt\" in f for f in expanded)\n            assert not any(\"file3.json\" in f for f in expanded)\n\n    def test_literal_paths(self):\n        \"\"\"Test literal file paths (no glob characters).\"\"\"\n        with tempfile.TemporaryDirectory() as temp_dir:\n            temp_path = Path(temp_dir)\n\n            # Create test files\n            (temp_path / \"file1.md\").write_text(\"# File 1\\n\\nContent\")\n            (temp_path / \"file2.md\").write_text(\"# File 2\\n\\nContent\")\n\n            # Test literal paths\n            patterns = [str(temp_path / \"file1.md\"), str(temp_path / \"file2.md\")]\n            expanded = expand_glob_patterns(patterns)\n\n            assert len(expanded) == 2\n            assert any(\"file1.md\" in f for f in expanded)\n            assert any(\"file2.md\" in f for f in expanded)\n\n    def test_mixed_patterns_and_literals(self):\n        \"\"\"Test mixing glob patterns with literal paths.\"\"\"\n        with tempfile.TemporaryDirectory() as temp_dir:\n            temp_path = Path(temp_dir)\n\n            # Create test files\n            (temp_path / \"file1.md\").write_text(\"# File 1\\n\\nContent\")\n            (temp_path / \"file2.md\").write_text(\"# File 2\\n\\nContent\")\n            (temp_path / \"file3.md\").write_text(\"# File 3\\n\\nContent\")\n\n            # Test mixed patterns\n            patterns = [\n                str(temp_path / \"*.md\"),  # Glob pattern\n                str(temp_path / \"file3.md\"),  # Literal path\n            ]\n            expanded = expand_glob_patterns(patterns)\n\n            # Should get all 3 files (glob + literal, deduplicated)\n            assert len(expanded) == 3\n            assert any(\"file1.md\" in f for f in expanded)\n            assert any(\"file2.md\" in f for f in expanded)\n            assert any(\"file3.md\" in f for f in expanded)\n\n    def test_nonexistent_glob_pattern(self):\n        \"\"\"Test glob pattern that matches no files.\"\"\"\n        with tempfile.TemporaryDirectory() as temp_dir:\n            temp_path = Path(temp_dir)\n\n            # Test pattern that won't match anything\n            pattern = str(temp_path / \"nonexistent*.md\")\n            expanded = expand_glob_patterns([pattern])\n\n            # Should include the pattern itself when no matches found\n            assert len(expanded) == 1\n            assert \"nonexistent*.md\" in expanded[0]\n\n    def test_deduplication_and_sorting(self):\n        \"\"\"Test that results are deduplicated and sorted.\"\"\"\n        with tempfile.TemporaryDirectory() as temp_dir:\n            temp_path = Path(temp_dir)\n\n            # Create test files\n            (temp_path / \"b_file.md\").write_text(\"# B File\\n\\nContent\")\n            (temp_path / \"a_file.md\").write_text(\"# A File\\n\\nContent\")\n            (temp_path / \"c_file.md\").write_text(\"# C File\\n\\nContent\")\n\n            # Test glob pattern\n            pattern = str(temp_path / \"*.md\")\n            expanded = expand_glob_patterns([pattern])\n\n            # Should be sorted alphabetically\n            assert len(expanded) == 3\n            assert \"a_file.md\" in expanded[0]\n            assert \"b_file.md\" in expanded[1]\n            assert \"c_file.md\" in expanded[2]\n",
    "kind": "text"
  },
  {
    "path": "jsonify2ai\\modules\\note2json\\tests\\test_json_passthrough.py",
    "ext": ".py",
    "size": 7007,
    "sig": "7ab4ae109a0f36b5bf67386a2c6d545a3f976220",
    "head": "\"\"\"\nTests for JSON input format handling and validation.\n\"\"\"\n\nimport json\nimport tempfile\nfrom pathlib import Path\nimport pytest\nfrom note_to_json.parser import read_input\nfrom ._helpers import normalize_text, message_startswith\n\n\nclass TestJSONPassthrough:\n    \"\"\"Test JSON input format handling and validation.\"\"\"\n\n    def test_valid_json_passthrough(self):\n        \"\"\"Test that well-formed JSON files round-trip correctly.\"\"\"\n        json_data = {\n            \"title\": \"Test JSON\",\n            \"timestamp\": \"2023-01-01T00:00:00Z\",\n            \"raw_text\": \"JSON content\",\n            \"plain_text\": \"JSON content\",\n            \"tags\": [\"test\", \"json\"],\n            \"headers\": [\"Test JSON\"],\n            \"reflections\": [\"This is a test\"],\n        }\n\n        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".json\", delete=False) as f:\n            json.dump(json_data, f)\n            temp_path = Path(f.name)\n\n        try:\n            # Read and parse the JSON file\n            with open(temp_path, \"r\", encoding=\"utf-8\") as f:\n                content = f.read()\n\n            parsed = read_input(content, \"json\", filename_hint=\"test\")\n\n            # Should preserve the original data\n            assert parsed[\"title\"] == \"Test JSON\"\n            assert parsed[\"timestamp\"] == \"2023-01-01T00:00:00Z\"\n            assert parsed[\"raw_text\"] == \"JSON content\"\n            assert parsed[\"plain_text\"] == \"JSON content\"\n            assert parsed[\"tags\"] == [\"test\", \"json\"]\n            assert parsed[\"headers\"] == [\"Test JSON\"]\n            assert parsed[\"reflections\"] == [\"This is a test\"]\n        finally:\n            temp_path.unlink()\n\n    def test_json_with_extra_fields(self):\n        \"\"\"Test JSON with extra fields gets normalized.\"\"\"\n        json_data = {\n            \"title\": \"Extra Fields Test\",\n            \"extra_field\": \"This should be ignored\",\n            \"another_extra\": 123,\n            \"nested\": {\"key\": \"value\"},\n        }\n\n        content = json.dumps(json_data)\n        parsed = read_input(content, \"json\", filename_hint=\"test\")\n\n        # Should only include schema fields\n        assert parsed[\"title\"] == \"Extra Fields Test\"\n        assert \"extra_field\" not in parsed\n        assert \"another_extra\" not in parsed\n        assert \"nested\" not in parsed\n\n        # Should have required fields\n        assert \"timestamp\" in parsed\n        assert \"raw_text\" in parsed\n        assert \"plain_text\" in parsed\n\n    def test_json_with_missing_required_fields(self):\n        \"\"\"Test JSON with missing required fields gets normalized.\"\"\"\n        json_data = {\n            \"title\": \"Missing Fields Test\"\n            # Missing timestamp, raw_text, plain_text\n        }\n\n        content = json.dumps(json_data)\n        parsed = read_input(content, \"json\", filename_hint=\"test\")\n\n        # Should add missing required fields\n        assert parsed[\"title\"] == \"Missing Fields Test\"\n        assert \"timestamp\" in parsed\n        assert \"raw_text\" in parsed\n        assert \"plain_text\" in parsed\n\n        # raw_text and plain_text should contain the minified JSON\n        assert json_data[\"title\"] in parsed[\"raw_text\"]\n        assert json_data[\"title\"] in parsed[\"plain_text\"]\n\n    def test_json_array_input(self):\n        \"\"\"Test JSON array input gets normalized.\"\"\"\n        json_data = [\"item1\", \"item2\", \"item3\"]\n\n        content = json.dumps(json_data)\n        parsed = read_input(content, \"json\", filename_hint=\"test\")\n\n        # Should create a normalized structure\n        assert parsed[\"title\"] == \"test\"  # filename_hint\n        assert \"timestamp\" in parsed\n        assert \"raw_text\" in parsed\n        assert \"plain_text\" in parsed\n\n        # raw_text should contain the minified JSON\n        assert \"item1\" in parsed[\"raw_text\"]\n        assert \"item3\" in parsed[\"raw_text\"]\n        # plain_text should contain the normalized content\n        assert \"item1\" in parsed[\"plain_text\"]\n        assert \"item3\" in parsed[\"plain_text\"]\n\n    def test_json_primitive_input(self):\n        \"\"\"Test JSON primitive input gets normalized.\"\"\"\n        test_cases = [\n            (\"string\", \"string\"),\n            (123, \"123\"),\n            (True, \"true\"),\n            (None, \"null\"),\n        ]\n\n        for input_value, expected_text in test_cases:\n            content = json.dumps(input_value)\n            parsed = read_input(content, \"json\", filename_hint=\"test\")\n\n            assert parsed[\"title\"] == \"test\"\n            assert \"timestamp\" in parsed\n            assert \"raw_text\" in parsed\n            assert \"plain_text\" in parsed\n            # Use normalize_text for cross-platform stability\n            assert normalize_text(expected_text) in normalize_text(parsed[\"plain_text\"])\n\n    def test_invalid_json_fails_gracefully(self):\n        \"\"\"Test that malformed JSON fails with clear error message.\"\"\"\n        invalid_json_cases = [\n            \"{ invalid json\",\n            '{\"title\": \"test\",}',\n            '{\"title\": \"test\" \"missing\": \"comma\"}',\n            '{\"title\": \"test\", \"unclosed\": \"quote}',\n            '{\"title\": \"test\", \"null_bytes\": \"\\u0000\"}',\n        ]\n\n        for invalid_json in invalid_json_cases:\n            with pytest.raises(ValueError) as exc_info:\n                read_input(invalid_json, \"json\", filename_hint=\"test\")\n\n            error_msg = str(exc_info.value)\n            # Use message_startswith for error message checking\n            assert message_startswith(error_msg, \"Invalid JSON input\")\n            assert \"use `--input-format md|txt`\" in error_msg\n\n    def test_json_with_unicode_content(self):\n        \"\"\"Test JSON with unicode content handles correctly.\"\"\"\n        json_data = {\n            \"title\": \"Unicode Test \ud83c\udf89\",\n            \"content\": \"Content with \u00e9mojis and \ud83d\ude80 symbols\",\n        }\n\n        content = json.dumps(json_data, ensure_ascii=False)\n        parsed = read_input(content, \"json\", filename_hint=\"test\")\n\n        assert parsed[\"title\"] == \"Unicode Test \ud83c\udf89\"\n        assert \"\u00e9mojis\" in parsed[\"raw_text\"]\n        assert \"\ud83d\ude80\" in parsed[\"raw_text\"]\n        assert \"\ud83c\udf89\" in parsed[\"raw_text\"]\n        # plain_text should also contain the normalized content\n        assert \"\u00e9mojis\" in parsed[\"plain_text\"]\n        assert \"\ud83d\ude80\" in parsed[\"plain_text\"]\n        assert \"\ud83c\udf89\" in parsed[\"plain_text\"]\n\n    def test_json_tags_normalization(self):\n        \"\"\"Test that JSON tags get properly normalized.\"\"\"\n        test_cases = [\n            # Valid tags\n            ([\"tag1\", \"tag2\"], [\"tag1\", \"tag2\"]),\n            ([\"#tag1\", \"#tag2\"], [\"tag1\", \"tag2\"]),  # Hash prefix stripped\n            ([123, True, \"string\"], [\"123\", \"true\", \"string\"]),  # Converted to strings\n            ([], []),  # Empty list\n            (None, []),  # None becomes empty list\n        ]\n\n        for input_tags, expected_tags in test_cases:\n            json_data = {\"title\": \"Tags Test\", \"tags\": input_tags}\n            content = json.dumps(json_data)\n            parsed = read_input(content, \"json\", filename_hint=\"test\")\n\n            assert parsed[\"tags\"] == expected_tags\n",
    "kind": "text"
  },
  {
    "path": "jsonify2ai\\modules\\note2json\\tests\\test_parser.py",
    "ext": ".py",
    "size": 1124,
    "sig": "cadae6c7627b252a65d418e6664f91663de2f3b1",
    "head": "import json\nfrom pathlib import Path\nfrom note_to_json.parser import parse_file\n\nFIXTURE = Path(__file__).parent / \"fixtures\" / \"sample.md\"\n\n\ndef test_parse_returns_valid_json():\n    data = parse_file(FIXTURE)\n    assert isinstance(data, dict)\n    assert \"title\" in data and isinstance(data[\"title\"], str)\n    json.dumps(data)  # ensures serialisable\n\n\ndef test_parse_extracts_required_fields():\n    data = parse_file(FIXTURE)\n    required_fields = [\"title\", \"timestamp\", \"raw_text\", \"plain_text\"]\n    for field in required_fields:\n        assert field in data, f\"Missing required field: {field}\"\n\n\ndef test_parse_extracts_metadata():\n    data = parse_file(FIXTURE)\n    assert data[\"title\"] == \"Test Entry\"\n    assert data[\"date\"] == \"2025-01-15\"\n    assert data[\"tone\"] == \"Neutral\"\n    assert \"test\" in data[\"tags\"]\n    assert \"demo\" in data[\"tags\"]\n    assert \"pytest\" in data[\"tags\"]\n\n\ndef test_parse_extracts_summary_and_reflections():\n    data = parse_file(FIXTURE)\n    assert \"summary\" in data\n    assert \"reflections\" in data\n    assert isinstance(data[\"reflections\"], list)\n    assert len(data[\"reflections\"]) == 3\n",
    "kind": "text"
  },
  {
    "path": "jsonify2ai\\modules\\note2json\\tests\\test_resilience.py",
    "ext": ".py",
    "size": 12901,
    "sig": "a503ce976067b89c8eb339855c65e6b854ed7f38",
    "head": "\"\"\"\nTests for CLI resilience improvements.\n\"\"\"\n\nimport tempfile\nfrom pathlib import Path\nimport pytest\nfrom note_to_json.parser import read_input, ParsingError, _fix_common_validation_issues\nfrom note_to_json.utils import read_text_safely\nfrom ._helpers import message_startswith\n\n\nclass TestParserResilience:\n    \"\"\"Test parser resilience improvements.\"\"\"\n\n    def test_empty_input_handling(self):\n        \"\"\"Test that empty inputs are handled gracefully.\"\"\"\n        with pytest.raises(ParsingError) as exc_info:\n            read_input(\"\", \"auto\")\n\n        assert exc_info.value.error_type == \"empty_input\"\n        assert \"Empty or whitespace-only input\" in str(exc_info.value)\n\n    def test_whitespace_only_input(self):\n        \"\"\"Test that whitespace-only inputs are handled gracefully.\"\"\"\n        with pytest.raises(ParsingError) as exc_info:\n            read_input(\"   \\n\\t   \", \"auto\")\n\n        assert exc_info.value.error_type == \"empty_input\"\n\n    def test_malformed_json_handling(self):\n        \"\"\"Test that malformed JSON is handled gracefully.\"\"\"\n        with pytest.raises(ParsingError) as exc_info:\n            read_input(\"{invalid json\", \"json\")\n\n        assert exc_info.value.error_type == \"json_decode_error\"\n        assert \"Invalid JSON input\" in str(exc_info.value)\n\n    def test_validation_error_fixing(self):\n        \"\"\"Test that common validation issues are automatically fixed.\"\"\"\n        # Create data with validation issues\n        problematic_data = {\n            \"title\": None,  # Missing required field\n            \"timestamp\": 12345,  # Wrong type\n            \"raw_text\": [],  # Wrong type\n            \"plain_text\": None,  # Missing required field\n            \"tags\": \"not a list\",  # Wrong type\n        }\n\n        fixed_data = _fix_common_validation_issues(problematic_data)\n\n        # Check that issues were fixed\n        assert fixed_data[\"title\"] == \"untitled\"\n        assert isinstance(fixed_data[\"timestamp\"], str)\n        assert isinstance(fixed_data[\"raw_text\"], str)\n        assert isinstance(fixed_data[\"plain_text\"], str)\n        assert isinstance(fixed_data[\"tags\"], list)\n\n    def test_line_ending_handling(self):\n        \"\"\"Test that different line endings are handled correctly.\"\"\"\n        content = \"# Title\\r\\nContent with\\r\\nWindows line endings\"\n        parsed = read_input(content, \"auto\")\n\n        assert parsed[\"title\"] == \"Title\"\n        assert \"Content with\" in parsed[\"raw_text\"]\n\n    def test_null_byte_handling(self):\n        \"\"\"Test that null bytes are handled gracefully.\"\"\"\n        content = \"# Title\\nContent with \\x00 null bytes\"\n        parsed = read_input(content, \"auto\")\n\n        assert parsed[\"title\"] == \"Title\"\n        assert \"\\x00\" not in parsed[\"raw_text\"]\n\n    def test_long_content_handling(self):\n        \"\"\"Test that very long content is handled gracefully.\"\"\"\n        # Create content that's longer than the default max length\n        long_content = \"# Title\\n\" + \"x\" * 15000\n\n        # Long content should be truncated and processed, not cause an error\n        parsed = read_input(long_content, \"auto\")\n\n        # Should be processed successfully with truncation\n        assert \"title\" in parsed\n        assert \"timestamp\" in parsed\n        assert \"raw_text\" in parsed\n        assert \"plain_text\" in parsed\n\n        # Check that content was truncated\n        assert len(parsed[\"raw_text\"]) <= 10000 + len(\"... [truncated]\")\n        assert \"[truncated]\" in parsed[\"raw_text\"]\n\n\nclass TestEncodingResilience:\n    \"\"\"Test encoding utility resilience improvements.\"\"\"\n\n    def test_encoding_detection_fallback(self):\n        \"\"\"Test that encoding detection falls back gracefully.\"\"\"\n        # Create content with mixed encodings that can be encoded in latin-1\n        content = \"Test content with special chars: \u00e9mojis\"\n\n        with tempfile.NamedTemporaryFile(mode=\"wb\", suffix=\".md\", delete=False) as f:\n            # Write with a non-standard encoding\n            f.write(content.encode(\"latin-1\"))\n            temp_path = Path(f.name)\n\n        try:\n            # Should still be able to read it\n            text = read_text_safely(temp_path)\n            assert \"Test content\" in text\n        finally:\n            temp_path.unlink()\n\n    def test_text_validation(self):\n        \"\"\"Test text content validation.\"\"\"\n        # Valid text should work\n        valid_text = \"This is valid text content\"\n        # Since we removed the validation function, we'll just test that read_text_safely works\n        assert \"valid text content\" in valid_text\n\n    def test_text_sanitization(self):\n        \"\"\"Test text content sanitization.\"\"\"\n        # Since we removed the sanitization function, we'll just test basic functionality\n        problematic_text = (\n            \"Text with \\x00\\x01\\x02 problematic chars\\r\\nand line endings\"\n        )\n        # The text should still contain the problematic characters since we're not sanitizing\n        assert \"\\x00\" in problematic_text\n        assert \"\\x01\" in problematic_text\n        assert \"\\x02\" in problematic_text\n\n\nclass TestCLIResilience:\n    \"\"\"Test CLI resilience improvements.\"\"\"\n\n    def test_continue_on_error_flag(self):\n        \"\"\"Test that --continue-on-error allows processing to continue.\"\"\"\n        # This would need integration testing with actual CLI calls\n        # For now, we'll test the logic indirectly\n        from note_to_json.cli import determine_exit_code, ProcessingResult\n        from pathlib import Path\n\n        # Test with some failures but continue_on_error=True\n        results = [\n            ProcessingResult(Path(\"file1.md\"), True, data={\"title\": \"Test\"}),\n            ProcessingResult(\n                Path(\"file2.md\"),\n                False,\n                error=\"Parse failed\",\n                error_type=\"parsing_error\",\n            ),\n        ]\n        missing_files = []\n\n        exit_code = determine_exit_code(results, missing_files)\n        assert exit_code == 3  # Should indicate parsing errors\n\n    def test_processing_result_class(self):\n        \"\"\"Test the ProcessingResult class.\"\"\"\n        from note_to_json.cli import ProcessingResult\n        from pathlib import Path\n\n        # Test successful result\n        success_result = ProcessingResult(Path(\"test.md\"), True, data={\"title\": \"Test\"})\n        assert success_result.success\n        assert success_result.data[\"title\"] == \"Test\"\n        assert \"\u2705\" in str(success_result)\n\n        # Test failed result\n        failed_result = ProcessingResult(\n            Path(\"test.md\"), False, error=\"Parse failed\", error_type=\"parsing_error\"\n        )\n        assert not failed_result.success\n        assert failed_result.error == \"Parse failed\"\n        assert failed_result.error_type == \"parsing_error\"\n        assert \"\u274c\" in str(failed_result)\n\n\nclass TestErrorHandling:\n    \"\"\"Test comprehensive error handling.\"\"\"\n\n    def test_parsing_error_context(self):\n        \"\"\"Test that parsing errors include useful context.\"\"\"\n        try:\n            read_input(\"{invalid json\", \"json\")\n        except ParsingError as e:\n            assert e.error_type == \"json_decode_error\"\n            # Use message_startswith for error message checking\n            assert message_startswith(str(e), \"Invalid JSON input\")\n\n    def test_encoding_error_context(self):\n        \"\"\"Test that encoding errors include useful context.\"\"\"\n        # Create a file with bytes that will pass decoding but fail validation\n        # Use bytes with a high ratio of null bytes to trigger the null byte validation\n        # This will test the validation logic in our encoding function\n        problematic_bytes = (\n            b\"Valid text\" + b\"\\x00\" * 20\n        )  # 20 null bytes out of 30 total = 66% null ratio\n\n        with tempfile.NamedTemporaryFile(mode=\"wb\", suffix=\".md\", delete=False) as f:\n            f.write(problematic_bytes)\n            temp_path = Path(f.name)\n\n        try:\n            # This should fail validation with a clear error message\n            with pytest.raises(ValueError) as exc_info:\n          ",
    "kind": "text"
  },
  {
    "path": "jsonify2ai\\modules\\note2json\\tests\\test_stdin_modes.py",
    "ext": ".py",
    "size": 5371,
    "sig": "81fc1e5d732de3f2cb8b317c578bef1bdb5a36a3",
    "head": "\"\"\"\nTests for stdin input modes.\n\"\"\"\n\nimport io\nimport json\nimport pytest\nfrom note_to_json.parser import read_input\nfrom note_to_json.utils import read_stdin_safely\nfrom ._helpers import normalize_text, message_startswith\n\n\nclass TestStdinModes:\n    \"\"\"Test stdin input handling for various formats and encodings.\"\"\"\n\n    def test_stdin_markdown_utf8(self):\n        \"\"\"Test markdown input from stdin with UTF-8 encoding.\"\"\"\n        content = \"# Stdin Test\\n\\nContent from stdin with \u00e9mojis \ud83c\udf89\"\n        stdin_buffer = io.BytesIO(content.encode(\"utf-8\"))\n\n        # Test encoding utility\n        text = read_stdin_safely(stdin_buffer)\n        # Use normalize_text for cross-platform stability\n        assert normalize_text(text) == normalize_text(content)\n\n        # Test full parsing\n        parsed = read_input(text, \"auto\", filename_hint=\"stdin\")\n        assert parsed[\"title\"] == \"Stdin Test\"\n        assert \"\u00e9mojis\" in parsed[\"raw_text\"]\n        assert \"\ud83c\udf89\" in parsed[\"raw_text\"]\n        # plain_text should also contain the normalized content\n        assert \"\u00e9mojis\" in parsed[\"plain_text\"]\n        assert \"\ud83c\udf89\" in parsed[\"plain_text\"]\n\n    def test_stdin_markdown_utf8_bom(self):\n        \"\"\"Test markdown input from stdin with UTF-8 BOM.\"\"\"\n        content = \"# BOM Stdin Test\\n\\nContent with BOM\"\n        stdin_buffer = io.BytesIO(b\"\\xef\\xbb\\xbf\" + content.encode(\"utf-8\"))\n\n        text = read_stdin_safely(stdin_buffer)\n        # Use normalize_text for cross-platform stability\n        assert normalize_text(text) == normalize_text(content)  # BOM should be stripped\n\n        parsed = read_input(text, \"auto\", filename_hint=\"test\")\n        assert parsed[\"title\"] == \"BOM Stdin Test\"\n\n    def test_stdin_markdown_utf16_le(self):\n        \"\"\"Test markdown input from stdin with UTF-16 LE.\"\"\"\n        content = \"# UTF-16 Stdin Test\\n\\nContent in UTF-16 LE\"\n        stdin_buffer = io.BytesIO(content.encode(\"utf-16-le\"))\n\n        text = read_stdin_safely(stdin_buffer)\n        # Use normalize_text for cross-platform stability\n        assert normalize_text(text) == normalize_text(content)\n\n        parsed = read_input(text, \"auto\", filename_hint=\"stdin\")\n        assert parsed[\"title\"] == \"UTF-16 Stdin Test\"\n\n    def test_stdin_text_format(self):\n        \"\"\"Test text input from stdin with explicit format.\"\"\"\n        content = \"Plain text content\\n\\nNo markdown formatting\"\n        stdin_buffer = io.BytesIO(content.encode(\"utf-8\"))\n\n        text = read_stdin_safely(stdin_buffer)\n        parsed = read_input(text, \"txt\", filename_hint=\"stdin\")\n\n        assert parsed[\"title\"] == \"Plain text content\"\n        # Use normalize_text for cross-platform stability\n        assert normalize_text(parsed[\"raw_text\"]) == normalize_text(content)\n        assert normalize_text(parsed[\"plain_text\"]) == normalize_text(\n            content.replace(\"\\n\", \" \")\n        )\n\n    def test_stdin_json_format(self):\n        \"\"\"Test JSON input from stdin.\"\"\"\n        json_data = {\"title\": \"JSON Input\", \"content\": \"JSON content\"}\n        content = json.dumps(json_data)\n        stdin_buffer = io.BytesIO(content.encode(\"utf-8\"))\n\n        text = read_stdin_safely(stdin_buffer)\n        parsed = read_input(text, \"json\", filename_hint=\"stdin\")\n\n        assert parsed[\"title\"] == \"JSON Input\"\n        assert \"JSON content\" in parsed[\"raw_text\"]\n        assert \"JSON content\" in parsed[\"plain_text\"]\n\n    def test_stdin_json_utf16(self):\n        \"\"\"Test JSON input from stdin with UTF-16 encoding.\"\"\"\n        json_data = {\"title\": \"UTF-16 JSON\", \"content\": \"Content in UTF-16\"}\n        content = json.dumps(json_data)\n        stdin_buffer = io.BytesIO(content.encode(\"utf-16-le\"))\n\n        text = read_stdin_safely(stdin_buffer)\n        parsed = read_input(text, \"json\", filename_hint=\"stdin\")\n\n        assert parsed[\"title\"] == \"UTF-16 JSON\"\n        assert \"Content in UTF-16\" in parsed[\"raw_text\"]\n        assert \"Content in UTF-16\" in parsed[\"plain_text\"]\n\n    def test_stdin_invalid_encoding_fails_gracefully(self):\n        \"\"\"Test that invalid stdin encoding fails with clear error message.\"\"\"\n        # Create stdin with bytes that will pass decoding but fail validation\n        # Use bytes with a high ratio of null bytes to trigger validation failure\n        invalid_bytes = (\n            b\"Valid text\" + b\"\\x00\" * 20\n        )  # 20 null bytes out of 30 total = 66% null ratio\n        stdin_buffer = io.BytesIO(invalid_bytes)\n\n        with pytest.raises(ValueError) as exc_info:\n            read_stdin_safely(stdin_buffer)\n\n        error_msg = str(exc_info.value)\n        # Use message_startswith for error message checking\n        assert message_startswith(error_msg, \"Decoding error\")\n        assert \"try saving as UTF-8\" in error_msg\n\n    def test_stdin_invalid_json_fails_gracefully(self):\n        \"\"\"Test that invalid JSON from stdin fails with clear error message.\"\"\"\n        invalid_json = \"{ invalid json content\"\n        stdin_buffer = io.BytesIO(invalid_json.encode(\"utf-8\"))\n\n        text = read_stdin_safely(stdin_buffer)\n\n        with pytest.raises(ValueError) as exc_info:\n            read_input(text, \"json\", filename_hint=\"stdin\")\n\n        error_msg = str(exc_info.value)\n        # Use message_startswith for error message checking\n        assert message_startswith(error_msg, \"Invalid JSON input\")\n        assert \"use `--input-format md|txt`\" in error_msg\n",
    "kind": "text"
  },
  {
    "path": "jsonify2ai\\modules\\note2json\\tests\\test_utf8_and_utf16_files.py",
    "ext": ".py",
    "size": 5251,
    "sig": "fa3cd27f39ebb3a64518c47424de97c73b7b871c",
    "head": "\"\"\"\nTests for UTF-8 and UTF-16 file encoding handling.\n\"\"\"\n\nimport tempfile\nfrom pathlib import Path\nimport pytest\nfrom note_to_json.parser import read_input\nfrom note_to_json.utils import read_text_safely\nfrom ._helpers import normalize_text\n\n\nclass TestUTF8AndUTF16Files:\n    \"\"\"Test encoding detection and handling for various UTF formats.\"\"\"\n\n    def test_utf8_file(self):\n        \"\"\"Test UTF-8 file reading.\"\"\"\n        content = \"# Test Title\\n\\nThis is UTF-8 content with \u00e9mojis \ud83c\udf89\"\n\n        with tempfile.NamedTemporaryFile(mode=\"wb\", suffix=\".md\", delete=False) as f:\n            f.write(content.encode(\"utf-8\"))\n            temp_path = Path(f.name)\n\n        try:\n            # Test encoding utility\n            text = read_text_safely(temp_path)\n            # Use normalize_text for cross-platform stability\n            assert normalize_text(text) == normalize_text(content)\n\n            # Test full parsing\n            parsed = read_input(text, \"auto\", filename_hint=\"test\")\n            assert parsed[\"title\"] == \"Test Title\"\n            assert \"\u00e9mojis\" in parsed[\"raw_text\"]\n            assert \"\ud83c\udf89\" in parsed[\"raw_text\"]\n            assert \"\\u0000\" not in parsed[\"raw_text\"]  # No null bytes\n            # plain_text should also contain the normalized content\n            assert \"\u00e9mojis\" in parsed[\"plain_text\"]\n            assert \"\ud83c\udf89\" in parsed[\"plain_text\"]\n        finally:\n            temp_path.unlink()\n\n    def test_utf8_bom_file(self):\n        \"\"\"Test UTF-8 BOM file reading.\"\"\"\n        content = \"# BOM Test\\n\\nContent with BOM\"\n\n        with tempfile.NamedTemporaryFile(mode=\"wb\", suffix=\".md\", delete=False) as f:\n            f.write(b\"\\xef\\xbb\\xbf\")  # UTF-8 BOM\n            f.write(content.encode(\"utf-8\"))\n            temp_path = Path(f.name)\n\n        try:\n            text = read_text_safely(temp_path)\n            # Use normalize_text for cross-platform stability\n            assert normalize_text(text) == normalize_text(\n                content\n            )  # BOM should be stripped\n\n            parsed = read_input(text, \"auto\", filename_hint=\"test\")\n            assert parsed[\"title\"] == \"BOM Test\"\n        finally:\n            temp_path.unlink()\n\n    def test_utf16_le_file(self):\n        \"\"\"Test UTF-16 LE file reading.\"\"\"\n        content = \"# UTF-16 Test\\n\\nContent in UTF-16 LE\"\n\n        with tempfile.NamedTemporaryFile(mode=\"wb\", suffix=\".md\", delete=False) as f:\n            f.write(content.encode(\"utf-16-le\"))\n            temp_path = Path(f.name)\n\n        try:\n            text = read_text_safely(temp_path)\n            # Use normalize_text for cross-platform stability\n            assert normalize_text(text) == normalize_text(content)\n\n            parsed = read_input(text, \"auto\", filename_hint=\"test\")\n            assert parsed[\"title\"] == \"UTF-16 Test\"\n        finally:\n            temp_path.unlink()\n\n    def test_utf16_be_file(self):\n        \"\"\"Test UTF-16 BE file reading.\"\"\"\n        content = \"# UTF-16 BE Test\\n\\nContent in UTF-16 BE\"\n\n        with tempfile.NamedTemporaryFile(mode=\"wb\", suffix=\".md\", delete=False) as f:\n            f.write(content.encode(\"utf-16-be\"))\n            temp_path = Path(f.name)\n\n        try:\n            text = read_text_safely(temp_path)\n            # Use normalize_text for cross-platform stability\n            assert normalize_text(text) == normalize_text(content)\n\n            parsed = read_input(text, \"auto\", filename_hint=\"test\")\n            assert parsed[\"title\"] == \"UTF-16 BE Test\"\n        finally:\n            temp_path.unlink()\n\n    def test_utf16_file(self):\n        \"\"\"Test UTF-16 file reading (system default endianness).\"\"\"\n        content = \"# UTF-16 Default Test\\n\\nContent in UTF-16\"\n\n        with tempfile.NamedTemporaryFile(mode=\"wb\", suffix=\".md\", delete=False) as f:\n            f.write(content.encode(\"utf-16\"))\n            temp_path = Path(f.name)\n\n        try:\n            text = read_text_safely(temp_path)\n            # Use normalize_text for cross-platform stability\n            assert normalize_text(text) == normalize_text(content)\n\n            parsed = read_input(text, \"auto\", filename_hint=\"test\")\n            assert parsed[\"title\"] == \"UTF-16 Default Test\"\n        finally:\n            temp_path.unlink()\n\n    def test_invalid_encoding_fails_gracefully(self):\n        \"\"\"Test that invalid encodings fail with clear error message.\"\"\"\n        # Create a file with bytes that will pass decoding but fail validation\n        # Use bytes with a high ratio of null bytes to trigger validation failure\n        invalid_bytes = (\n            b\"Valid text\" + b\"\\x00\" * 20\n        )  # 20 null bytes out of 30 total = 66% null ratio\n\n        with tempfile.NamedTemporaryFile(mode=\"wb\", suffix=\".md\", delete=False) as f:\n            f.write(invalid_bytes)\n            temp_path = Path(f.name)\n\n        try:\n            with pytest.raises(ValueError) as exc_info:\n                read_text_safely(temp_path)\n\n            error_msg = str(exc_info.value)\n            # Use message_startswith for error message checking\n            from ._helpers import message_startswith\n\n            assert message_startswith(error_msg, \"Decoding error\")\n            assert \"try saving as UTF-8\" in error_msg\n        finally:\n            temp_path.unlink()\n",
    "kind": "text"
  },
  {
    "path": "jsonify2ai\\modules\\note2json\\tests\\_helpers.py",
    "ext": ".py",
    "size": 271,
    "sig": "4ae755a5a174e94564cf40998636c81a291c36e7",
    "head": "def normalize_text(s: str) -> str:\n    # normalize newlines and trim trailing whitespace typical of shell-created files\n    return s.replace(\"\\r\\n\", \"\\n\").rstrip()\n\n\ndef message_startswith(msg: str, prefix: str) -> bool:\n    return normalize_text(msg).startswith(prefix)\n",
    "kind": "text"
  },
  {
    "path": "jsonify2ai\\modules\\note2json\\tests\\__init__.py",
    "ext": ".py",
    "size": 30,
    "sig": "ab5b7f05a2f73ca8df3ef58919a2683daf679799",
    "head": "# Tests package for note2json\n",
    "kind": "text"
  },
  {
    "path": "jsonify2ai\\modules\\note2json\\tests\\fixtures\\sample.md",
    "ext": ".md",
    "size": 294,
    "sig": "b3299042c29978bc0348e0e26eb8838353e636b7",
    "head": "# Test Entry\n**Date:** 2025-01-15\n**Tags:** #test #demo #pytest\n**Tone:** Neutral\n\n**Summary:**\nThis is a test markdown entry for automated testing.\n\n**Core Reflections:**\n- Testing is important for code quality\n- Automated tests catch bugs early\n- CI/CD pipelines improve development workflow\n",
    "kind": "text"
  },
  {
    "path": "scripts\\smoke_text.ps1",
    "ext": ".ps1",
    "size": 492,
    "sig": "c15b69a156ecf640f02396340651f03b5a79a6de",
    "head": "$ErrorActionPreference = \"Stop\"\nWrite-Host 'Running API upload smoke test...'\n$env:PORT_API = $env:PORT_API -or 8082\ncurl.exe -F 'file=@.\\README.md' \"http://localhost:$env:PORT_API/upload\"\nWrite-Host ' running worker process/text smoke test...'\n$env:PORT_WORKER = $env:PORT_WORKER -or 8090\ncurl.exe -X POST \"http://localhost:$env:PORT_WORKER/process/text\" ` \n\u00a0 -H \"Content-Type: application/json\" ` \n\u00a0 -d \"{\\\"document_id\\\":\\\"00000000-0000-0000-0000-000000000000\\\",\\\"text\\\":\\\"hello world\\\"}\"",
    "kind": "text"
  },
  {
    "path": "web\\Dockerfile",
    "ext": "",
    "size": 241,
    "sig": "037251835f4a896561e37111f4609eb255ece810",
    "head": "FROM node:20-alpine\n\nWORKDIR /app\n\n# Copy package files\nCOPY package*.json ./\n\n# Install dependencies\nRUN npm ci\n\n# Copy source code\nCOPY . .\n\n# Expose port\nEXPOSE 5173\n\n# Start development server\nCMD [\"npm\", \"run\", \"dev\"]\n",
    "kind": "text"
  },
  {
    "path": "web\\index.html",
    "ext": ".html",
    "size": 386,
    "sig": "ec101f967bac154c4c7068436467b622d315ae05",
    "head": "<!doctype html>\n<html lang=\"en\">\n  <head>\n    <meta charset=\"UTF-8\" />\n    <link rel=\"icon\" type=\"image/svg+xml\" href=\"/vite.svg\" />\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n    <title>jsonify2ai Memory System</title>\n  </head>\n  <body>\n    <div id=\"root\"></div>\n    <script type=\"module\" src=\"/src/main.tsx\"></script>\n  </body>\n</html>\n",
    "kind": "text"
  },
  {
    "path": "web\\package.json",
    "ext": ".json",
    "size": 779,
    "sig": "2ab0e6d9119f84962a9d2cc228977adad24f58a5",
    "head": "{\n  \"name\": \"jsonify2ai-web\",\n  \"private\": true,\n  \"version\": \"0.0.0\",\n  \"type\": \"module\",\n  \"scripts\": {\n    \"dev\": \"vite\",\n    \"build\": \"tsc && vite build\",\n    \"lint\": \"eslint . --ext ts,tsx --report-unused-disable-directives --max-warnings 0\",\n    \"preview\": \"vite preview\"\n  },\n  \"dependencies\": {\n    \"react\": \"^18.2.0\",\n    \"react-dom\": \"^18.2.0\"\n  },\n  \"devDependencies\": {\n    \"@types/react\": \"^18.2.43\",\n    \"@types/react-dom\": \"^18.2.17\",\n    \"@typescript-eslint/eslint-plugin\": \"^6.14.0\",\n    \"@typescript-eslint/parser\": \"^6.14.0\",\n    \"@vitejs/plugin-react\": \"^4.2.1\",\n    \"eslint\": \"^8.55.0\",\n    \"eslint-plugin-react-hooks\": \"^4.6.0\",\n    \"eslint-plugin-react-refresh\": \"^0.4.5\",\n    \"typescript\": \"^5.2.2\",\n    \"vite\": \"^5.0.8\"\n  }\n}\n",
    "kind": "text"
  },
  {
    "path": "web\\README.md",
    "ext": ".md",
    "size": 186,
    "sig": "506b9e6fd9b4f0fc13cf4f2874e70f4bf6e2a2b6",
    "head": "# Web Service\n\nVite + React + TypeScript frontend for the jsonify2ai memory system.\n\n## Development\n\n```bash\nnpm run dev\n```\n\n## Docker\n\n```bash\ndocker compose up web\n```\n",
    "kind": "text"
  },
  {
    "path": "web\\tsconfig.json",
    "ext": ".json",
    "size": 630,
    "sig": "3035cfa20693cf7f70b521aa411b2466b1f53abe",
    "head": "{\n  \"compilerOptions\": {\n    \"target\": \"ES2020\",\n    \"useDefineForClassFields\": true,\n    \"lib\": [\"ES2020\", \"DOM\", \"DOM.Iterable\"],\n    \"module\": \"ESNext\",\n    \"skipLibCheck\": true,\n\n    /* Bundler mode */\n    \"moduleResolution\": \"bundler\",\n    \"allowImportingTsExtensions\": true,\n    \"resolveJsonModule\": true,\n    \"isolatedModules\": true,\n    \"noEmit\": true,\n    \"jsx\": \"react-jsx\",\n\n    /* Linting */\n    \"strict\": true,\n    \"noUnusedLocals\": true,\n    \"noUnusedParameters\": true,\n    \"noFallthroughCasesInSwitch\": true\n  },\n  \"include\": [\"src\"],\n  \"references\": [{ \"path\": \"./tsconfig.node.json\" }]\n}\n",
    "kind": "text"
  },
  {
    "path": "web\\tsconfig.node.json",
    "ext": ".json",
    "size": 223,
    "sig": "a0beefcc3029a83d10955c59ba20bafdcf2f477a",
    "head": "{\n  \"compilerOptions\": {\n    \"composite\": true,\n    \"skipLibCheck\": true,\n    \"module\": \"ESNext\",\n    \"moduleResolution\": \"bundler\",\n    \"allowSyntheticDefaultImports\": true\n  },\n  \"include\": [\"vite.config.ts\"]\n}\n",
    "kind": "text"
  },
  {
    "path": "web\\vite.config.ts",
    "ext": ".ts",
    "size": 228,
    "sig": "315446ee52e986ddeab34f739e849316aee5d2aa",
    "head": "import { defineConfig } from 'vite'\nimport react from '@vitejs/plugin-react'\n\n// https://vitejs.dev/config/\nexport default defineConfig({\n  plugins: [react()],\n  server: {\n    host: '0.0.0.0',\n    port: 5173,\n  },\n})\n",
    "kind": "text"
  },
  {
    "path": "web\\src\\App.css",
    "ext": ".css",
    "size": 1461,
    "sig": "bc6d06bf4c141ec0d64c5ebcb1949cfc1b8ce01f",
    "head": ".App {\n  text-align: center;\n  min-height: 100vh;\n  background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n  display: flex;\n  align-items: center;\n  justify-content: center;\n}\n\n.App-header {\n  background: rgba(255, 255, 255, 0.95);\n  padding: 2rem;\n  border-radius: 1rem;\n  box-shadow: 0 10px 30px rgba(0, 0, 0, 0.2);\n  max-width: 600px;\n  width: 90%;\n}\n\n.App-header h1 {\n  color: #333;\n  margin-bottom: 2rem;\n  font-size: 2.5rem;\n  font-weight: 700;\n}\n\n.health-status h2 {\n  color: #555;\n  margin-bottom: 1.5rem;\n  font-size: 1.5rem;\n}\n\n.service-status {\n  display: flex;\n  flex-direction: column;\n  gap: 1rem;\n  margin-bottom: 2rem;\n}\n\n.status {\n  padding: 1rem;\n  border-radius: 0.5rem;\n  font-weight: 600;\n  font-size: 1.1rem;\n  transition: all 0.3s ease;\n}\n\n.status.loading {\n  background: #f0f0f0;\n  color: #666;\n  border: 2px solid #ddd;\n}\n\n.status.healthy {\n  background: #d4edda;\n  color: #155724;\n  border: 2px solid #c3e6cb;\n}\n\n.status.unhealthy {\n  background: #f8d7da;\n  color: #721c24;\n  border: 2px solid #f5c6cb;\n}\n\n.refresh-btn {\n  background: #667eea;\n  color: white;\n  border: none;\n  padding: 0.75rem 1.5rem;\n  border-radius: 0.5rem;\n  font-size: 1rem;\n  font-weight: 600;\n  cursor: pointer;\n  transition: all 0.3s ease;\n}\n\n.refresh-btn:hover {\n  background: #5a6fd8;\n  transform: translateY(-2px);\n  box-shadow: 0 5px 15px rgba(102, 126, 234, 0.4);\n}\n",
    "kind": "text"
  },
  {
    "path": "web\\src\\App.tsx",
    "ext": ".tsx",
    "size": 2035,
    "sig": "f467299dd1d5a79fa316e035118c0f57ef2a0cc2",
    "head": "import { useState, useEffect } from 'react'\nimport './App.css'\n\ninterface HealthStatus {\n  api: boolean | null\n  worker: boolean | null\n}\n\nfunction App() {\n  const [healthStatus, setHealthStatus] = useState<HealthStatus>({\n    api: null,\n    worker: null\n  })\n\n  const checkHealth = async () => {\n    try {\n      const apiResponse = await fetch('http://localhost:8080/health')\n      const apiOk = apiResponse.ok\n      setHealthStatus(prev => ({ ...prev, api: apiOk }))\n    } catch (error) {\n      setHealthStatus(prev => ({ ...prev, api: false }))\n    }\n\n    try {\n      const workerResponse = await fetch('http://localhost:8090/health')\n      const workerOk = workerResponse.ok\n      setHealthStatus(prev => ({ ...prev, worker: workerOk }))\n    } catch (error) {\n      setHealthStatus(prev => ({ ...prev, worker: false }))\n    }\n  }\n\n  useEffect(() => {\n    checkHealth()\n    const interval = setInterval(checkHealth, 5000) // Check every 5 seconds\n    return () => clearInterval(interval)\n  }, [])\n\n  return (\n    <div className=\"App\">\n      <header className=\"App-header\">\n        <h1>jsonify2ai Memory System</h1>\n        <div className=\"health-status\">\n          <h2>Service Health</h2>\n          <div className=\"service-status\">\n            <div className={`status ${healthStatus.api === null ? 'loading' : healthStatus.api ? 'healthy' : 'unhealthy'}`}>\n              API Service: {healthStatus.api === null ? 'Checking...' : healthStatus.api ? 'Healthy' : 'Unhealthy'}\n            </div>\n            <div className={`status ${healthStatus.worker === null ? 'loading' : healthStatus.worker ? 'healthy' : 'unhealthy'}`}>\n              Worker Service: {healthStatus.worker === null ? 'Checking...' : healthStatus.worker ? 'Healthy' : 'Unhealthy'}\n            </div>\n          </div>\n          <button onClick={checkHealth} className=\"refresh-btn\">\n            Refresh Health Check\n          </button>\n        </div>\n      </header>\n    </div>\n  )\n}\n\nexport default App\n",
    "kind": "text"
  },
  {
    "path": "web\\src\\index.css",
    "ext": ".css",
    "size": 431,
    "sig": "724e1c82c4d594dbfb7d83e512445e6a1c728088",
    "head": "* {\n  margin: 0;\n  padding: 0;\n  box-sizing: border-box;\n}\n\nbody {\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Oxygen',\n    'Ubuntu', 'Cantarell', 'Fira Sans', 'Droid Sans', 'Helvetica Neue',\n    sans-serif;\n  -webkit-font-smoothing: antialiased;\n  -moz-osx-font-smoothing: grayscale;\n}\n\ncode {\n  font-family: source-code-pro, Menlo, Monaco, Consolas, 'Courier New',\n    monospace;\n}\n",
    "kind": "text"
  },
  {
    "path": "web\\src\\main.tsx",
    "ext": ".tsx",
    "size": 246,
    "sig": "bee7ccff88d3fb804cbf84647c595a0ad33be25c",
    "head": "import React from 'react'\nimport ReactDOM from 'react-dom/client'\nimport App from './App.tsx'\nimport './index.css'\n\nReactDOM.createRoot(document.getElementById('root')!).render(\n  <React.StrictMode>\n    <App />\n  </React.StrictMode>,\n)\n",
    "kind": "text"
  },
  {
    "path": "worker\\Dockerfile",
    "ext": "",
    "size": 325,
    "sig": "673c4c5e02feff871cd6cf4f1ab0148817e8f18e",
    "head": "FROM python:3.11-slim\n\nWORKDIR /app\n\n# Copy requirements and install dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy project files\nCOPY . .\n\n# Expose port\nEXPOSE 8090\n\n# Run the application\nCMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8090\"]\n",
    "kind": "text"
  },
  {
    "path": "worker\\README.md",
    "ext": ".md",
    "size": 222,
    "sig": "709add6ced2f749fdc7aa877029d178cc07df627",
    "head": "# Worker Service\n\nFastAPI-based worker service for the jsonify2ai memory system.\n\n## Development\n\n```bash\nuvicorn app.main:app --host 0.0.0.0 --port 8090\n```\n\n## Docker\n\n```bash\ndocker compose up worker\n```\n",
    "kind": "text"
  },
  {
    "path": "worker\\requirements.txt",
    "ext": ".txt",
    "size": 197,
    "sig": "4d6a898606c32c6b79f31546a2b3947351e0406a",
    "head": "fastapi==0.104.1\nuvicorn[standard]==0.24.0\npydantic>=2.5.0\npydantic-settings==2.1.0\nqdrant-client==1.7.0\nrequests==2.31.0\npsycopg==3.1.13\npython-dotenv==1.0.0\nhttpx==0.25.2\npytest==7.4.3\n",
    "kind": "text"
  },
  {
    "path": "worker\\app\\config.py",
    "ext": ".py",
    "size": 753,
    "sig": "27c441365a7674b48ad3247e4db9b8fa4f70aee8",
    "head": "import os\nfrom pathlib import Path\nfrom pydantic_settings import BaseSettings\nfrom dotenv import load_dotenv\n\n# Load .env from repo root if present\nrepo_root = Path(__file__).parent.parent.parent\nenv_path = repo_root / \".env\"\nif env_path.exists():\n    load_dotenv(env_path)\n\nclass Settings(BaseSettings):\n    OLLAMA_URL: str = \"http://host.docker.internal:11434\"\n    QDRANT_URL: str = \"http://host.docker.internal:6333\"\n    QDRANT_COLLECTION: str = \"jsonify2ai_chunks\"\n    EMBEDDINGS_MODEL: str = \"nomic-embed-text\"\n    EMBEDDING_DIM: int = 768\n    CHUNK_SIZE: int = 800\n    CHUNK_OVERLAP: int = 100\n    DEBUG_CONFIG: str = \"0\"\n\n    class Config:\n        env_file = \".env\"\n        case_sensitive = False\n\nsettings = Settings()\n",
    "kind": "text"
  },
  {
    "path": "worker\\app\\main.py",
    "ext": ".py",
    "size": 2305,
    "sig": "0dcfc652096af59abe3ae8605359ec0b87ab40eb",
    "head": "import os\nfrom fastapi import FastAPI\nfrom .config import settings\nfrom .routers import process\n\napp = FastAPI(title=\"jsonify2ai Worker\", version=\"1.0.0\")\n\n# Log configuration on startup\n@app.on_event(\"startup\")\nasync def startup_event():\n    print(f\"Worker config -> model={settings.EMBEDDINGS_MODEL} dim={settings.EMBEDDING_DIM} dev_mode={os.getenv('EMBED_DEV_MODE', '0')} qdrant={settings.QDRANT_URL} ollama={settings.OLLAMA_URL} collection={settings.QDRANT_COLLECTION}\")\n\n# Include routers\napp.include_router(process.router, prefix=\"/process\", tags=[\"processing\"])\n\n@app.get(\"/health\")\nasync def health():\n    return {\"ok\": True}\n\n@app.get(\"/debug/config\")\nasync def debug_config():\n    \"\"\"Debug endpoint to show configuration (gated by DEBUG_CONFIG=1)\"\"\"\n    if settings.DEBUG_CONFIG != \"1\":\n        from fastapi import HTTPException\n        raise HTTPException(status_code=404, detail=\"Not found\")\n    \n    # Mask any potential secrets in URLs\n    def mask_url(url: str) -> str:\n        if not url:\n            return url\n        # Simple masking - replace password parts if they exist\n        if \"://\" in url and \"@\" in url:\n            # URL has auth, mask the password part\n            parts = url.split(\"@\")\n            if len(parts) == 2:\n                auth_part = parts[0]\n                if \":\" in auth_part:\n                    scheme_host = auth_part.split(\"://\")[0] + \"://\"\n                    username = auth_part.split(\"://\")[1].split(\":\")[0]\n                    return f\"{scheme_host}{username}:***@{parts[1]}\"\n        return url\n    \n    return {\n        \"model\": settings.EMBEDDINGS_MODEL,\n        \"dim\": settings.EMBEDDING_DIM,\n        \"dev_mode\": os.getenv(\"EMBED_DEV_MODE\", \"0\"),\n        \"qdrant_url\": mask_url(settings.QDRANT_URL),\n        \"ollama_url\": mask_url(settings.OLLAMA_URL),\n        \"collection\": settings.QDRANT_COLLECTION,\n        \"chunk_size\": settings.CHUNK_SIZE,\n        \"chunk_overlap\": settings.CHUNK_OVERLAP,\n        \"debug_enabled\": settings.DEBUG_CONFIG == \"1\"\n    }\n\n@app.get(\"/\")\nasync def root():\n    return {\"message\": \"jsonify2ai Worker Service\"}\n\nif __name__ == \"__main__\":\n    import uvicorn\n    port = int(os.getenv(\"PORT_WORKER\", \"8090\"))\n    uvicorn.run(app, host=\"0.0.0.0\", port=port)\n",
    "kind": "text"
  },
  {
    "path": "worker\\app\\__init__.py",
    "ext": ".py",
    "size": 22,
    "sig": "5f54f453afab01cb47b70f31bc9977b289a6fe97",
    "head": "# Worker app package\n",
    "kind": "text"
  },
  {
    "path": "worker\\app\\routers\\process.py",
    "ext": ".py",
    "size": 4994,
    "sig": "3509f44328b8a28fe904e76cfe93b42244763fc0",
    "head": "import uuid\nimport os\nfrom pathlib import Path\nfrom typing import Optional\nfrom fastapi import APIRouter, HTTPException\nfrom pydantic import BaseModel, Field\nfrom ..services.chunker import chunk_text\nfrom ..services.embed_ollama import embed_texts\nfrom ..services.qdrant_client import get_qdrant_client, upsert_points\nfrom ..services.qdrant_minimal import ensure_collection_minimal\nfrom ..config import settings\n\nrouter = APIRouter()\n\nclass ProcessTextRequest(BaseModel):\n    document_id: str = Field(..., description=\"Unique document identifier\")\n    text: Optional[str] = Field(None, description=\"Raw text content\")\n    path: Optional[str] = Field(None, description=\"Path to text file\")\n\nclass ProcessTextResponse(BaseModel):\n    ok: bool\n    document_id: str\n    chunks: int\n    embedded: int\n    upserted: int\n    collection: str\n    error: Optional[str] = None\n\n@router.post(\"/text\", response_model=ProcessTextResponse)\nasync def process_text(request: ProcessTextRequest):\n    \"\"\"\n    Process text: chunk, embed, and store in Qdrant.\n    \n    Accepts either raw text or file path, processes into chunks,\n    generates embeddings via Ollama, and stores in vector database.\n    \"\"\"\n    try:\n        # Determine text content\n        text_content = request.text\n        \n        if request.path and not text_content:\n            # Read from file path\n            file_path = Path(request.path)\n            \n            if not file_path.exists():\n                raise HTTPException(status_code=400, detail=\"File not found\")\n            \n            if file_path.stat().st_size > 5 * 1024 * 1024:  # 5MB limit\n                raise HTTPException(status_code=400, detail=\"File too large (>5MB)\")\n            \n            try:\n                text_content = file_path.read_text(encoding='utf-8')\n            except Exception as e:\n                raise HTTPException(status_code=400, detail=f\"Failed to read file: {str(e)}\")\n        \n        if not text_content or not text_content.strip():\n            raise HTTPException(status_code=400, detail=\"No text content provided\")\n        \n        # Chunk the text\n        chunks = chunk_text(\n            text_content, \n            settings.CHUNK_SIZE, \n            settings.CHUNK_OVERLAP\n        )\n        \n        if not chunks:\n            raise HTTPException(status_code=400, detail=\"No chunks generated\")\n        \n        # Generate embeddings\n        try:\n            embeddings = embed_texts(chunks, settings.EMBEDDINGS_MODEL, settings.OLLAMA_URL, settings.EMBEDDING_DIM)\n            \n            if len(embeddings) != len(chunks):\n                raise ValueError(\"Embedding count mismatch\")\n                \n        except ValueError as e:\n            # Return 502 for embedding errors\n            return ProcessTextResponse(\n                ok=False,\n                document_id=request.document_id,\n                chunks=len(chunks),\n                embedded=0,\n                upserted=0,\n                collection=settings.QDRANT_COLLECTION,\n                error=f\"Embedding error: {str(e)}\"\n            )\n        \n        # Prepare Qdrant data\n        qdrant_client = get_qdrant_client()\n        collection_name = settings.QDRANT_COLLECTION\n        \n        # Ensure collection exists with correct dimensions using minimal helper\n        success, error_msg = ensure_collection_minimal(collection_name, settings.EMBEDDING_DIM)\n        if not success:\n            return ProcessTextResponse(\n                ok=False,\n                document_id=request.document_id,\n                chunks=len(chunks),\n                embedded=len(embeddings),\n                upserted=0,\n                collection=collection_name,\n                error=f\"Collection error: {error_msg}\"\n            )\n        \n        # Prepare payloads and IDs\n        payloads = []\n        ids = []\n        \n        for idx, chunk in enumerate(chunks):\n            payloads.append({\n                \"document_id\": request.document_id,\n                \"idx\": idx,\n                \"text\": chunk\n            })\n            ids.append(f\"{request.document_id}:{idx}\")\n        \n        # Upsert to Qdrant\n        upsert_points(qdrant_client, collection_name, embeddings, payloads, ids)\n        \n        return ProcessTextResponse(\n            ok=True,\n            document_id=request.document_id,\n            chunks=len(chunks),\n            embedded=len(embeddings),\n            upserted=len(ids),\n            collection=collection_name\n        )\n        \n    except HTTPException:\n        raise\n    except Exception as e:\n        # Return 502 for external service errors\n        return ProcessTextResponse(\n            ok=False,\n            document_id=request.document_id,\n            chunks=0,\n            embedded=0,\n            upserted=0,\n            collection=settings.QDRANT_COLLECTION,\n            error=str(e)\n        )\n",
    "kind": "text"
  },
  {
    "path": "worker\\app\\routers\\__init__.py",
    "ext": ".py",
    "size": 19,
    "sig": "0a7197d5040c4c2b8c2387e0f32a200897f1d2f5",
    "head": "# Routers package\n",
    "kind": "text"
  },
  {
    "path": "worker\\app\\services\\chunker.py",
    "ext": ".py",
    "size": 973,
    "sig": "79cc7f554844e81f56fb51520b70927d10d59480",
    "head": "def chunk_text(text: str, size: int, overlap: int) -> list[str]:\n    \"\"\"\n    Chunk text using sliding window approach.\n    \n    Args:\n        text: Input text to chunk\n        size: Maximum chunk size in characters\n        overlap: Overlap between chunks in characters\n    \n    Returns:\n        List of text chunks\n    \"\"\"\n    if not text or size <= 0:\n        return []\n    \n    if len(text) <= size:\n        return [text]\n    \n    chunks = []\n    start = 0\n    \n    while start < len(text):\n        end = min(start + size, len(text))\n        chunk = text[start:end]\n        chunks.append(chunk)\n        \n        if end >= len(text):\n            break\n            \n        # Move start position, accounting for overlap\n        start = end - overlap\n        # Ensure we don't go backwards\n        if start <= 0:\n            start = 1\n        # Ensure we make progress\n        if start >= end:\n            break\n    \n    return chunks\n",
    "kind": "text"
  },
  {
    "path": "worker\\app\\services\\embed_ollama.py",
    "ext": ".py",
    "size": 3568,
    "sig": "c141fca0607031cb6b17a488b974f4a2d7ce08f8",
    "head": "import requests\nimport hashlib\nimport os\nfrom typing import List\nimport sys\nfrom pathlib import Path\n\n# Add the app directory to the path for imports\nsys.path.insert(0, str(Path(__file__).parent.parent))\nfrom config import settings\n\ndef _parse_embeddings(json_obj) -> List[List[float]]:\n    \"\"\"\n    Parse embeddings from Ollama API response.\n    \n    Handles both response shapes:\n    - Single input: {\"embedding\": [...]}\n    - Batch input: {\"embeddings\": [{\"embedding\": [...]}, ...]}\n    \n    Args:\n        json_obj: Parsed JSON response from Ollama\n        \n    Returns:\n        List of embedding vectors\n        \n    Raises:\n        ValueError: If response shape is unexpected\n    \"\"\"\n    if \"embedding\" in json_obj:\n        # Single input response\n        return [json_obj[\"embedding\"]]\n    elif \"embeddings\" in json_obj:\n        # Batch response\n        return [item[\"embedding\"] for item in json_obj[\"embeddings\"]]\n    else:\n        raise ValueError(\"Unexpected Ollama response format\")\n\ndef _generate_dummy_embedding(text: str, dim: int) -> List[float]:\n    \"\"\"\n    Generate deterministic dummy embedding for dev mode.\n    \n    Args:\n        text: Input text to hash\n        dim: Embedding dimension\n        \n    Returns:\n        List of floats in [0, 1) based on text hash\n    \"\"\"\n    # Create stable hash\n    hash_obj = hashlib.sha256(text.encode('utf-8'))\n    hash_bytes = hash_obj.digest()\n    \n    # Map hash bytes to floats in [0, 1)\n    embedding = []\n    for i in range(dim):\n        byte_idx = i % len(hash_bytes)\n        # Normalize byte value to [0, 1)\n        embedding.append(hash_bytes[byte_idx] / 256.0)\n    \n    return embedding\n\ndef embed_texts(texts: List[str], model: str = None, base_url: str = None, dim: int = None) -> List[List[float]]:\n    \"\"\"\n    Embed texts using Ollama embeddings API or dev mode.\n    \n    Args:\n        texts: List of texts to embed\n        model: Model name (defaults to config)\n        base_url: Ollama base URL (defaults to config)\n        dim: Embedding dimension (defaults to config)\n    \n    Returns:\n        List of embedding vectors\n        \n    Raises:\n        ValueError: On API errors or response format issues\n    \"\"\"\n    if not texts:\n        return []\n    \n    model = model or settings.EMBEDDINGS_MODEL\n    base_url = base_url or settings.OLLAMA_URL\n    dim = dim or settings.EMBEDDING_DIM\n    \n    # Check for dev mode\n    if os.getenv(\"EMBED_DEV_MODE\") == \"1\":\n        return [_generate_dummy_embedding(text, dim) for text in texts]\n    \n    # Call Ollama API\n    url = f\"{base_url}/api/embeddings\"\n    payload = {\n        \"model\": model,\n        \"input\": texts\n    }\n    \n    try:\n        response = requests.post(url, json=payload, timeout=30)\n        response.raise_for_status()\n        \n        result = response.json()\n        embeddings = _parse_embeddings(result)\n        \n        # Validate response length matches input\n        if len(embeddings) != len(texts):\n            raise ValueError(f\"Embedding count mismatch: expected {len(texts)}, got {len(embeddings)}\")\n        \n        return embeddings\n        \n    except requests.HTTPError as e:\n        raise ValueError(f\"Ollama API error: {e}\")\n    except requests.RequestException as e:\n        raise ValueError(f\"Network error: {e}\")\n    except (ValueError, KeyError, TypeError) as e:\n        raise ValueError(f\"Response parsing error: {e}\")\n    except Exception as e:\n        raise ValueError(f\"Unexpected error: {e}\")\n",
    "kind": "text"
  },
  {
    "path": "worker\\app\\services\\qdrant_client.py",
    "ext": ".py",
    "size": 2313,
    "sig": "2fb9fc9be1de680a125ec674dd7fa44e30fbe66c",
    "head": "from qdrant_client import QdrantClient\nfrom qdrant_client.models import Distance, VectorParams\nfrom typing import List, Dict, Any\nfrom ..config import settings\n\ndef get_qdrant_client() -> QdrantClient:\n    \"\"\"Get Qdrant client instance.\"\"\"\n    return QdrantClient(url=settings.QDRANT_URL)\n\ndef ensure_collection(client: QdrantClient, name: str, dim: int) -> None:\n    \"\"\"\n    Ensure Qdrant collection exists with correct dimensions.\n    \n    Args:\n        client: Qdrant client instance\n        name: Collection name\n        dim: Expected vector dimension\n    \n    Raises:\n        ValueError: If collection exists with wrong dimensions\n    \"\"\"\n    collections = client.get_collections()\n    collection_names = [c.name for c in collections.collections]\n    \n    if name in collection_names:\n        # Verify existing collection dimensions\n        collection_info = client.get_collection(name)\n        current_dim = collection_info.config.params.vectors.size\n        \n        if current_dim != dim:\n            raise ValueError(\n                f\"Collection '{name}' exists with dimension {current_dim}, \"\n                f\"but model expects {dim}. Please use a different collection name \"\n                f\"or change the embedding model.\"\n            )\n    else:\n        # Create new collection\n        client.create_collection(\n            collection_name=name,\n            vectors_config=VectorParams(size=dim, distance=Distance.COSINE)\n        )\n\ndef upsert_points(\n    client: QdrantClient, \n    name: str, \n    embeddings: List[List[float]], \n    payloads: List[Dict[str, Any]], \n    ids: List[str]\n) -> None:\n    \"\"\"\n    Upsert points to Qdrant collection.\n    \n    Args:\n        client: Qdrant client instance\n        name: Collection name\n        embeddings: List of embedding vectors\n        payloads: List of payload dictionaries\n        ids: List of point IDs\n    \"\"\"\n    points = []\n    for i, (embedding, payload, point_id) in enumerate(zip(embeddings, payloads, ids)):\n        points.append({\n            \"id\": point_id,\n            \"vector\": embedding,\n            \"payload\": payload\n        })\n    \n    client.upsert(\n        collection_name=name,\n        points=points,\n        parallel=1  # MVP: sequential processing\n    )\n",
    "kind": "text"
  },
  {
    "path": "worker\\app\\services\\qdrant_minimal.py",
    "ext": ".py",
    "size": 3474,
    "sig": "b8fc419e5a5d44a87957e44b9692d8b22b297c4b",
    "head": "import requests\nimport logging\nfrom typing import Optional, Tuple\nfrom ..config import settings\n\nlogger = logging.getLogger(__name__)\n\ndef ensure_collection_minimal(name: str, dim: int) -> Tuple[bool, Optional[str]]:\n    \"\"\"\n    Ensure Qdrant collection exists with correct dimensions using minimal HTTP requests.\n    \n    Args:\n        name: Collection name\n        dim: Expected vector dimension\n    \n    Returns:\n        Tuple of (success: bool, error_message: Optional[str])\n    \"\"\"\n    try:\n        # Check if collection exists\n        url = f\"{settings.QDRANT_URL}/collections/{name}\"\n        response = requests.get(url, timeout=10)\n        \n        if response.status_code == 200:\n            # Collection exists, verify dimensions\n            try:\n                data = response.json()\n                # Extract only the essential fields, ignore optimizer_config entirely\n                vectors_config = data.get(\"config\", {}).get(\"params\", {}).get(\"vectors\", {})\n                current_dim = vectors_config.get(\"size\")\n                distance = vectors_config.get(\"distance\")\n                \n                if current_dim is None:\n                    return False, f\"Collection '{name}' exists but has no vector size configuration\"\n                \n                if current_dim != dim:\n                    return False, f\"Collection '{name}' exists with dimension {current_dim}, but model expects {dim}\"\n                \n                logger.info(f\"Collection '{name}' verified: size={current_dim}, distance={distance}\")\n                return True, None\n                \n            except (KeyError, TypeError) as e:\n                return False, f\"Collection '{name}' exists but has invalid configuration: {str(e)}\"\n                \n        elif response.status_code == 404:\n            # Collection doesn't exist, create it\n            create_url = f\"{settings.QDRANT_URL}/collections/{name}\"\n            create_data = {\n                \"vectors\": {\n                    \"size\": dim,\n                    \"distance\": \"Cosine\"\n                }\n            }\n            \n            create_response = requests.put(create_url, json=create_data, timeout=10)\n            \n            if create_response.status_code == 200:\n                logger.info(f\"Collection '{name}' created: size={dim}, distance=Cosine\")\n                return True, None\n            else:\n                error_msg = f\"Failed to create collection '{name}': status {create_response.status_code}\"\n                try:\n                    body = create_response.text.strip()\n                    if body:\n                        error_msg += f\", response: {body}\"\n                except:\n                    pass\n                return False, error_msg\n                \n        else:\n            # Unexpected status code\n            error_msg = f\"Unexpected response checking collection '{name}': status {response.status_code}\"\n            try:\n                body = response.text.strip()\n                if body:\n                    error_msg += f\", response: {body}\"\n            except:\n                pass\n            return False, error_msg\n            \n    except requests.exceptions.RequestException as e:\n        return False, f\"Request error ensuring collection '{name}': {str(e)}\"\n    except Exception as e:\n        return False, f\"Unexpected error ensuring collection '{name}': {str(e)}\"\n",
    "kind": "text"
  },
  {
    "path": "worker\\app\\services\\__init__.py",
    "ext": ".py",
    "size": 20,
    "sig": "2bf64297f064210c51b2b33d8cbd169f5b28b553",
    "head": "# Services package\n",
    "kind": "text"
  },
  {
    "path": "worker\\tests\\test_embed_unit.py",
    "ext": ".py",
    "size": 6213,
    "sig": "e94e58a5bce08de539e7e36959cac4f0c9959538",
    "head": "import pytest\nfrom unittest.mock import patch, Mock\nimport os\nimport sys\nimport requests\nfrom pathlib import Path\n\n# Add the app directory to the path\nsys.path.insert(0, str(Path(__file__).parent.parent / \"app\"))\n\nfrom services.embed_ollama import embed_texts, _parse_embeddings, _generate_dummy_embedding\n\n\nclass TestEmbedOllama:\n    \n    def test_dev_mode_returns_correct_shape(self):\n        \"\"\"Test that dev mode returns vectors of correct shape and is deterministic.\"\"\"\n        # Set dev mode\n        os.environ[\"EMBED_DEV_MODE\"] = \"1\"\n        \n        try:\n            texts = [\"hello world\", \"test text\"]\n            embeddings = embed_texts(texts, dim=768)\n            \n            # Check shape\n            assert len(embeddings) == 2\n            assert len(embeddings[0]) == 768\n            assert len(embeddings[1]) == 768\n            \n            # Check deterministic (same input -> same vector)\n            embeddings2 = embed_texts(texts, dim=768)\n            assert embeddings == embeddings2\n            \n            # Check values are in [0, 1)\n            for emb in embeddings:\n                for val in emb:\n                    assert 0 <= val < 1\n                    \n        finally:\n            # Clean up\n            if \"EMBED_DEV_MODE\" in os.environ:\n                del os.environ[\"EMBED_DEV_MODE\"]\n    \n    def test_dev_mode_different_texts_different_vectors(self):\n        \"\"\"Test that different texts produce different vectors in dev mode.\"\"\"\n        os.environ[\"EMBED_DEV_MODE\"] = \"1\"\n        \n        try:\n            text1 = [\"hello\"]\n            text2 = [\"world\"]\n            \n            emb1 = embed_texts(text1, dim=10)\n            emb2 = embed_texts(text2, dim=10)\n            \n            # Different texts should produce different vectors\n            assert emb1 != emb2\n            \n        finally:\n            if \"EMBED_DEV_MODE\" in os.environ:\n                del os.environ[\"EMBED_DEV_MODE\"]\n    \n    def test_parse_embeddings_single_input(self):\n        \"\"\"Test parser accepts single-input shape.\"\"\"\n        response = {\"embedding\": [0.1, 0.2, 0.3]}\n        result = _parse_embeddings(response)\n        \n        assert result == [[0.1, 0.2, 0.3]]\n    \n    def test_parse_embeddings_batch_input(self):\n        \"\"\"Test parser accepts batch shape.\"\"\"\n        response = {\n            \"embeddings\": [\n                {\"embedding\": [0.1, 0.2, 0.3]},\n                {\"embedding\": [0.4, 0.5, 0.6]}\n            ]\n        }\n        result = _parse_embeddings(response)\n        \n        assert result == [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]]\n    \n    def test_parse_embeddings_invalid_format(self):\n        \"\"\"Test parser accepts batch shape.\"\"\"\n        response = {\"data\": [{\"embedding\": [0.1, 0.2]}]}\n        \n        with pytest.raises(ValueError, match=\"Unexpected Ollama response format\"):\n            _parse_embeddings(response)\n    \n    @patch('requests.post')\n    def test_ollama_api_single_response(self, mock_post):\n        \"\"\"Test Ollama API call with single response format.\"\"\"\n        # Mock successful response\n        mock_response = Mock()\n        mock_response.status_code = 200\n        mock_response.json.return_value = {\"embedding\": [0.1, 0.2, 0.3]}\n        mock_post.return_value = mock_response\n        \n        # Ensure dev mode is off\n        if \"EMBED_DEV_MODE\" in os.environ:\n            del os.environ[\"EMBED_DEV_MODE\"]\n        \n        result = embed_texts([\"test\"], dim=3)\n        \n        assert result == [[0.1, 0.2, 0.3]]\n        mock_post.assert_called_once()\n    \n    @patch('requests.post')\n    def test_ollama_api_batch_response(self, mock_post):\n        \"\"\"Test Ollama API call with batch response format.\"\"\"\n        # Mock successful response\n        mock_response = Mock()\n        mock_response.status_code = 200\n        mock_response.json.return_value = {\n            \"embeddings\": [\n                {\"embedding\": [0.1, 0.2, 0.3]},\n                {\"embedding\": [0.4, 0.5, 0.6]}\n            ]\n        }\n        mock_post.return_value = mock_response\n        \n        # Ensure dev mode is off\n        if \"EMBED_DEV_MODE\" in os.environ:\n            del os.environ[\"EMBED_DEV_MODE\"]\n        \n        result = embed_texts([\"test1\", \"test2\"], dim=3)\n        \n        assert result == [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]]\n        mock_post.assert_called_once()\n    \n    @patch('requests.post')\n    def test_ollama_api_http_error(self, mock_post):\n        \"\"\"Test Ollama API call with HTTP error.\"\"\"\n        # Mock HTTP error\n        mock_response = Mock()\n        mock_response.status_code = 500\n        mock_response.raise_for_status.side_effect = requests.HTTPError(\"Internal Server Error\")\n        mock_post.return_value = mock_response\n        \n        # Ensure dev mode is off\n        if \"EMBED_DEV_MODE\" in os.environ:\n            del os.environ[\"EMBED_DEV_MODE\"]\n        \n        with pytest.raises(ValueError, match=\"Ollama API error\"):\n            embed_texts([\"test\"], dim=3)\n    \n    @patch('requests.post')\n    def test_ollama_api_count_mismatch(self, mock_post):\n        \"\"\"Test Ollama API call with count mismatch.\"\"\"\n        # Mock response with wrong count\n        mock_response = Mock()\n        mock_response.status_code = 200\n        mock_response.json.return_value = {\"embedding\": [0.1, 0.2, 0.3]}\n        mock_post.return_value = mock_response\n        \n        # Ensure dev mode is off\n        if \"EMBED_DEV_MODE\" in os.environ:\n            del os.environ[\"EMBED_DEV_MODE\"]\n        \n        with pytest.raises(ValueError, match=\"Embedding count mismatch\"):\n            embed_texts([\"test1\", \"test2\"], dim=3)\n    \n    def test_generate_dummy_embedding(self):\n        \"\"\"Test dummy embedding generation.\"\"\"\n        text = \"hello world\"\n        dim = 10\n        \n        result = _generate_dummy_embedding(text, dim)\n        \n        assert len(result) == dim\n        assert all(0 <= val < 1 for val in result)\n        \n        # Same text should produce same embedding\n        result2 = _generate_dummy_embedding(text, dim)\n        assert result == result2\n",
    "kind": "text"
  },
  {
    "path": "worker\\tests\\test_process_unit.py",
    "ext": ".py",
    "size": 4149,
    "sig": "adc05efd958045efd6ceb566f4abf043e833791d",
    "head": "import os\nimport pytest\nfrom unittest.mock import patch, MagicMock\nfrom app.services.chunker import chunk_text\nfrom app.routers.process import ProcessTextRequest, ProcessTextResponse\n\nclass TestChunker:\n    \"\"\"Unit tests for text chunking service.\"\"\"\n    \n    def test_chunk_text_empty(self):\n        \"\"\"Test chunking empty text.\"\"\"\n        result = chunk_text(\"\", 100, 20)\n        assert result == []\n    \n    def test_chunk_text_smaller_than_chunk(self):\n        \"\"\"Test text smaller than chunk size.\"\"\"\n        text = \"Hello world\"\n        result = chunk_text(text, 100, 20)\n        assert result == [\"Hello world\"]\n    \n    def test_chunk_text_exact_chunk_size(self):\n        \"\"\"Test text exactly chunk size.\"\"\"\n        text = \"a\" * 100\n        result = chunk_text(text, 100, 20)\n        assert result == [\"a\" * 100]\n    \n    def test_chunk_text_with_overlap(self):\n        \"\"\"Test chunking with overlap.\"\"\"\n        text = \"a\" * 200\n        result = chunk_text(text, 100, 20)\n        # With 200 chars, chunk size 100, overlap 20, we get 3 chunks:\n        # First: 0-100 (100 chars), Second: 80-180 (100 chars), Third: 160-200 (40 chars)\n        assert len(result) == 3\n        assert result[0] == \"a\" * 100\n        assert result[1] == \"a\" * 100\n        assert result[2] == \"a\" * 40\n        # Check overlap between first two chunks\n        assert result[0][-20:] == result[1][:20]\n    \n    def test_chunk_text_multiple_chunks(self):\n        \"\"\"Test multiple chunks.\"\"\"\n        text = \"a\" * 300\n        result = chunk_text(text, 100, 20)\n        # With 300 chars, chunk size 100, overlap 20, we get 4 chunks:\n        # First: 0-100, Second: 80-180, Third: 160-260, Fourth: 240-300\n        assert len(result) == 4\n        assert all(len(chunk) <= 100 for chunk in result)\n        # Check overlaps\n        assert result[0][-20:] == result[1][:20]\n        assert result[1][-20:] == result[2][:20]\n        assert result[2][-20:] == result[3][:20]\n    \n    def test_chunk_text_invalid_params(self):\n        \"\"\"Test invalid parameters.\"\"\"\n        result = chunk_text(\"hello\", 0, 20)\n        assert result == []\n        \n        result = chunk_text(\"hello\", -1, 20)\n        assert result == []\n\nclass TestProcessTextRequest:\n    \"\"\"Test request/response models.\"\"\"\n    \n    def test_process_text_request(self):\n        \"\"\"Test request model validation.\"\"\"\n        request = ProcessTextRequest(\n            document_id=\"test-123\",\n            text=\"Hello world\"\n        )\n        assert request.document_id == \"test-123\"\n        assert request.text == \"Hello world\"\n        assert request.path is None\n    \n    def test_process_text_response(self):\n        \"\"\"Test response model validation.\"\"\"\n        response = ProcessTextResponse(\n            ok=True,\n            document_id=\"test-123\",\n            chunks=2,\n            embedded=2,\n            upserted=2,\n            collection=\"test_collection\"\n        )\n        assert response.ok is True\n        assert response.chunks == 2\n        assert response.collection == \"test_collection\"\n\n@pytest.mark.skipif(\n    os.getenv(\"SERVICES_UP\") != \"1\",\n    reason=\"SERVICES_UP not set to 1 - skipping networked tests\"\n)\nclass TestProcessTextIntegration:\n    \"\"\"Integration tests that require external services.\"\"\"\n    \n    def test_process_text_endpoint_smoke(self, client):\n        \"\"\"Smoke test for /process/text endpoint.\"\"\"\n        # This test will be skipped unless SERVICES_UP=1\n        request_data = {\n            \"document_id\": \"00000000-0000-0000-0000-000000000000\",\n            \"text\": \"Hello world, this is a test document for the jsonify2ai pipeline.\"\n        }\n        \n        response = client.post(\"/process/text\", json=request_data)\n        assert response.status_code == 200\n        \n        data = response.json()\n        assert data[\"ok\"] is True\n        assert data[\"document_id\"] == request_data[\"document_id\"]\n        assert data[\"chunks\"] > 0\n        assert data[\"embedded\"] > 0\n        assert data[\"upserted\"] > 0\n        assert data[\"collection\"] == \"jsonify2ai_chunks\"\n",
    "kind": "text"
  },
  {
    "path": "worker\\tests\\test_qdrant_minimal.py",
    "ext": ".py",
    "size": 4232,
    "sig": "f524a1d8f36a68e293384a2b123f3fc530cac539",
    "head": "import pytest\nfrom unittest.mock import patch, Mock\nfrom app.services.qdrant_minimal import ensure_collection_minimal\n\nclass TestQdrantMinimal:\n    \n    @patch('requests.get')\n    @patch('requests.put')\n    def test_ensure_collection_existing_success(self, mock_put, mock_get):\n        \"\"\"Test successful verification of existing collection\"\"\"\n        # Mock existing collection response\n        mock_response = Mock()\n        mock_response.status_code = 200\n        mock_response.json.return_value = {\n            \"config\": {\n                \"params\": {\n                    \"vectors\": {\n                        \"size\": 768,\n                        \"distance\": \"Cosine\"\n                    }\n                }\n            }\n        }\n        mock_get.return_value = mock_response\n        \n        success, error = ensure_collection_minimal(\"test_collection\", 768)\n        \n        assert success is True\n        assert error is None\n        mock_get.assert_called_once()\n        mock_put.assert_not_called()\n    \n    @patch('requests.get')\n    @patch('requests.put')\n    def test_ensure_collection_existing_dimension_mismatch(self, mock_put, mock_get):\n        \"\"\"Test dimension mismatch in existing collection\"\"\"\n        # Mock existing collection with wrong dimension\n        mock_response = Mock()\n        mock_response.status_code = 200\n        mock_response.json.return_value = {\n            \"config\": {\n                \"params\": {\n                    \"vectors\": {\n                        \"size\": 512,\n                        \"distance\": \"Cosine\"\n                    }\n                }\n            }\n        }\n        mock_get.return_value = mock_response\n        \n        success, error = ensure_collection_minimal(\"test_collection\", 768)\n        \n        assert success is False\n        assert \"dimension 512, but model expects 768\" in error\n        mock_get.assert_called_once()\n        mock_put.assert_not_called()\n    \n    @patch('requests.get')\n    @patch('requests.put')\n    def test_ensure_collection_create_new(self, mock_put, mock_get):\n        \"\"\"Test creating new collection\"\"\"\n        # Mock collection not found\n        mock_get_response = Mock()\n        mock_get_response.status_code = 404\n        mock_get.return_value = mock_get_response\n        \n        # Mock successful creation\n        mock_put_response = Mock()\n        mock_put_response.status_code = 200\n        mock_put.return_value = mock_put_response\n        \n        success, error = ensure_collection_minimal(\"test_collection\", 768)\n        \n        assert success is True\n        assert error is None\n        mock_get.assert_called_once()\n        mock_put.assert_called_once()\n        \n        # Verify PUT call arguments\n        put_call = mock_put.call_args\n        assert put_call[0][0].endswith(\"/collections/test_collection\")\n        assert put_call[1][\"json\"] == {\n            \"vectors\": {\n                \"size\": 768,\n                \"distance\": \"Cosine\"\n            }\n        }\n    \n    @patch('requests.get')\n    @patch('requests.put')\n    def test_ensure_collection_create_failure(self, mock_put, mock_get):\n        \"\"\"Test failure when creating collection\"\"\"\n        # Mock collection not found\n        mock_get_response = Mock()\n        mock_get_response.status_code = 404\n        mock_get.return_value = mock_get_response\n        \n        # Mock creation failure\n        mock_put_response = Mock()\n        mock_put_response.status_code = 500\n        mock_put_response.text = \"Internal server error\"\n        mock_put.return_value = mock_put_response\n        \n        success, error = ensure_collection_minimal(\"test_collection\", 768)\n        \n        assert success is False\n        assert \"Failed to create collection\" in error\n        assert \"status 500\" in error\n        assert \"Internal server error\" in error\n    \n    @patch('requests.get')\n    def test_ensure_collection_request_exception(self, mock_get):\n        \"\"\"Test handling of request exceptions\"\"\"\n        mock_get.side_effect = Exception(\"Connection error\")\n        \n        success, error = ensure_collection_minimal(\"test_collection\", 768)\n        \n        assert success is False\n        assert \"Unexpected error\" in error\n        assert \"Connection error\" in error\n",
    "kind": "text"
  },
  {
    "path": "worker\\tests\\__init__.py",
    "ext": ".py",
    "size": 17,
    "sig": "f9a9ae1aa8fee796573c8a2fc3f1cc17bec3cf31",
    "head": "# Tests package\n",
    "kind": "text"
  }
]