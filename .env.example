# ===== Core endpoints (reuse your existing local services) =====
# If Qdrant/Ollama run outside this stack, keep host.docker.internal.
QDRANT_URL=http://host.docker.internal:6333   # alt: http://localhost:6333
OLLAMA_URL=http://host.docker.internal:11434  # alt: http://localhost:11434

# ===== Worker service =====
PORT_WORKER=8090
QDRANT_COLLECTION=jsonify2ai_chunks_768

# Chunk & embed
EMBEDDINGS_MODEL=nomic-embed-text:latest
EMBEDDING_DIM=768
CHUNK_SIZE=800
CHUNK_OVERLAP=100

# Dev toggles (safe defaults while building)
EMBED_DEV_MODE=1     # 1 = stub vectors (no heavy deps)
AUDIO_DEV_MODE=1     # 1 = stub/fast path (dev-safe default)
HF_HUB_ENABLE_HF_TRANSFER=0  # keep disabled unless you need fast HF downloads

# Optional features
IMAGES_CAPTION=0     # 1 to enable BLIP captioning in worker

# Ask settings
ASK_MODE=retrieval   # retrieval | llm
ASK_MODEL=qwen2.5:3b-instruct-q4_K_M

# STT (only if you turn audio back on later)
STT_MODEL=tiny
AUDIO_DEV_MODE=0          # set to 0 in shell to run real STT (override .env dev default)
WHISPER_MODEL=base.en
WHISPER_DEVICE=cpu
WHISPER_COMPUTE_TYPE=int8
# Dropâ€‘zone defaults (CLI scripts)
DROPZONE_DIR=data/dropzone
EXPORT_JSONL=data/exports/ingest.jsonl

# Lower this to build HNSW sooner (use with care)
INDEXING_THRESHOLD=100

# No secrets here; example file only.
