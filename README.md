<p align="center">
  <img src="docs/jsonify2ai_logo.png" alt="Jsonify2AI logo" width="165"/>
</p>

<h3 align="center">Your data. Your models. Your machine.</h3>

<p align="center">
  A local-first RAG engine that transforms messy files into searchable, AI-synthesized intelligence, running entirely on your hardware with zero cloud dependency.
</p>

---

[![MIT License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
![OS](https://img.shields.io/badge/tested-Linux%20%7C%20macOS%20%7C%20Windows-lightblue)
[![CI](https://github.com/Mugiwara555343/jsonify2ai/actions/workflows/ci.yml/badge.svg)](https://github.com/Mugiwara555343/jsonify2ai/actions/workflows/ci.yml)
[![GitHub](https://img.shields.io/badge/GitHub-Follow-black?logo=github)](https://github.com/Mugiwara555343)

---

## ğŸ”’ Why This Exists: Data Sovereignty

Most AI tools require sending your documents to a cloud API. Every query, every file, every conversation, routed through someone else's servers.

**Jsonify2ai takes the opposite approach:**

- **Nothing leaves your machine.** Every embedding, every search query, every LLM response is computed locally.
- **No API keys required.** No OpenAI, no cloud credits, no usage caps. You own the entire pipeline.
- **Full provenance.** Every chunk is SHA-256 hashed, timestamped, and traceable back to its source file.
- **Deterministic & idempotent.** Re-ingesting the same file produces zero duplicates â€” guaranteed by UUID5 deterministic document IDs.

---

## âš¡ The 400K Milestone

Most local RAG implementations break down after a few pages. Jsonify2ai was engineered to handle **massive local datasets** without precision loss:

| Metric | Value |
|--------|-------|
| **Tested corpus size** | 400,000+ characters (~100K tokens) |
| **Embedding model** | `nomic-embed-text` (768 dimensions) |
| **Vector similarity** | Cosine distance via Qdrant |
| **Chunk strategy** | 800-char sliding window, 100-char overlap, whitespace-aware cuts |
| **Deduplication** | SHA-256 file hash â†’ UUID5 deterministic IDs |
| **Batch ingestion** | 64 embeddings / 128 upserts per batch |

The pipeline is designed so that ingestion never blocks retrieval. You can search and ask questions while new documents are still being indexed.

---

## ğŸ§  Architecture: Spine-and-Worker

The system is built as a distributed microservice stack, a **Go API spine** for stability and concurrency, with a **Python worker** for the heavy AI/RAG logic.

| Service | Stack | Role |
|---------|-------|------|
| **API** | Go / Gin | Auth, rate limiting, CORS, reverse-proxy to Worker |
| **Worker** | Python / FastAPI | Ingestion, chunking, embedding, semantic search, LLM synthesis |
| **Qdrant** | Vector DB | 768-dim cosine similarity search with payload indexing |
| **Web UI** | React / Vite / TypeScript | Dark-mode interface with drag-and-drop, ask panel, document drawer |
| **Watcher** | Python daemon | Auto-ingests files dropped into `data/dropzone/` |

All five services are orchestrated via Docker Compose with health checks on every container.

---

## ğŸ›  Features

### Ingestion

- ğŸ“„ **Multi-format support** â€” TXT, MD, CSV, TSV, JSON, JSONL, HTML, PDF, DOCX
- ğŸ¤ **Audio transcription** â€” WAV, MP3, M4A, FLAC, OGG via Whisper *(optional module)*
- ğŸ–¼ï¸ **Image captioning** â€” BLIP-based caption extraction *(optional module)*
- ğŸ’¬ **ChatGPT export parsing** â€” Dedicated parser for conversation-aware chunking
- ğŸ—£ï¸ **Transcript detection** â€” Auto-detects and structures dialogue formats
- ğŸ“‚ **Dropzone watcher** â€” Daemon auto-ingests new files with configurable polling interval

### Search & Retrieval

- ğŸ” **Semantic search** â€” Embedding-based similarity search across your entire corpus
- ğŸ¯ **Scoped queries** â€” Filter by document, file path, content kind, or time range
- ğŸ¤– **LLM synthesis** â€” Ollama-backed "Ask" mode with multi-source cross-referencing
- ğŸ”§ **Model selection** â€” Choose any Ollama model for synthesis directly from the UI
- ğŸ“Š **Configurable depth** â€” Adjustable retrieval depth (`k`) for precision vs. breadth

### Data Management

- ğŸ“‹ **Document inventory** â€” Browse, inspect, and manage all indexed documents
- ğŸ—‘ï¸ **Deletion** â€” Remove documents from both chunk and image collections
- ğŸ“¦ **Export** â€” Download indexed data as JSONL or ZIP archives
- ğŸ” **Auth** â€” Bearer token authentication with `local` (open) and `strict` modes + rate limiting

### Developer Experience

- ğŸ–¥ï¸ **Dark-mode-first UI** â€” Markdown-native rendering with theme toggle
- ğŸ“¡ **Full-stack health checks** â€” `/health/full` verifies the entire API â†’ Worker â†’ Qdrant chain
- ğŸ“ **Telemetry** â€” Structured JSONL logging with rotation and ingest activity tracking
- ğŸ§ª **Smoke tests** â€” End-to-end and pre-commit validation scripts

---

## âš¡ Quickstart

### Prerequisites

- [Docker](https://www.docker.com/) and Docker Compose
- [Ollama](https://ollama.com/) running locally (for LLM synthesis)

**Pull the embedding model:**

```bash
ollama pull nomic-embed-text
```

### 1. Clone & Start

```bash
git clone https://github.com/Mugiwara555343/jsonify2ai.git
cd jsonify2ai
docker compose up --build -d
```

### 2. Access

| Endpoint | URL |
|----------|-----|
| **Web UI** | [http://localhost:5173](http://localhost:5173) |
| **API** | [http://localhost:8082](http://localhost:8082) |
| **Qdrant Dashboard** | [http://localhost:6333/dashboard](http://localhost:6333/dashboard) |

### 3. Verify

```bash
# Check full-stack health
curl http://localhost:8082/health/full

# Expected: {"ok":true,"api":true,"worker":true}
```

### 4. Ingest Your First File

Drop any supported file into `data/dropzone/` â€” the watcher daemon picks it up automatically. Or drag-and-drop directly in the Web UI.

---

## ğŸ—ºï¸ Roadmap

> Features that exist in code but are not yet polished, or are planned for future development.

| Feature | Status |
|---------|--------|
| Audio transcription (Whisper STT) | âš™ï¸ Code complete, requires optional deps (`requirements.audio.txt`) |
| Image captioning (BLIP) | âš™ï¸ Code complete, behind `IMAGES_CAPTION` flag |
| Hybrid search (vector + keyword) | ğŸ”¬ Qdrant text indexes created, full hybrid ranking in progress |
| Context depth slider in UI | ğŸ“ Backend `k` parameter wired, UI slider planned |
| Multi-collection federation | ğŸ“ Separate chunk/image collections exist, unified query coming |
| Streaming LLM responses | ğŸ“ Planned |

---

## ğŸ“– Philosophy

This project is the result of a **7-month solo intensive** at the intersection of local AI and data privacy. It represents a belief that consumer hardware not cloud APIs should be the default substrate for personal AI.

By treating LLMs as architectural partners rather than syntax generators, the focus has been on **system design, reliability, and scalability** over manual implementation.

The goal: prove that a single builder, using the right tools, can deploy production-grade AI infrastructure that rivals enterprise cloud solutions in accuracy, while keeping every byte on the user's machine.

---

## ğŸ“‚ Documentation

- **[API Reference](docs/API.md)** â€” Endpoints, request/response schemas, auth
- **[Architecture Deep-Dive](docs/ARCHITECTURE.md)** â€” Service topology, data flow, design decisions
- **[Data Model](docs/DATA_MODEL.md)** â€” Chunk schema, Qdrant collections, payload structure
- **[Contracts](docs/contracts.md)** â€” API/Worker interface contracts
- **[Golden Path](docs/golden_path.md)** â€” End-to-end verification runbook

---

## âš–ï¸ License

MIT â€” Hack it, extend it, keep it local.
